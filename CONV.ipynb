{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "# install packages\n",
    "import sys\n",
    "\n",
    "# !{sys.executable} -m pip install matplotlib\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "\n",
    "# read in datasets\n",
    "trainData = pd.read_csv(\"./Dataset/mnist_train.csv\", header=None)\n",
    "testData = pd.read_csv(\"./Dataset/mnist_test.csv\", header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get X and Y data\n",
    "yTrain = {'Y': trainData[0]}\n",
    "yTrain = pd.DataFrame(yTrain)\n",
    "del trainData[0]\n",
    "\n",
    "yTest = {'Y': testData[0]}\n",
    "yTest = pd.DataFrame(yTest)\n",
    "del testData[0]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Y\n",
       "1    6742\n",
       "7    6265\n",
       "3    6131\n",
       "2    5958\n",
       "9    5949\n",
       "0    5923\n",
       "6    5918\n",
       "8    5851\n",
       "4    5842\n",
       "5    5421\n",
       "dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# training digit count\n",
    "display(yTrain.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Test and Train Dataloader\n",
    "from torch.utils.data import DataLoader\n",
    "import torch\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "# create custom dataset\n",
    "class createDataset():\n",
    "    def __init__(self, images, numbers, transform=None):\n",
    "        self.images = images\n",
    "        self.numbers = numbers\n",
    "        # Store the transform to apply to the data\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.numbers)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image = self.images.iloc[idx].values\n",
    "        number =self.numbers.iloc[idx].values\n",
    "\n",
    "        # nromalize image\n",
    "        image = image/ 255\n",
    "        # reshape image for transformation\n",
    "        image = np.reshape(image,[1,28,28])\n",
    "        # image = np.pad(image,pad_width=2)\n",
    "\n",
    "        # Apply the transform to the image, if specified\n",
    "        if self.transform:\n",
    "            # convert numpy array to PIL image\n",
    "            image = Image.fromarray((image).astype(np.uint8))\n",
    "\n",
    "            # perform transformation on the image\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image, number\n",
    "\n",
    "trainDataset = createDataset(trainData,yTrain)\n",
    "testDataset = createDataset(testData,yTest)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Augment the dataset (adds robustness and increases training data)\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "\n",
    "# function to add guassain noise to the images\n",
    "class AddGaussianNoise(object):\n",
    "    def __init__(self, mean=0., std=1.):\n",
    "        self.std = std\n",
    "        self.mean = mean\n",
    "        \n",
    "    def __call__(self, tensor):\n",
    "        print(tensor)\n",
    "        return tensor + torch.randn(tensor.size()) * self.std + self.mean\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return self.__class__.__name__ + '(mean={0}, std={1})'.format(self.mean, self.std)\n",
    "\n",
    "\n",
    "# Create the transformations\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.RandomRotation(degrees=5),\n",
    "    transforms.ToTensor(),\n",
    "    AddGaussianNoise(0.,1.)\n",
    "])\n",
    "\n",
    "# Apply the transformations to the dataset\n",
    "# trainaugmented = createDataset(trainData,yTrain, transform = train_transform)\n",
    "trainaugmented = createDataset(trainData,yTrain)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([100, 1, 28, 28])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Create DataLoader for train, validation and testing\n",
    "batchSize = 32\n",
    "trainloader = DataLoader(trainaugmented,batch_size=100,shuffle=True)\n",
    "testloader = DataLoader(testDataset, batch_size=8)\n",
    "\n",
    "# size of neural network input\n",
    "inputSize = len(trainData.sample().values[0])\n",
    "\n",
    "# num of classes\n",
    "numClasses =  len(yTrain['Y'].unique())\n",
    "\n",
    "# shape of the input image\n",
    "print(next(iter(trainloader))[0].shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bound method Module.parameters of numberPrediction(\n",
      "  (conv1): Sequential(\n",
      "    (0): Conv2d(1, 16, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "    (1): ReLU()\n",
      "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (conv2): Sequential(\n",
      "    (0): Conv2d(16, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "    (1): ReLU()\n",
      "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (out): Linear(in_features=1568, out_features=10, bias=True)\n",
      ")>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Adam (\n",
       "Parameter Group 0\n",
       "    amsgrad: False\n",
       "    betas: (0.9, 0.999)\n",
       "    capturable: False\n",
       "    differentiable: False\n",
       "    eps: 1e-08\n",
       "    foreach: None\n",
       "    fused: False\n",
       "    lr: 0.01\n",
       "    maximize: False\n",
       "    weight_decay: 0\n",
       ")"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Build Network\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "\n",
    "class numberPrediction(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(numberPrediction, self).__init__()\n",
    "        self.conv1 = nn.Sequential(         \n",
    "            nn.Conv2d(\n",
    "                in_channels=1,              \n",
    "                out_channels=16,            \n",
    "                kernel_size=5,              \n",
    "                stride=1,                   \n",
    "                padding=2,                  \n",
    "            ),                              \n",
    "            nn.ReLU(),                      \n",
    "            nn.MaxPool2d(kernel_size=2),    \n",
    "        )\n",
    "        self.conv2 = nn.Sequential(         \n",
    "            nn.Conv2d(16, 32, 5, 1, 2),     \n",
    "            nn.ReLU(),                      \n",
    "            nn.MaxPool2d(2),                \n",
    "        )\n",
    "        # fully connected layer, output 10 classes\n",
    "        self.out = nn.Linear(32 * 7 * 7, 10)\n",
    "\n",
    "   \n",
    "        return\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = x.to(torch.float32)\n",
    "        x = self.conv1(x)\n",
    "        x = self.conv2(x)\n",
    "\n",
    "        # flatten the output of conv2 to (batch_size, 32 * 7 * 7)\n",
    "        x = x.view(x.size(0), -1)   \n",
    "\n",
    "\n",
    "        output = self.out(x)\n",
    "        return output, x    # return x for visualization\n",
    "\n",
    "# create and print model\n",
    "predictionModel = numberPrediction()\n",
    "print(predictionModel.parameters)\n",
    "\n",
    "\n",
    "# Declaring Criterion and Optimizer\n",
    "criterion = nn.CrossEntropyLoss()   \n",
    "criterion\n",
    "\n",
    "from torch import optim\n",
    "optimizer = optim.Adam(predictionModel.parameters(), lr = 0.01)   \n",
    "optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,     0] loss: 230.299, speed: 32.26, accuracy: 8.00 %\n",
      "[1,    10] loss: 2177.823, speed: 34.16, accuracy: 39.55 %\n",
      "[1,    20] loss: 2913.959, speed: 34.71, accuracy: 57.14 %\n",
      "[1,    30] loss: 3308.859, speed: 34.68, accuracy: 66.87 %\n",
      "[1,    40] loss: 3515.471, speed: 34.95, accuracy: 73.39 %\n",
      "[1,    50] loss: 3782.817, speed: 34.86, accuracy: 77.18 %\n",
      "[1,    60] loss: 3969.740, speed: 34.96, accuracy: 80.00 %\n",
      "[1,    70] loss: 4153.863, speed: 35.01, accuracy: 81.99 %\n",
      "[1,    80] loss: 4279.073, speed: 34.99, accuracy: 83.72 %\n",
      "[1,    90] loss: 4432.793, speed: 35.05, accuracy: 85.00 %\n",
      "[1,   100] loss: 4531.792, speed: 35.13, accuracy: 86.16 %\n",
      "[1,   110] loss: 4685.018, speed: 35.09, accuracy: 87.07 %\n",
      "[1,   120] loss: 4782.672, speed: 35.11, accuracy: 87.93 %\n",
      "[1,   130] loss: 4879.291, speed: 35.17, accuracy: 88.59 %\n",
      "[1,   140] loss: 4992.591, speed: 35.25, accuracy: 89.11 %\n",
      "[1,   150] loss: 5083.356, speed: 35.23, accuracy: 89.63 %\n",
      "[1,   160] loss: 5174.708, speed: 35.26, accuracy: 90.09 %\n",
      "[1,   170] loss: 5247.869, speed: 35.33, accuracy: 90.54 %\n",
      "[1,   180] loss: 5367.349, speed: 35.28, accuracy: 90.87 %\n",
      "[1,   190] loss: 5492.976, speed: 35.25, accuracy: 91.16 %\n",
      "[1,   200] loss: 5572.237, speed: 35.27, accuracy: 91.48 %\n",
      "[1,   210] loss: 5645.485, speed: 35.30, accuracy: 91.78 %\n",
      "[1,   220] loss: 5718.425, speed: 35.30, accuracy: 92.05 %\n",
      "[1,   230] loss: 5793.841, speed: 35.30, accuracy: 92.29 %\n",
      "[1,   240] loss: 5886.354, speed: 35.31, accuracy: 92.49 %\n",
      "[1,   250] loss: 5946.296, speed: 35.31, accuracy: 92.71 %\n",
      "[1,   260] loss: 6026.799, speed: 35.28, accuracy: 92.90 %\n",
      "[1,   270] loss: 6118.365, speed: 35.28, accuracy: 93.03 %\n",
      "[1,   280] loss: 6205.713, speed: 35.30, accuracy: 93.16 %\n",
      "[1,   290] loss: 6282.076, speed: 35.24, accuracy: 93.31 %\n",
      "[1,   300] loss: 6336.778, speed: 35.23, accuracy: 93.46 %\n",
      "[1,   310] loss: 6383.708, speed: 35.23, accuracy: 93.62 %\n",
      "[1,   320] loss: 6457.561, speed: 35.24, accuracy: 93.74 %\n",
      "[1,   330] loss: 6533.972, speed: 35.22, accuracy: 93.85 %\n",
      "[1,   340] loss: 6603.094, speed: 35.23, accuracy: 93.98 %\n",
      "[1,   350] loss: 6665.326, speed: 35.24, accuracy: 94.10 %\n",
      "[1,   360] loss: 6720.264, speed: 35.24, accuracy: 94.22 %\n",
      "[1,   370] loss: 6793.498, speed: 35.22, accuracy: 94.32 %\n",
      "[1,   380] loss: 6865.265, speed: 35.25, accuracy: 94.40 %\n",
      "[1,   390] loss: 6934.238, speed: 35.23, accuracy: 94.47 %\n",
      "[1,   400] loss: 6999.656, speed: 35.24, accuracy: 94.57 %\n",
      "[1,   410] loss: 7072.000, speed: 35.23, accuracy: 94.64 %\n",
      "[1,   420] loss: 7162.556, speed: 35.25, accuracy: 94.71 %\n",
      "[1,   430] loss: 7235.739, speed: 35.24, accuracy: 94.78 %\n",
      "[1,   440] loss: 7306.913, speed: 35.24, accuracy: 94.85 %\n",
      "[1,   450] loss: 7388.240, speed: 35.25, accuracy: 94.92 %\n",
      "[1,   460] loss: 7441.636, speed: 35.27, accuracy: 95.00 %\n",
      "[1,   470] loss: 7509.313, speed: 35.27, accuracy: 95.07 %\n",
      "[1,   480] loss: 7581.370, speed: 35.26, accuracy: 95.12 %\n",
      "[1,   490] loss: 7653.289, speed: 35.27, accuracy: 95.17 %\n",
      "[1,   500] loss: 7727.142, speed: 35.29, accuracy: 95.22 %\n",
      "[1,   510] loss: 7797.544, speed: 35.29, accuracy: 95.26 %\n",
      "[1,   520] loss: 7873.787, speed: 35.31, accuracy: 95.31 %\n",
      "[1,   530] loss: 7924.148, speed: 35.31, accuracy: 95.37 %\n",
      "[1,   540] loss: 7995.820, speed: 35.32, accuracy: 95.42 %\n",
      "[1,   550] loss: 8086.613, speed: 35.33, accuracy: 95.45 %\n",
      "[1,   560] loss: 8145.001, speed: 35.33, accuracy: 95.50 %\n",
      "[1,   570] loss: 8227.545, speed: 35.33, accuracy: 95.54 %\n",
      "[1,   580] loss: 8289.476, speed: 35.34, accuracy: 95.58 %\n",
      "[1,   590] loss: 8366.409, speed: 35.34, accuracy: 95.61 %\n",
      "[2,     0] loss: 7.445, speed: 32.26, accuracy: 97.00 %\n",
      "[2,    10] loss: 98.223, speed: 34.81, accuracy: 96.82 %\n",
      "[2,    20] loss: 148.732, speed: 35.18, accuracy: 97.33 %\n",
      "[2,    30] loss: 217.554, speed: 35.47, accuracy: 97.61 %\n",
      "[2,    40] loss: 269.203, speed: 35.34, accuracy: 97.83 %\n",
      "[2,    50] loss: 306.239, speed: 35.22, accuracy: 98.00 %\n",
      "[2,    60] loss: 347.762, speed: 34.88, accuracy: 98.10 %\n",
      "[2,    70] loss: 423.549, speed: 34.79, accuracy: 98.07 %\n",
      "[2,    80] loss: 465.530, speed: 34.72, accuracy: 98.10 %\n",
      "[2,    90] loss: 522.893, speed: 34.83, accuracy: 98.10 %\n",
      "[2,   100] loss: 571.513, speed: 34.84, accuracy: 98.13 %\n",
      "[2,   110] loss: 615.072, speed: 34.93, accuracy: 98.18 %\n",
      "[2,   120] loss: 657.767, speed: 34.98, accuracy: 98.21 %\n",
      "[2,   130] loss: 703.294, speed: 35.08, accuracy: 98.24 %\n",
      "[2,   140] loss: 734.214, speed: 35.12, accuracy: 98.26 %\n",
      "[2,   150] loss: 787.374, speed: 35.11, accuracy: 98.26 %\n",
      "[2,   160] loss: 832.517, speed: 35.13, accuracy: 98.29 %\n",
      "[2,   170] loss: 893.435, speed: 35.16, accuracy: 98.29 %\n",
      "[2,   180] loss: 923.365, speed: 35.24, accuracy: 98.33 %\n",
      "[2,   190] loss: 954.486, speed: 35.24, accuracy: 98.36 %\n",
      "[2,   200] loss: 998.438, speed: 35.28, accuracy: 98.37 %\n",
      "[2,   210] loss: 1037.523, speed: 35.33, accuracy: 98.39 %\n",
      "[2,   220] loss: 1099.964, speed: 35.33, accuracy: 98.38 %\n",
      "[2,   230] loss: 1157.916, speed: 35.34, accuracy: 98.39 %\n",
      "[2,   240] loss: 1208.857, speed: 35.38, accuracy: 98.39 %\n",
      "[2,   250] loss: 1255.540, speed: 35.40, accuracy: 98.40 %\n",
      "[2,   260] loss: 1290.705, speed: 35.39, accuracy: 98.41 %\n",
      "[2,   270] loss: 1338.312, speed: 35.38, accuracy: 98.41 %\n",
      "[2,   280] loss: 1413.229, speed: 35.38, accuracy: 98.40 %\n",
      "[2,   290] loss: 1512.304, speed: 35.35, accuracy: 98.37 %\n",
      "[2,   300] loss: 1588.679, speed: 35.30, accuracy: 98.33 %\n",
      "[2,   310] loss: 1635.062, speed: 35.30, accuracy: 98.34 %\n",
      "[2,   320] loss: 1669.050, speed: 35.31, accuracy: 98.36 %\n",
      "[2,   330] loss: 1748.457, speed: 35.31, accuracy: 98.34 %\n",
      "[2,   340] loss: 1842.388, speed: 35.30, accuracy: 98.30 %\n",
      "[2,   350] loss: 1889.493, speed: 35.30, accuracy: 98.30 %\n",
      "[2,   360] loss: 1946.913, speed: 35.28, accuracy: 98.30 %\n",
      "[2,   370] loss: 1985.463, speed: 35.30, accuracy: 98.32 %\n",
      "[2,   380] loss: 2024.270, speed: 35.32, accuracy: 98.33 %\n",
      "[2,   390] loss: 2068.692, speed: 35.30, accuracy: 98.34 %\n",
      "[2,   400] loss: 2129.530, speed: 35.30, accuracy: 98.34 %\n",
      "[2,   410] loss: 2162.463, speed: 35.29, accuracy: 98.36 %\n",
      "[2,   420] loss: 2211.359, speed: 35.29, accuracy: 98.36 %\n",
      "[2,   430] loss: 2260.099, speed: 35.29, accuracy: 98.37 %\n",
      "[2,   440] loss: 2302.145, speed: 35.29, accuracy: 98.38 %\n",
      "[2,   450] loss: 2353.336, speed: 35.32, accuracy: 98.38 %\n",
      "[2,   460] loss: 2401.836, speed: 35.32, accuracy: 98.39 %\n",
      "[2,   470] loss: 2453.326, speed: 35.30, accuracy: 98.39 %\n",
      "[2,   480] loss: 2501.779, speed: 35.30, accuracy: 98.38 %\n",
      "[2,   490] loss: 2587.796, speed: 35.32, accuracy: 98.36 %\n",
      "[2,   500] loss: 2656.349, speed: 35.32, accuracy: 98.36 %\n",
      "[2,   510] loss: 2714.881, speed: 35.33, accuracy: 98.35 %\n",
      "[2,   520] loss: 2747.687, speed: 35.34, accuracy: 98.37 %\n",
      "[2,   530] loss: 2803.534, speed: 35.34, accuracy: 98.36 %\n",
      "[2,   540] loss: 2863.276, speed: 35.34, accuracy: 98.36 %\n",
      "[2,   550] loss: 2939.582, speed: 35.34, accuracy: 98.35 %\n",
      "[2,   560] loss: 3005.840, speed: 35.35, accuracy: 98.34 %\n",
      "[2,   570] loss: 3077.201, speed: 35.33, accuracy: 98.33 %\n",
      "[2,   580] loss: 3128.260, speed: 35.32, accuracy: 98.33 %\n",
      "[2,   590] loss: 3186.972, speed: 35.33, accuracy: 98.32 %\n",
      "[3,     0] loss: 3.196, speed: 35.71, accuracy: 99.00 %\n",
      "[3,    10] loss: 32.056, speed: 35.48, accuracy: 98.73 %\n",
      "[3,    20] loss: 64.018, speed: 35.35, accuracy: 98.95 %\n",
      "[3,    30] loss: 80.621, speed: 34.60, accuracy: 99.10 %\n",
      "[3,    40] loss: 110.023, speed: 34.72, accuracy: 99.10 %\n",
      "[3,    50] loss: 156.110, speed: 34.72, accuracy: 98.92 %\n",
      "[3,    60] loss: 206.286, speed: 34.84, accuracy: 98.89 %\n",
      "[3,    70] loss: 266.302, speed: 34.82, accuracy: 98.86 %\n",
      "[3,    80] loss: 319.983, speed: 34.82, accuracy: 98.77 %\n",
      "[3,    90] loss: 362.348, speed: 34.77, accuracy: 98.76 %\n",
      "[3,   100] loss: 420.164, speed: 34.73, accuracy: 98.70 %\n",
      "[3,   110] loss: 455.816, speed: 34.73, accuracy: 98.74 %\n",
      "[3,   120] loss: 501.613, speed: 34.55, accuracy: 98.76 %\n",
      "[3,   130] loss: 550.734, speed: 34.52, accuracy: 98.75 %\n",
      "[3,   140] loss: 596.542, speed: 34.52, accuracy: 98.71 %\n",
      "[3,   150] loss: 644.780, speed: 34.45, accuracy: 98.70 %\n",
      "[3,   160] loss: 703.975, speed: 34.50, accuracy: 98.68 %\n",
      "[3,   170] loss: 735.087, speed: 34.62, accuracy: 98.67 %\n",
      "[3,   180] loss: 752.696, speed: 34.65, accuracy: 98.71 %\n",
      "[3,   190] loss: 800.807, speed: 34.65, accuracy: 98.71 %\n",
      "[3,   200] loss: 852.824, speed: 34.70, accuracy: 98.69 %\n",
      "[3,   210] loss: 921.521, speed: 34.70, accuracy: 98.68 %\n",
      "[3,   220] loss: 942.119, speed: 34.68, accuracy: 98.69 %\n",
      "[3,   230] loss: 972.900, speed: 34.73, accuracy: 98.69 %\n",
      "[3,   240] loss: 1003.361, speed: 34.78, accuracy: 98.71 %\n",
      "[3,   250] loss: 1059.862, speed: 34.81, accuracy: 98.71 %\n",
      "[3,   260] loss: 1122.994, speed: 34.84, accuracy: 98.69 %\n",
      "[3,   270] loss: 1174.230, speed: 34.89, accuracy: 98.68 %\n",
      "[3,   280] loss: 1221.434, speed: 34.91, accuracy: 98.67 %\n",
      "[3,   290] loss: 1260.476, speed: 34.89, accuracy: 98.67 %\n",
      "[3,   300] loss: 1301.369, speed: 34.81, accuracy: 98.66 %\n",
      "[3,   310] loss: 1336.122, speed: 34.81, accuracy: 98.67 %\n",
      "[3,   320] loss: 1370.359, speed: 34.84, accuracy: 98.69 %\n",
      "[3,   330] loss: 1420.337, speed: 34.85, accuracy: 98.68 %\n",
      "[3,   340] loss: 1464.910, speed: 34.84, accuracy: 98.67 %\n",
      "[3,   350] loss: 1522.896, speed: 34.87, accuracy: 98.65 %\n",
      "[3,   360] loss: 1574.775, speed: 34.86, accuracy: 98.66 %\n",
      "[3,   370] loss: 1632.182, speed: 34.88, accuracy: 98.65 %\n",
      "[3,   380] loss: 1684.676, speed: 34.91, accuracy: 98.64 %\n",
      "[3,   390] loss: 1753.495, speed: 34.88, accuracy: 98.64 %\n",
      "[3,   400] loss: 1796.692, speed: 34.84, accuracy: 98.63 %\n",
      "[3,   410] loss: 1828.197, speed: 34.78, accuracy: 98.64 %\n",
      "[3,   420] loss: 1861.905, speed: 34.78, accuracy: 98.64 %\n",
      "[3,   430] loss: 1921.160, speed: 34.74, accuracy: 98.63 %\n",
      "[3,   440] loss: 1957.223, speed: 34.73, accuracy: 98.63 %\n",
      "[3,   450] loss: 2036.603, speed: 34.75, accuracy: 98.61 %\n",
      "[3,   460] loss: 2097.270, speed: 34.79, accuracy: 98.60 %\n",
      "[3,   470] loss: 2133.505, speed: 34.79, accuracy: 98.60 %\n",
      "[3,   480] loss: 2189.439, speed: 34.81, accuracy: 98.60 %\n",
      "[3,   490] loss: 2257.459, speed: 34.83, accuracy: 98.58 %\n",
      "[3,   500] loss: 2284.933, speed: 34.83, accuracy: 98.58 %\n",
      "[3,   510] loss: 2344.904, speed: 34.84, accuracy: 98.57 %\n",
      "[3,   520] loss: 2386.736, speed: 34.82, accuracy: 98.57 %\n",
      "[3,   530] loss: 2444.942, speed: 34.83, accuracy: 98.57 %\n",
      "[3,   540] loss: 2506.021, speed: 34.81, accuracy: 98.56 %\n",
      "[3,   550] loss: 2556.694, speed: 34.82, accuracy: 98.56 %\n",
      "[3,   560] loss: 2590.568, speed: 34.82, accuracy: 98.56 %\n",
      "[3,   570] loss: 2628.530, speed: 34.80, accuracy: 98.56 %\n",
      "[3,   580] loss: 2682.762, speed: 34.81, accuracy: 98.57 %\n",
      "[3,   590] loss: 2712.767, speed: 34.83, accuracy: 98.58 %\n",
      "[4,     0] loss: 7.382, speed: 35.72, accuracy: 98.00 %\n",
      "[4,    10] loss: 59.799, speed: 36.07, accuracy: 98.64 %\n",
      "[4,    20] loss: 84.767, speed: 36.14, accuracy: 98.76 %\n",
      "[4,    30] loss: 125.425, speed: 36.22, accuracy: 98.65 %\n",
      "[4,    40] loss: 158.508, speed: 35.90, accuracy: 98.68 %\n",
      "[4,    50] loss: 185.146, speed: 35.61, accuracy: 98.73 %\n",
      "[4,    60] loss: 220.754, speed: 35.59, accuracy: 98.67 %\n",
      "[4,    70] loss: 246.447, speed: 35.66, accuracy: 98.76 %\n",
      "[4,    80] loss: 286.692, speed: 35.46, accuracy: 98.75 %\n",
      "[4,    90] loss: 346.210, speed: 35.38, accuracy: 98.67 %\n",
      "[4,   100] loss: 379.714, speed: 35.39, accuracy: 98.69 %\n",
      "[4,   110] loss: 408.244, speed: 35.33, accuracy: 98.75 %\n",
      "[4,   120] loss: 447.097, speed: 35.32, accuracy: 98.75 %\n",
      "[4,   130] loss: 474.379, speed: 35.29, accuracy: 98.79 %\n",
      "[4,   140] loss: 520.564, speed: 35.27, accuracy: 98.77 %\n",
      "[4,   150] loss: 577.915, speed: 35.34, accuracy: 98.75 %\n",
      "[4,   160] loss: 637.984, speed: 35.35, accuracy: 98.71 %\n",
      "[4,   170] loss: 694.006, speed: 35.33, accuracy: 98.67 %\n",
      "[4,   180] loss: 748.176, speed: 35.23, accuracy: 98.66 %\n",
      "[4,   190] loss: 797.583, speed: 35.21, accuracy: 98.64 %\n",
      "[4,   200] loss: 825.138, speed: 35.16, accuracy: 98.66 %\n",
      "[4,   210] loss: 845.496, speed: 35.11, accuracy: 98.69 %\n",
      "[4,   220] loss: 882.858, speed: 35.07, accuracy: 98.69 %\n",
      "[4,   230] loss: 905.902, speed: 35.13, accuracy: 98.71 %\n",
      "[4,   240] loss: 951.065, speed: 35.14, accuracy: 98.72 %\n",
      "[4,   250] loss: 989.644, speed: 35.11, accuracy: 98.73 %\n",
      "[4,   260] loss: 1023.184, speed: 35.11, accuracy: 98.74 %\n",
      "[4,   270] loss: 1039.992, speed: 35.12, accuracy: 98.77 %\n",
      "[4,   280] loss: 1081.075, speed: 35.14, accuracy: 98.77 %\n",
      "[4,   290] loss: 1147.795, speed: 35.12, accuracy: 98.76 %\n",
      "[4,   300] loss: 1211.895, speed: 35.15, accuracy: 98.75 %\n",
      "[4,   310] loss: 1261.135, speed: 35.17, accuracy: 98.75 %\n",
      "[4,   320] loss: 1315.918, speed: 35.18, accuracy: 98.73 %\n",
      "[4,   330] loss: 1375.295, speed: 35.23, accuracy: 98.72 %\n",
      "[4,   340] loss: 1412.430, speed: 35.22, accuracy: 98.71 %\n",
      "[4,   350] loss: 1450.965, speed: 35.25, accuracy: 98.70 %\n",
      "[4,   360] loss: 1529.965, speed: 35.27, accuracy: 98.69 %\n",
      "[4,   370] loss: 1556.039, speed: 35.32, accuracy: 98.70 %\n",
      "[4,   380] loss: 1595.890, speed: 35.32, accuracy: 98.69 %\n",
      "[4,   390] loss: 1640.262, speed: 35.32, accuracy: 98.68 %\n",
      "[4,   400] loss: 1673.626, speed: 35.32, accuracy: 98.67 %\n",
      "[4,   410] loss: 1705.573, speed: 35.33, accuracy: 98.68 %\n",
      "[4,   420] loss: 1774.918, speed: 35.33, accuracy: 98.67 %\n",
      "[4,   430] loss: 1799.049, speed: 35.33, accuracy: 98.69 %\n",
      "[4,   440] loss: 1833.587, speed: 35.35, accuracy: 98.70 %\n",
      "[4,   450] loss: 1873.856, speed: 35.36, accuracy: 98.70 %\n",
      "[4,   460] loss: 1921.331, speed: 35.36, accuracy: 98.70 %\n",
      "[4,   470] loss: 1996.723, speed: 35.31, accuracy: 98.68 %\n",
      "[4,   480] loss: 2067.682, speed: 35.30, accuracy: 98.67 %\n",
      "[4,   490] loss: 2149.903, speed: 35.28, accuracy: 98.66 %\n",
      "[4,   500] loss: 2179.246, speed: 35.28, accuracy: 98.66 %\n",
      "[4,   510] loss: 2215.528, speed: 35.30, accuracy: 98.66 %\n",
      "[4,   520] loss: 2244.928, speed: 35.31, accuracy: 98.67 %\n",
      "[4,   530] loss: 2299.270, speed: 35.27, accuracy: 98.66 %\n",
      "[4,   540] loss: 2331.085, speed: 35.26, accuracy: 98.66 %\n",
      "[4,   550] loss: 2372.370, speed: 35.23, accuracy: 98.66 %\n",
      "[4,   560] loss: 2427.914, speed: 35.21, accuracy: 98.66 %\n",
      "[4,   570] loss: 2464.545, speed: 35.16, accuracy: 98.66 %\n",
      "[4,   580] loss: 2494.219, speed: 35.15, accuracy: 98.66 %\n",
      "[4,   590] loss: 2526.396, speed: 35.13, accuracy: 98.67 %\n",
      "[5,     0] loss: 2.786, speed: 33.33, accuracy: 99.00 %\n",
      "[5,    10] loss: 34.816, speed: 36.30, accuracy: 98.91 %\n",
      "[5,    20] loss: 78.604, speed: 35.90, accuracy: 98.86 %\n",
      "[5,    30] loss: 93.382, speed: 35.71, accuracy: 99.03 %\n",
      "[5,    40] loss: 119.541, speed: 35.44, accuracy: 99.05 %\n",
      "[5,    50] loss: 148.305, speed: 35.34, accuracy: 99.04 %\n",
      "[5,    60] loss: 165.260, speed: 35.22, accuracy: 99.08 %\n",
      "[5,    70] loss: 186.177, speed: 35.25, accuracy: 99.06 %\n",
      "[5,    80] loss: 230.919, speed: 35.17, accuracy: 99.02 %\n",
      "[5,    90] loss: 270.699, speed: 35.16, accuracy: 99.02 %\n",
      "[5,   100] loss: 312.180, speed: 35.07, accuracy: 99.00 %\n",
      "[5,   110] loss: 363.784, speed: 34.98, accuracy: 98.93 %\n",
      "[5,   120] loss: 399.095, speed: 34.89, accuracy: 98.91 %\n",
      "[5,   130] loss: 435.598, speed: 34.84, accuracy: 98.92 %\n",
      "[5,   140] loss: 469.928, speed: 34.78, accuracy: 98.91 %\n",
      "[5,   150] loss: 483.807, speed: 34.66, accuracy: 98.95 %\n",
      "[5,   160] loss: 540.195, speed: 34.71, accuracy: 98.91 %\n",
      "[5,   170] loss: 583.726, speed: 34.82, accuracy: 98.88 %\n",
      "[5,   180] loss: 638.557, speed: 34.72, accuracy: 98.88 %\n",
      "[5,   190] loss: 660.120, speed: 34.65, accuracy: 98.91 %\n",
      "[5,   200] loss: 694.930, speed: 34.57, accuracy: 98.91 %\n",
      "[5,   210] loss: 724.843, speed: 34.52, accuracy: 98.90 %\n",
      "[5,   220] loss: 769.889, speed: 34.56, accuracy: 98.89 %\n",
      "[5,   230] loss: 814.241, speed: 34.61, accuracy: 98.89 %\n",
      "[5,   240] loss: 860.883, speed: 34.67, accuracy: 98.90 %\n",
      "[5,   250] loss: 911.228, speed: 34.70, accuracy: 98.87 %\n",
      "[5,   260] loss: 984.575, speed: 34.72, accuracy: 98.82 %\n",
      "[5,   270] loss: 1042.337, speed: 34.77, accuracy: 98.81 %\n",
      "[5,   280] loss: 1089.369, speed: 34.79, accuracy: 98.82 %\n",
      "[5,   290] loss: 1116.648, speed: 34.82, accuracy: 98.82 %\n",
      "[5,   300] loss: 1162.458, speed: 34.88, accuracy: 98.82 %\n",
      "[5,   310] loss: 1188.049, speed: 34.90, accuracy: 98.82 %\n",
      "[5,   320] loss: 1220.818, speed: 34.91, accuracy: 98.83 %\n",
      "[5,   330] loss: 1272.742, speed: 34.94, accuracy: 98.82 %\n",
      "[5,   340] loss: 1315.370, speed: 34.93, accuracy: 98.81 %\n",
      "[5,   350] loss: 1362.203, speed: 34.94, accuracy: 98.81 %\n",
      "[5,   360] loss: 1424.598, speed: 34.97, accuracy: 98.80 %\n",
      "[5,   370] loss: 1455.404, speed: 34.98, accuracy: 98.80 %\n",
      "[5,   380] loss: 1478.451, speed: 34.98, accuracy: 98.81 %\n",
      "[5,   390] loss: 1522.967, speed: 34.97, accuracy: 98.81 %\n",
      "[5,   400] loss: 1556.982, speed: 34.99, accuracy: 98.81 %\n",
      "[5,   410] loss: 1602.846, speed: 35.01, accuracy: 98.80 %\n",
      "[5,   420] loss: 1665.946, speed: 34.99, accuracy: 98.79 %\n",
      "[5,   430] loss: 1738.802, speed: 35.01, accuracy: 98.78 %\n",
      "[5,   440] loss: 1799.902, speed: 35.02, accuracy: 98.76 %\n",
      "[5,   450] loss: 1848.129, speed: 34.99, accuracy: 98.76 %\n",
      "[5,   460] loss: 1901.709, speed: 34.95, accuracy: 98.74 %\n",
      "[5,   470] loss: 1933.696, speed: 34.97, accuracy: 98.75 %\n",
      "[5,   480] loss: 2001.174, speed: 34.99, accuracy: 98.73 %\n",
      "[5,   490] loss: 2045.477, speed: 34.96, accuracy: 98.72 %\n",
      "[5,   500] loss: 2078.189, speed: 34.97, accuracy: 98.73 %\n",
      "[5,   510] loss: 2108.302, speed: 34.98, accuracy: 98.73 %\n",
      "[5,   520] loss: 2162.617, speed: 34.98, accuracy: 98.73 %\n",
      "[5,   530] loss: 2196.591, speed: 34.94, accuracy: 98.72 %\n",
      "[5,   540] loss: 2250.486, speed: 34.96, accuracy: 98.71 %\n",
      "[5,   550] loss: 2284.417, speed: 34.99, accuracy: 98.71 %\n",
      "[5,   560] loss: 2378.211, speed: 34.97, accuracy: 98.71 %\n",
      "[5,   570] loss: 2424.999, speed: 34.99, accuracy: 98.71 %\n",
      "[5,   580] loss: 2467.286, speed: 35.00, accuracy: 98.71 %\n",
      "[5,   590] loss: 2501.685, speed: 35.03, accuracy: 98.71 %\n",
      "[6,     0] loss: 12.201, speed: 35.71, accuracy: 98.00 %\n",
      "[6,    10] loss: 53.336, speed: 35.83, accuracy: 98.64 %\n",
      "[6,    20] loss: 97.667, speed: 35.47, accuracy: 98.62 %\n",
      "[6,    30] loss: 115.725, speed: 35.47, accuracy: 98.81 %\n",
      "[6,    40] loss: 129.079, speed: 35.28, accuracy: 98.98 %\n",
      "[6,    50] loss: 145.543, speed: 35.37, accuracy: 99.10 %\n",
      "[6,    60] loss: 182.587, speed: 35.20, accuracy: 99.03 %\n",
      "[6,    70] loss: 213.256, speed: 35.25, accuracy: 99.03 %\n",
      "[6,    80] loss: 242.984, speed: 35.00, accuracy: 99.01 %\n",
      "[6,    90] loss: 268.307, speed: 35.01, accuracy: 99.05 %\n",
      "[6,   100] loss: 314.074, speed: 35.04, accuracy: 99.05 %\n",
      "[6,   110] loss: 357.002, speed: 35.03, accuracy: 99.04 %\n",
      "[6,   120] loss: 404.705, speed: 35.11, accuracy: 99.02 %\n",
      "[6,   130] loss: 447.392, speed: 35.12, accuracy: 99.01 %\n",
      "[6,   140] loss: 477.213, speed: 35.13, accuracy: 98.99 %\n",
      "[6,   150] loss: 489.818, speed: 35.18, accuracy: 99.02 %\n",
      "[6,   160] loss: 524.606, speed: 35.19, accuracy: 99.03 %\n",
      "[6,   170] loss: 541.780, speed: 35.25, accuracy: 99.05 %\n",
      "[6,   180] loss: 576.902, speed: 35.26, accuracy: 99.04 %\n",
      "[6,   190] loss: 619.134, speed: 35.28, accuracy: 99.04 %\n",
      "[6,   200] loss: 687.469, speed: 35.29, accuracy: 99.03 %\n",
      "[6,   210] loss: 718.352, speed: 35.26, accuracy: 99.03 %\n",
      "[6,   220] loss: 744.679, speed: 35.25, accuracy: 99.03 %\n",
      "[6,   230] loss: 776.329, speed: 35.30, accuracy: 99.03 %\n",
      "[6,   240] loss: 826.563, speed: 35.29, accuracy: 99.00 %\n",
      "[6,   250] loss: 849.898, speed: 35.26, accuracy: 99.02 %\n",
      "[6,   260] loss: 910.581, speed: 35.22, accuracy: 98.98 %\n",
      "[6,   270] loss: 932.221, speed: 35.21, accuracy: 98.99 %\n",
      "[6,   280] loss: 978.764, speed: 35.19, accuracy: 98.98 %\n",
      "[6,   290] loss: 1014.288, speed: 35.22, accuracy: 98.98 %\n",
      "[6,   300] loss: 1044.437, speed: 35.25, accuracy: 98.99 %\n",
      "[6,   310] loss: 1078.415, speed: 35.26, accuracy: 98.98 %\n",
      "[6,   320] loss: 1131.490, speed: 35.22, accuracy: 98.97 %\n",
      "[6,   330] loss: 1186.161, speed: 35.23, accuracy: 98.94 %\n",
      "[6,   340] loss: 1203.395, speed: 35.23, accuracy: 98.96 %\n",
      "[6,   350] loss: 1249.788, speed: 35.20, accuracy: 98.95 %\n",
      "[6,   360] loss: 1281.723, speed: 35.21, accuracy: 98.96 %\n",
      "[6,   370] loss: 1303.293, speed: 35.23, accuracy: 98.96 %\n",
      "[6,   380] loss: 1335.587, speed: 35.22, accuracy: 98.96 %\n",
      "[6,   390] loss: 1364.130, speed: 35.23, accuracy: 98.97 %\n",
      "[6,   400] loss: 1425.842, speed: 35.22, accuracy: 98.95 %\n",
      "[6,   410] loss: 1492.324, speed: 35.21, accuracy: 98.93 %\n",
      "[6,   420] loss: 1557.551, speed: 35.18, accuracy: 98.90 %\n",
      "[6,   430] loss: 1590.892, speed: 35.16, accuracy: 98.90 %\n",
      "[6,   440] loss: 1621.098, speed: 35.19, accuracy: 98.91 %\n",
      "[6,   450] loss: 1694.271, speed: 35.18, accuracy: 98.90 %\n",
      "[6,   460] loss: 1735.129, speed: 35.17, accuracy: 98.90 %\n",
      "[6,   470] loss: 1759.543, speed: 35.14, accuracy: 98.90 %\n",
      "[6,   480] loss: 1780.388, speed: 35.15, accuracy: 98.90 %\n",
      "[6,   490] loss: 1795.673, speed: 35.14, accuracy: 98.91 %\n",
      "[6,   500] loss: 1822.593, speed: 35.13, accuracy: 98.91 %\n",
      "[6,   510] loss: 1887.411, speed: 35.14, accuracy: 98.91 %\n",
      "[6,   520] loss: 1950.423, speed: 35.15, accuracy: 98.90 %\n",
      "[6,   530] loss: 1989.930, speed: 35.14, accuracy: 98.89 %\n",
      "[6,   540] loss: 2049.349, speed: 35.17, accuracy: 98.89 %\n",
      "[6,   550] loss: 2094.004, speed: 35.20, accuracy: 98.88 %\n",
      "[6,   560] loss: 2151.515, speed: 35.19, accuracy: 98.88 %\n",
      "[6,   570] loss: 2182.417, speed: 35.13, accuracy: 98.89 %\n",
      "[6,   580] loss: 2203.763, speed: 35.14, accuracy: 98.90 %\n",
      "[6,   590] loss: 2249.183, speed: 35.16, accuracy: 98.89 %\n",
      "[7,     0] loss: 3.459, speed: 35.71, accuracy: 99.00 %\n",
      "[7,    10] loss: 19.726, speed: 35.83, accuracy: 99.45 %\n",
      "[7,    20] loss: 37.442, speed: 36.08, accuracy: 99.33 %\n",
      "[7,    30] loss: 55.590, speed: 35.94, accuracy: 99.35 %\n",
      "[7,    40] loss: 73.558, speed: 35.92, accuracy: 99.32 %\n",
      "[7,    50] loss: 132.299, speed: 35.95, accuracy: 99.16 %\n",
      "[7,    60] loss: 161.095, speed: 35.93, accuracy: 99.10 %\n",
      "[7,    70] loss: 196.044, speed: 35.74, accuracy: 99.10 %\n",
      "[7,    80] loss: 220.190, speed: 35.77, accuracy: 99.10 %\n",
      "[7,    90] loss: 260.806, speed: 35.82, accuracy: 99.10 %\n",
      "[7,   100] loss: 300.619, speed: 35.78, accuracy: 99.05 %\n",
      "[7,   110] loss: 326.466, speed: 35.82, accuracy: 99.06 %\n",
      "[7,   120] loss: 371.685, speed: 35.77, accuracy: 99.04 %\n",
      "[7,   130] loss: 404.323, speed: 35.78, accuracy: 99.02 %\n",
      "[7,   140] loss: 432.631, speed: 35.74, accuracy: 99.01 %\n",
      "[7,   150] loss: 456.081, speed: 35.81, accuracy: 99.01 %\n",
      "[7,   160] loss: 491.270, speed: 35.88, accuracy: 99.01 %\n",
      "[7,   170] loss: 543.329, speed: 35.86, accuracy: 98.98 %\n",
      "[7,   180] loss: 578.745, speed: 35.84, accuracy: 98.98 %\n",
      "[7,   190] loss: 597.141, speed: 35.87, accuracy: 98.99 %\n",
      "[7,   200] loss: 621.324, speed: 35.82, accuracy: 99.00 %\n",
      "[7,   210] loss: 639.360, speed: 35.75, accuracy: 99.03 %\n",
      "[7,   220] loss: 672.870, speed: 35.72, accuracy: 99.02 %\n",
      "[7,   230] loss: 692.010, speed: 35.73, accuracy: 99.03 %\n",
      "[7,   240] loss: 719.244, speed: 35.71, accuracy: 99.04 %\n",
      "[7,   250] loss: 751.613, speed: 35.72, accuracy: 99.04 %\n",
      "[7,   260] loss: 783.748, speed: 35.75, accuracy: 99.04 %\n",
      "[7,   270] loss: 802.253, speed: 35.77, accuracy: 99.04 %\n",
      "[7,   280] loss: 834.168, speed: 35.72, accuracy: 99.04 %\n",
      "[7,   290] loss: 858.734, speed: 35.73, accuracy: 99.05 %\n",
      "[7,   300] loss: 879.719, speed: 35.75, accuracy: 99.05 %\n",
      "[7,   310] loss: 934.684, speed: 35.77, accuracy: 99.03 %\n",
      "[7,   320] loss: 957.632, speed: 35.76, accuracy: 99.04 %\n",
      "[7,   330] loss: 985.603, speed: 35.75, accuracy: 99.04 %\n",
      "[7,   340] loss: 1017.364, speed: 35.76, accuracy: 99.04 %\n",
      "[7,   350] loss: 1048.007, speed: 35.72, accuracy: 99.04 %\n",
      "[7,   360] loss: 1077.729, speed: 35.73, accuracy: 99.04 %\n",
      "[7,   370] loss: 1115.356, speed: 35.76, accuracy: 99.04 %\n",
      "[7,   380] loss: 1156.275, speed: 35.78, accuracy: 99.03 %\n",
      "[7,   390] loss: 1191.972, speed: 35.77, accuracy: 99.02 %\n",
      "[7,   400] loss: 1260.368, speed: 35.79, accuracy: 99.00 %\n",
      "[7,   410] loss: 1285.593, speed: 35.81, accuracy: 99.00 %\n",
      "[7,   420] loss: 1324.922, speed: 35.84, accuracy: 99.00 %\n",
      "[7,   430] loss: 1361.670, speed: 35.82, accuracy: 99.00 %\n",
      "[7,   440] loss: 1379.752, speed: 35.83, accuracy: 99.00 %\n",
      "[7,   450] loss: 1424.190, speed: 35.84, accuracy: 99.01 %\n",
      "[7,   460] loss: 1462.661, speed: 35.83, accuracy: 99.01 %\n",
      "[7,   470] loss: 1511.056, speed: 35.85, accuracy: 99.00 %\n",
      "[7,   480] loss: 1537.814, speed: 35.87, accuracy: 99.00 %\n",
      "[7,   490] loss: 1575.506, speed: 35.86, accuracy: 99.01 %\n",
      "[7,   500] loss: 1616.351, speed: 35.85, accuracy: 99.00 %\n",
      "[7,   510] loss: 1680.357, speed: 35.86, accuracy: 98.98 %\n",
      "[7,   520] loss: 1729.374, speed: 35.85, accuracy: 98.97 %\n",
      "[7,   530] loss: 1787.734, speed: 35.85, accuracy: 98.96 %\n",
      "[7,   540] loss: 1827.829, speed: 35.86, accuracy: 98.96 %\n",
      "[7,   550] loss: 1889.011, speed: 35.86, accuracy: 98.95 %\n",
      "[7,   560] loss: 1935.120, speed: 35.87, accuracy: 98.94 %\n",
      "[7,   570] loss: 1976.538, speed: 35.84, accuracy: 98.94 %\n",
      "[7,   580] loss: 2041.722, speed: 35.84, accuracy: 98.92 %\n",
      "[7,   590] loss: 2112.411, speed: 35.84, accuracy: 98.91 %\n",
      "[8,     0] loss: 2.208, speed: 33.33, accuracy: 98.00 %\n",
      "[8,    10] loss: 31.463, speed: 36.18, accuracy: 98.91 %\n",
      "[8,    20] loss: 69.715, speed: 35.84, accuracy: 98.81 %\n",
      "[8,    30] loss: 94.934, speed: 36.17, accuracy: 98.84 %\n",
      "[8,    40] loss: 152.798, speed: 36.06, accuracy: 98.83 %\n",
      "[8,    50] loss: 209.761, speed: 35.79, accuracy: 98.82 %\n",
      "[8,    60] loss: 251.311, speed: 35.92, accuracy: 98.80 %\n",
      "[8,    70] loss: 284.536, speed: 36.00, accuracy: 98.79 %\n",
      "[8,    80] loss: 314.242, speed: 35.97, accuracy: 98.83 %\n",
      "[8,    90] loss: 331.349, speed: 36.07, accuracy: 98.90 %\n",
      "[8,   100] loss: 367.197, speed: 36.16, accuracy: 98.89 %\n",
      "[8,   110] loss: 386.977, speed: 36.16, accuracy: 98.93 %\n",
      "[8,   120] loss: 441.546, speed: 36.12, accuracy: 98.92 %\n",
      "[8,   130] loss: 455.131, speed: 36.05, accuracy: 98.96 %\n",
      "[8,   140] loss: 473.680, speed: 36.03, accuracy: 98.99 %\n",
      "[8,   150] loss: 501.606, speed: 36.00, accuracy: 98.99 %\n",
      "[8,   160] loss: 521.730, speed: 36.04, accuracy: 99.00 %\n",
      "[8,   170] loss: 564.151, speed: 36.09, accuracy: 98.99 %\n",
      "[8,   180] loss: 602.051, speed: 36.06, accuracy: 98.98 %\n",
      "[8,   190] loss: 621.387, speed: 36.04, accuracy: 98.99 %\n",
      "[8,   200] loss: 643.505, speed: 36.03, accuracy: 99.01 %\n",
      "[8,   210] loss: 660.673, speed: 36.04, accuracy: 99.03 %\n",
      "[8,   220] loss: 683.586, speed: 36.06, accuracy: 99.04 %\n",
      "[8,   230] loss: 699.821, speed: 36.07, accuracy: 99.06 %\n",
      "[8,   240] loss: 721.409, speed: 36.06, accuracy: 99.07 %\n",
      "[8,   250] loss: 767.931, speed: 36.05, accuracy: 99.04 %\n",
      "[8,   260] loss: 802.263, speed: 36.01, accuracy: 99.04 %\n",
      "[8,   270] loss: 863.597, speed: 36.00, accuracy: 99.03 %\n",
      "[8,   280] loss: 885.026, speed: 35.98, accuracy: 99.03 %\n",
      "[8,   290] loss: 911.816, speed: 35.95, accuracy: 99.03 %\n",
      "[8,   300] loss: 940.740, speed: 35.98, accuracy: 99.05 %\n",
      "[8,   310] loss: 974.553, speed: 35.94, accuracy: 99.04 %\n",
      "[8,   320] loss: 1002.918, speed: 35.95, accuracy: 99.05 %\n",
      "[8,   330] loss: 1054.692, speed: 35.87, accuracy: 99.03 %\n",
      "[8,   340] loss: 1120.992, speed: 35.86, accuracy: 99.01 %\n",
      "[8,   350] loss: 1161.320, speed: 35.87, accuracy: 99.02 %\n",
      "[8,   360] loss: 1203.021, speed: 35.84, accuracy: 99.01 %\n",
      "[8,   370] loss: 1233.811, speed: 35.84, accuracy: 99.02 %\n",
      "[8,   380] loss: 1260.030, speed: 35.85, accuracy: 99.02 %\n",
      "[8,   390] loss: 1290.990, speed: 35.87, accuracy: 99.02 %\n",
      "[8,   400] loss: 1320.200, speed: 35.84, accuracy: 99.02 %\n",
      "[8,   410] loss: 1337.668, speed: 35.83, accuracy: 99.03 %\n",
      "[8,   420] loss: 1362.607, speed: 35.85, accuracy: 99.03 %\n",
      "[8,   430] loss: 1427.246, speed: 35.85, accuracy: 99.02 %\n",
      "[8,   440] loss: 1478.973, speed: 35.84, accuracy: 99.00 %\n",
      "[8,   450] loss: 1547.778, speed: 35.86, accuracy: 98.98 %\n",
      "[8,   460] loss: 1585.250, speed: 35.87, accuracy: 98.97 %\n",
      "[8,   470] loss: 1651.985, speed: 35.89, accuracy: 98.96 %\n",
      "[8,   480] loss: 1693.779, speed: 35.89, accuracy: 98.96 %\n",
      "[8,   490] loss: 1734.292, speed: 35.91, accuracy: 98.96 %\n",
      "[8,   500] loss: 1775.158, speed: 35.92, accuracy: 98.96 %\n",
      "[8,   510] loss: 1807.453, speed: 35.89, accuracy: 98.96 %\n",
      "[8,   520] loss: 1851.182, speed: 35.89, accuracy: 98.97 %\n",
      "[8,   530] loss: 1901.525, speed: 35.88, accuracy: 98.96 %\n",
      "[8,   540] loss: 1927.459, speed: 35.89, accuracy: 98.96 %\n",
      "[8,   550] loss: 1998.614, speed: 35.87, accuracy: 98.95 %\n",
      "[8,   560] loss: 2047.156, speed: 35.88, accuracy: 98.94 %\n",
      "[8,   570] loss: 2120.216, speed: 35.88, accuracy: 98.93 %\n",
      "[8,   580] loss: 2159.424, speed: 35.86, accuracy: 98.93 %\n",
      "[8,   590] loss: 2203.200, speed: 35.85, accuracy: 98.92 %\n",
      "[9,     0] loss: 0.038, speed: 35.72, accuracy: 100.00 %\n",
      "[9,    10] loss: 62.088, speed: 34.48, accuracy: 98.55 %\n",
      "[9,    20] loss: 85.210, speed: 34.88, accuracy: 98.76 %\n",
      "[9,    30] loss: 123.948, speed: 35.51, accuracy: 98.81 %\n",
      "[9,    40] loss: 170.753, speed: 35.62, accuracy: 98.78 %\n",
      "[9,    50] loss: 194.524, speed: 35.56, accuracy: 98.82 %\n",
      "[9,    60] loss: 225.450, speed: 35.61, accuracy: 98.89 %\n",
      "[9,    70] loss: 263.700, speed: 35.57, accuracy: 98.86 %\n",
      "[9,    80] loss: 290.245, speed: 35.67, accuracy: 98.84 %\n",
      "[9,    90] loss: 312.977, speed: 35.73, accuracy: 98.87 %\n",
      "[9,   100] loss: 377.151, speed: 35.75, accuracy: 98.84 %\n",
      "[9,   110] loss: 448.344, speed: 35.74, accuracy: 98.82 %\n",
      "[9,   120] loss: 470.103, speed: 35.68, accuracy: 98.88 %\n",
      "[9,   130] loss: 504.189, speed: 35.73, accuracy: 98.89 %\n",
      "[9,   140] loss: 567.297, speed: 35.71, accuracy: 98.86 %\n",
      "[9,   150] loss: 598.101, speed: 35.70, accuracy: 98.86 %\n",
      "[9,   160] loss: 616.705, speed: 35.60, accuracy: 98.89 %\n",
      "[9,   170] loss: 651.173, speed: 35.61, accuracy: 98.91 %\n",
      "[9,   180] loss: 670.570, speed: 35.68, accuracy: 98.92 %\n",
      "[9,   190] loss: 688.808, speed: 35.63, accuracy: 98.94 %\n",
      "[9,   200] loss: 700.839, speed: 35.66, accuracy: 98.97 %\n",
      "[9,   210] loss: 739.356, speed: 35.67, accuracy: 98.97 %\n",
      "[9,   220] loss: 760.972, speed: 35.63, accuracy: 98.99 %\n",
      "[9,   230] loss: 793.049, speed: 35.63, accuracy: 98.99 %\n",
      "[9,   240] loss: 852.917, speed: 35.62, accuracy: 98.95 %\n",
      "[9,   250] loss: 897.579, speed: 35.65, accuracy: 98.95 %\n",
      "[9,   260] loss: 935.365, speed: 35.64, accuracy: 98.93 %\n",
      "[9,   270] loss: 955.246, speed: 35.66, accuracy: 98.94 %\n",
      "[9,   280] loss: 982.391, speed: 35.67, accuracy: 98.96 %\n",
      "[9,   290] loss: 1013.643, speed: 35.70, accuracy: 98.97 %\n",
      "[9,   300] loss: 1039.634, speed: 35.64, accuracy: 98.97 %\n",
      "[9,   310] loss: 1082.161, speed: 35.65, accuracy: 98.97 %\n",
      "[9,   320] loss: 1113.302, speed: 35.68, accuracy: 98.97 %\n",
      "[9,   330] loss: 1125.344, speed: 35.70, accuracy: 98.99 %\n",
      "[9,   340] loss: 1153.039, speed: 35.69, accuracy: 99.00 %\n",
      "[9,   350] loss: 1180.979, speed: 35.71, accuracy: 99.00 %\n",
      "[9,   360] loss: 1203.721, speed: 35.70, accuracy: 99.01 %\n",
      "[9,   370] loss: 1218.741, speed: 35.68, accuracy: 99.02 %\n",
      "[9,   380] loss: 1245.720, speed: 35.69, accuracy: 99.02 %\n",
      "[9,   390] loss: 1261.278, speed: 35.71, accuracy: 99.03 %\n",
      "[9,   400] loss: 1289.571, speed: 35.72, accuracy: 99.03 %\n",
      "[9,   410] loss: 1332.460, speed: 35.71, accuracy: 99.03 %\n",
      "[9,   420] loss: 1366.929, speed: 35.69, accuracy: 99.02 %\n",
      "[9,   430] loss: 1407.721, speed: 35.68, accuracy: 99.02 %\n",
      "[9,   440] loss: 1451.262, speed: 35.67, accuracy: 99.01 %\n",
      "[9,   450] loss: 1476.132, speed: 35.67, accuracy: 99.01 %\n",
      "[9,   460] loss: 1515.897, speed: 35.68, accuracy: 99.00 %\n",
      "[9,   470] loss: 1568.326, speed: 35.68, accuracy: 98.99 %\n",
      "[9,   480] loss: 1606.700, speed: 35.68, accuracy: 98.99 %\n",
      "[9,   490] loss: 1654.577, speed: 35.68, accuracy: 98.98 %\n",
      "[9,   500] loss: 1726.970, speed: 35.69, accuracy: 98.97 %\n",
      "[9,   510] loss: 1754.549, speed: 35.69, accuracy: 98.97 %\n",
      "[9,   520] loss: 1815.009, speed: 35.70, accuracy: 98.97 %\n",
      "[9,   530] loss: 1852.206, speed: 35.68, accuracy: 98.97 %\n",
      "[9,   540] loss: 1867.858, speed: 35.70, accuracy: 98.97 %\n",
      "[9,   550] loss: 1886.968, speed: 35.69, accuracy: 98.98 %\n",
      "[9,   560] loss: 1918.749, speed: 35.67, accuracy: 98.98 %\n",
      "[9,   570] loss: 1936.703, speed: 35.65, accuracy: 98.99 %\n",
      "[9,   580] loss: 2002.669, speed: 35.63, accuracy: 98.99 %\n",
      "[9,   590] loss: 2041.292, speed: 35.63, accuracy: 98.99 %\n",
      "[10,     0] loss: 0.700, speed: 32.26, accuracy: 100.00 %\n",
      "[10,    10] loss: 30.088, speed: 35.08, accuracy: 98.82 %\n",
      "[10,    20] loss: 36.554, speed: 35.02, accuracy: 99.24 %\n",
      "[10,    30] loss: 48.823, speed: 34.27, accuracy: 99.32 %\n",
      "[10,    40] loss: 87.582, speed: 34.41, accuracy: 99.17 %\n",
      "[10,    50] loss: 102.609, speed: 34.42, accuracy: 99.22 %\n",
      "[10,    60] loss: 113.176, speed: 34.59, accuracy: 99.28 %\n",
      "[10,    70] loss: 134.620, speed: 34.56, accuracy: 99.27 %\n",
      "[10,    80] loss: 150.724, speed: 34.68, accuracy: 99.32 %\n",
      "[10,    90] loss: 169.954, speed: 34.69, accuracy: 99.33 %\n",
      "[10,   100] loss: 200.711, speed: 34.81, accuracy: 99.32 %\n",
      "[10,   110] loss: 215.760, speed: 34.88, accuracy: 99.32 %\n",
      "[10,   120] loss: 243.038, speed: 34.97, accuracy: 99.31 %\n",
      "[10,   130] loss: 259.372, speed: 34.96, accuracy: 99.31 %\n",
      "[10,   140] loss: 284.299, speed: 35.10, accuracy: 99.28 %\n",
      "[10,   150] loss: 353.408, speed: 35.09, accuracy: 99.24 %\n",
      "[10,   160] loss: 372.946, speed: 35.13, accuracy: 99.25 %\n",
      "[10,   170] loss: 402.067, speed: 35.17, accuracy: 99.26 %\n",
      "[10,   180] loss: 429.569, speed: 35.26, accuracy: 99.26 %\n",
      "[10,   190] loss: 440.372, speed: 35.27, accuracy: 99.27 %\n",
      "[10,   200] loss: 453.343, speed: 35.26, accuracy: 99.28 %\n",
      "[10,   210] loss: 486.822, speed: 35.29, accuracy: 99.28 %\n",
      "[10,   220] loss: 514.730, speed: 35.32, accuracy: 99.27 %\n",
      "[10,   230] loss: 548.836, speed: 35.30, accuracy: 99.26 %\n",
      "[10,   240] loss: 584.458, speed: 35.34, accuracy: 99.23 %\n",
      "[10,   250] loss: 611.275, speed: 35.38, accuracy: 99.24 %\n",
      "[10,   260] loss: 645.009, speed: 35.39, accuracy: 99.23 %\n",
      "[10,   270] loss: 679.368, speed: 35.37, accuracy: 99.21 %\n",
      "[10,   280] loss: 729.376, speed: 35.39, accuracy: 99.20 %\n",
      "[10,   290] loss: 763.842, speed: 35.38, accuracy: 99.20 %\n",
      "[10,   300] loss: 782.098, speed: 35.34, accuracy: 99.20 %\n",
      "[10,   310] loss: 835.117, speed: 35.37, accuracy: 99.19 %\n",
      "[10,   320] loss: 876.088, speed: 35.39, accuracy: 99.19 %\n",
      "[10,   330] loss: 911.504, speed: 35.39, accuracy: 99.19 %\n",
      "[10,   340] loss: 936.720, speed: 35.39, accuracy: 99.19 %\n",
      "[10,   350] loss: 985.818, speed: 35.43, accuracy: 99.19 %\n",
      "[10,   360] loss: 1051.423, speed: 35.45, accuracy: 99.18 %\n",
      "[10,   370] loss: 1093.381, speed: 35.42, accuracy: 99.17 %\n",
      "[10,   380] loss: 1124.966, speed: 35.41, accuracy: 99.17 %\n",
      "[10,   390] loss: 1166.483, speed: 35.44, accuracy: 99.15 %\n",
      "[10,   400] loss: 1206.499, speed: 35.45, accuracy: 99.14 %\n",
      "[10,   410] loss: 1232.516, speed: 35.43, accuracy: 99.14 %\n",
      "[10,   420] loss: 1261.115, speed: 35.44, accuracy: 99.14 %\n",
      "[10,   430] loss: 1335.611, speed: 35.44, accuracy: 99.12 %\n",
      "[10,   440] loss: 1363.772, speed: 35.42, accuracy: 99.12 %\n",
      "[10,   450] loss: 1427.264, speed: 35.43, accuracy: 99.10 %\n",
      "[10,   460] loss: 1495.220, speed: 35.43, accuracy: 99.09 %\n",
      "[10,   470] loss: 1536.690, speed: 35.42, accuracy: 99.09 %\n",
      "[10,   480] loss: 1582.620, speed: 35.42, accuracy: 99.08 %\n",
      "[10,   490] loss: 1597.849, speed: 35.42, accuracy: 99.09 %\n",
      "[10,   500] loss: 1669.949, speed: 35.44, accuracy: 99.08 %\n",
      "[10,   510] loss: 1697.715, speed: 35.44, accuracy: 99.08 %\n",
      "[10,   520] loss: 1744.205, speed: 35.43, accuracy: 99.08 %\n",
      "[10,   530] loss: 1791.575, speed: 35.44, accuracy: 99.07 %\n",
      "[10,   540] loss: 1887.089, speed: 35.43, accuracy: 99.06 %\n",
      "[10,   550] loss: 1955.428, speed: 35.42, accuracy: 99.04 %\n",
      "[10,   560] loss: 2001.344, speed: 35.44, accuracy: 99.04 %\n",
      "[10,   570] loss: 2055.619, speed: 35.46, accuracy: 99.04 %\n",
      "[10,   580] loss: 2089.484, speed: 35.46, accuracy: 99.03 %\n",
      "[10,   590] loss: 2121.487, speed: 35.46, accuracy: 99.03 %\n",
      "[11,     0] loss: 17.541, speed: 34.48, accuracy: 96.00 %\n",
      "[11,    10] loss: 63.429, speed: 35.60, accuracy: 98.55 %\n",
      "[11,    20] loss: 139.257, speed: 35.29, accuracy: 98.57 %\n",
      "[11,    30] loss: 159.946, speed: 35.15, accuracy: 98.77 %\n",
      "[11,    40] loss: 183.152, speed: 35.31, accuracy: 98.88 %\n",
      "[11,    50] loss: 218.565, speed: 35.42, accuracy: 98.92 %\n",
      "[11,    60] loss: 240.190, speed: 35.30, accuracy: 98.98 %\n",
      "[11,    70] loss: 278.782, speed: 35.41, accuracy: 98.99 %\n",
      "[11,    80] loss: 306.112, speed: 35.34, accuracy: 99.01 %\n",
      "[11,    90] loss: 338.736, speed: 35.40, accuracy: 99.09 %\n",
      "[11,   100] loss: 354.739, speed: 35.37, accuracy: 99.12 %\n",
      "[11,   110] loss: 402.475, speed: 35.39, accuracy: 99.09 %\n",
      "[11,   120] loss: 421.939, speed: 35.28, accuracy: 99.10 %\n",
      "[11,   130] loss: 452.175, speed: 35.09, accuracy: 99.11 %\n",
      "[11,   140] loss: 477.473, speed: 35.18, accuracy: 99.13 %\n",
      "[11,   150] loss: 506.397, speed: 35.20, accuracy: 99.13 %\n",
      "[11,   160] loss: 574.256, speed: 35.20, accuracy: 99.11 %\n",
      "[11,   170] loss: 631.098, speed: 35.27, accuracy: 99.06 %\n",
      "[11,   180] loss: 695.107, speed: 35.30, accuracy: 99.02 %\n",
      "[11,   190] loss: 718.216, speed: 35.31, accuracy: 99.04 %\n",
      "[11,   200] loss: 755.462, speed: 35.31, accuracy: 99.01 %\n",
      "[11,   210] loss: 784.251, speed: 35.33, accuracy: 99.00 %\n",
      "[11,   220] loss: 849.136, speed: 35.38, accuracy: 98.97 %\n",
      "[11,   230] loss: 889.609, speed: 35.38, accuracy: 98.96 %\n",
      "[11,   240] loss: 952.073, speed: 35.40, accuracy: 98.94 %\n",
      "[11,   250] loss: 996.086, speed: 35.43, accuracy: 98.94 %\n",
      "[11,   260] loss: 1007.534, speed: 35.50, accuracy: 98.96 %\n",
      "[11,   270] loss: 1042.811, speed: 35.47, accuracy: 98.97 %\n",
      "[11,   280] loss: 1067.799, speed: 35.49, accuracy: 98.97 %\n",
      "[11,   290] loss: 1102.101, speed: 35.49, accuracy: 98.98 %\n",
      "[11,   300] loss: 1128.198, speed: 35.51, accuracy: 98.99 %\n",
      "[11,   310] loss: 1182.228, speed: 35.45, accuracy: 98.97 %\n",
      "[11,   320] loss: 1217.973, speed: 35.47, accuracy: 98.97 %\n",
      "[11,   330] loss: 1244.016, speed: 35.49, accuracy: 98.98 %\n",
      "[11,   340] loss: 1291.387, speed: 35.47, accuracy: 98.98 %\n",
      "[11,   350] loss: 1324.014, speed: 35.50, accuracy: 98.99 %\n",
      "[11,   360] loss: 1341.254, speed: 35.49, accuracy: 99.00 %\n",
      "[11,   370] loss: 1369.976, speed: 35.48, accuracy: 99.00 %\n",
      "[11,   380] loss: 1428.699, speed: 35.49, accuracy: 98.99 %\n",
      "[11,   390] loss: 1464.782, speed: 35.46, accuracy: 98.98 %\n",
      "[11,   400] loss: 1503.666, speed: 35.46, accuracy: 98.98 %\n",
      "[11,   410] loss: 1570.923, speed: 35.44, accuracy: 98.97 %\n",
      "[11,   420] loss: 1639.532, speed: 35.46, accuracy: 98.97 %\n",
      "[11,   430] loss: 1677.438, speed: 35.45, accuracy: 98.97 %\n",
      "[11,   440] loss: 1744.809, speed: 35.45, accuracy: 98.95 %\n",
      "[11,   450] loss: 1775.042, speed: 35.44, accuracy: 98.96 %\n",
      "[11,   460] loss: 1825.740, speed: 35.42, accuracy: 98.95 %\n",
      "[11,   470] loss: 1860.594, speed: 35.43, accuracy: 98.95 %\n",
      "[11,   480] loss: 1922.642, speed: 35.41, accuracy: 98.95 %\n",
      "[11,   490] loss: 1949.963, speed: 35.41, accuracy: 98.96 %\n",
      "[11,   500] loss: 1994.915, speed: 35.41, accuracy: 98.95 %\n",
      "[11,   510] loss: 2032.451, speed: 35.42, accuracy: 98.95 %\n",
      "[11,   520] loss: 2063.225, speed: 35.40, accuracy: 98.96 %\n",
      "[11,   530] loss: 2111.113, speed: 35.39, accuracy: 98.95 %\n",
      "[11,   540] loss: 2127.686, speed: 35.39, accuracy: 98.96 %\n",
      "[11,   550] loss: 2154.841, speed: 35.39, accuracy: 98.97 %\n",
      "[11,   560] loss: 2190.760, speed: 35.41, accuracy: 98.97 %\n",
      "[11,   570] loss: 2214.703, speed: 35.40, accuracy: 98.97 %\n",
      "[11,   580] loss: 2239.289, speed: 35.41, accuracy: 98.97 %\n",
      "[11,   590] loss: 2272.443, speed: 35.40, accuracy: 98.97 %\n",
      "[12,     0] loss: 3.630, speed: 33.33, accuracy: 98.00 %\n",
      "[12,    10] loss: 23.595, speed: 35.37, accuracy: 99.45 %\n",
      "[12,    20] loss: 49.070, speed: 34.88, accuracy: 99.43 %\n",
      "[12,    30] loss: 72.828, speed: 34.07, accuracy: 99.39 %\n",
      "[12,    40] loss: 82.774, speed: 33.79, accuracy: 99.41 %\n",
      "[12,    50] loss: 115.503, speed: 33.67, accuracy: 99.43 %\n",
      "[12,    60] loss: 135.900, speed: 33.60, accuracy: 99.43 %\n",
      "[12,    70] loss: 152.059, speed: 33.93, accuracy: 99.42 %\n",
      "[12,    80] loss: 174.687, speed: 34.24, accuracy: 99.41 %\n",
      "[12,    90] loss: 215.120, speed: 34.24, accuracy: 99.36 %\n",
      "[12,   100] loss: 242.800, speed: 34.27, accuracy: 99.35 %\n",
      "[12,   110] loss: 276.956, speed: 34.45, accuracy: 99.32 %\n",
      "[12,   120] loss: 312.869, speed: 34.54, accuracy: 99.28 %\n",
      "[12,   130] loss: 327.826, speed: 34.61, accuracy: 99.31 %\n",
      "[12,   140] loss: 358.082, speed: 34.61, accuracy: 99.31 %\n",
      "[12,   150] loss: 388.960, speed: 34.69, accuracy: 99.30 %\n",
      "[12,   160] loss: 444.357, speed: 34.66, accuracy: 99.25 %\n",
      "[12,   170] loss: 472.810, speed: 34.72, accuracy: 99.25 %\n",
      "[12,   180] loss: 512.071, speed: 34.80, accuracy: 99.23 %\n",
      "[12,   190] loss: 544.773, speed: 34.84, accuracy: 99.23 %\n",
      "[12,   200] loss: 599.260, speed: 34.88, accuracy: 99.21 %\n",
      "[12,   210] loss: 622.420, speed: 34.94, accuracy: 99.22 %\n",
      "[12,   220] loss: 663.919, speed: 34.98, accuracy: 99.21 %\n",
      "[12,   230] loss: 709.078, speed: 34.96, accuracy: 99.20 %\n",
      "[12,   240] loss: 739.177, speed: 34.95, accuracy: 99.19 %\n",
      "[12,   250] loss: 754.975, speed: 35.02, accuracy: 99.19 %\n",
      "[12,   260] loss: 780.991, speed: 35.08, accuracy: 99.19 %\n",
      "[12,   270] loss: 808.621, speed: 35.07, accuracy: 99.18 %\n",
      "[12,   280] loss: 829.234, speed: 35.10, accuracy: 99.19 %\n",
      "[12,   290] loss: 868.142, speed: 35.11, accuracy: 99.18 %\n",
      "[12,   300] loss: 900.099, speed: 35.10, accuracy: 99.18 %\n",
      "[12,   310] loss: 944.342, speed: 35.08, accuracy: 99.17 %\n",
      "[12,   320] loss: 960.931, speed: 35.10, accuracy: 99.17 %\n",
      "[12,   330] loss: 1024.196, speed: 35.10, accuracy: 99.16 %\n",
      "[12,   340] loss: 1062.155, speed: 35.11, accuracy: 99.15 %\n",
      "[12,   350] loss: 1084.164, speed: 35.16, accuracy: 99.16 %\n",
      "[12,   360] loss: 1105.632, speed: 35.17, accuracy: 99.16 %\n",
      "[12,   370] loss: 1168.901, speed: 35.19, accuracy: 99.14 %\n",
      "[12,   380] loss: 1228.984, speed: 35.18, accuracy: 99.13 %\n",
      "[12,   390] loss: 1259.319, speed: 35.17, accuracy: 99.13 %\n",
      "[12,   400] loss: 1293.175, speed: 35.19, accuracy: 99.13 %\n",
      "[12,   410] loss: 1328.062, speed: 35.18, accuracy: 99.14 %\n",
      "[12,   420] loss: 1355.169, speed: 35.18, accuracy: 99.14 %\n",
      "[12,   430] loss: 1389.894, speed: 35.19, accuracy: 99.14 %\n",
      "[12,   440] loss: 1433.876, speed: 35.22, accuracy: 99.14 %\n",
      "[12,   450] loss: 1475.407, speed: 35.21, accuracy: 99.13 %\n",
      "[12,   460] loss: 1520.466, speed: 35.21, accuracy: 99.12 %\n",
      "[12,   470] loss: 1561.295, speed: 35.20, accuracy: 99.12 %\n",
      "[12,   480] loss: 1618.564, speed: 35.19, accuracy: 99.11 %\n",
      "[12,   490] loss: 1665.220, speed: 35.17, accuracy: 99.11 %\n",
      "[12,   500] loss: 1705.665, speed: 35.19, accuracy: 99.12 %\n",
      "[12,   510] loss: 1728.002, speed: 35.21, accuracy: 99.12 %\n",
      "[12,   520] loss: 1769.001, speed: 35.22, accuracy: 99.11 %\n",
      "[12,   530] loss: 1797.441, speed: 35.23, accuracy: 99.12 %\n",
      "[12,   540] loss: 1835.483, speed: 35.22, accuracy: 99.12 %\n",
      "[12,   550] loss: 1887.306, speed: 35.24, accuracy: 99.12 %\n",
      "[12,   560] loss: 1932.059, speed: 35.26, accuracy: 99.12 %\n",
      "[12,   570] loss: 1971.223, speed: 35.28, accuracy: 99.12 %\n",
      "[12,   580] loss: 2036.773, speed: 35.27, accuracy: 99.10 %\n",
      "[12,   590] loss: 2065.960, speed: 35.28, accuracy: 99.11 %\n",
      "[13,     0] loss: 11.472, speed: 34.48, accuracy: 98.00 %\n",
      "[13,    10] loss: 22.481, speed: 35.03, accuracy: 99.55 %\n",
      "[13,    20] loss: 56.605, speed: 34.71, accuracy: 99.24 %\n",
      "[13,    30] loss: 80.865, speed: 35.19, accuracy: 99.26 %\n",
      "[13,    40] loss: 132.729, speed: 35.44, accuracy: 99.22 %\n",
      "[13,    50] loss: 198.940, speed: 35.66, accuracy: 99.04 %\n",
      "[13,    60] loss: 231.401, speed: 35.61, accuracy: 99.02 %\n",
      "[13,    70] loss: 264.256, speed: 35.59, accuracy: 99.04 %\n",
      "[13,    80] loss: 305.961, speed: 35.46, accuracy: 98.99 %\n",
      "[13,    90] loss: 318.992, speed: 35.38, accuracy: 99.07 %\n",
      "[13,   100] loss: 358.862, speed: 35.33, accuracy: 99.06 %\n",
      "[13,   110] loss: 390.430, speed: 35.25, accuracy: 99.05 %\n",
      "[13,   120] loss: 428.573, speed: 35.23, accuracy: 99.04 %\n",
      "[13,   130] loss: 480.484, speed: 35.18, accuracy: 99.02 %\n",
      "[13,   140] loss: 528.642, speed: 35.29, accuracy: 99.03 %\n",
      "[13,   150] loss: 564.688, speed: 35.32, accuracy: 99.05 %\n",
      "[13,   160] loss: 571.696, speed: 35.34, accuracy: 99.09 %\n",
      "[13,   170] loss: 612.353, speed: 35.32, accuracy: 99.09 %\n",
      "[13,   180] loss: 633.421, speed: 35.35, accuracy: 99.10 %\n",
      "[13,   190] loss: 711.655, speed: 35.37, accuracy: 99.09 %\n",
      "[13,   200] loss: 726.573, speed: 35.27, accuracy: 99.09 %\n",
      "[13,   210] loss: 758.217, speed: 35.31, accuracy: 99.09 %\n",
      "[13,   220] loss: 808.519, speed: 35.29, accuracy: 99.08 %\n",
      "[13,   230] loss: 842.521, speed: 35.34, accuracy: 99.09 %\n",
      "[13,   240] loss: 887.287, speed: 35.34, accuracy: 99.05 %\n",
      "[13,   250] loss: 924.800, speed: 35.32, accuracy: 99.04 %\n",
      "[13,   260] loss: 941.719, speed: 35.32, accuracy: 99.06 %\n",
      "[13,   270] loss: 980.732, speed: 35.29, accuracy: 99.05 %\n",
      "[13,   280] loss: 1043.133, speed: 35.30, accuracy: 99.03 %\n",
      "[13,   290] loss: 1053.866, speed: 35.28, accuracy: 99.05 %\n",
      "[13,   300] loss: 1068.806, speed: 35.28, accuracy: 99.07 %\n",
      "[13,   310] loss: 1106.288, speed: 35.28, accuracy: 99.08 %\n",
      "[13,   320] loss: 1125.168, speed: 35.27, accuracy: 99.08 %\n",
      "[13,   330] loss: 1157.955, speed: 35.30, accuracy: 99.09 %\n",
      "[13,   340] loss: 1204.937, speed: 35.30, accuracy: 99.08 %\n",
      "[13,   350] loss: 1221.955, speed: 35.32, accuracy: 99.09 %\n",
      "[13,   360] loss: 1241.652, speed: 35.35, accuracy: 99.10 %\n",
      "[13,   370] loss: 1270.020, speed: 35.35, accuracy: 99.10 %\n",
      "[13,   380] loss: 1304.295, speed: 35.36, accuracy: 99.10 %\n",
      "[13,   390] loss: 1329.164, speed: 35.37, accuracy: 99.10 %\n",
      "[13,   400] loss: 1384.015, speed: 35.36, accuracy: 99.10 %\n",
      "[13,   410] loss: 1458.376, speed: 35.33, accuracy: 99.09 %\n",
      "[13,   420] loss: 1499.305, speed: 35.32, accuracy: 99.08 %\n",
      "[13,   430] loss: 1540.321, speed: 35.32, accuracy: 99.07 %\n",
      "[13,   440] loss: 1556.906, speed: 35.33, accuracy: 99.09 %\n",
      "[13,   450] loss: 1587.324, speed: 35.33, accuracy: 99.08 %\n",
      "[13,   460] loss: 1611.358, speed: 35.35, accuracy: 99.10 %\n",
      "[13,   470] loss: 1661.727, speed: 35.36, accuracy: 99.09 %\n",
      "[13,   480] loss: 1718.621, speed: 35.34, accuracy: 99.09 %\n",
      "[13,   490] loss: 1750.446, speed: 35.35, accuracy: 99.09 %\n",
      "[13,   500] loss: 1798.567, speed: 35.35, accuracy: 99.08 %\n",
      "[13,   510] loss: 1893.659, speed: 35.36, accuracy: 99.08 %\n",
      "[13,   520] loss: 1943.249, speed: 35.35, accuracy: 99.07 %\n",
      "[13,   530] loss: 2003.223, speed: 35.36, accuracy: 99.07 %\n",
      "[13,   540] loss: 2034.429, speed: 35.37, accuracy: 99.07 %\n",
      "[13,   550] loss: 2067.423, speed: 35.34, accuracy: 99.07 %\n",
      "[13,   560] loss: 2129.665, speed: 35.34, accuracy: 99.05 %\n",
      "[13,   570] loss: 2183.454, speed: 35.37, accuracy: 99.05 %\n",
      "[13,   580] loss: 2244.467, speed: 35.37, accuracy: 99.04 %\n",
      "[13,   590] loss: 2263.517, speed: 35.35, accuracy: 99.05 %\n",
      "[14,     0] loss: 0.079, speed: 33.33, accuracy: 100.00 %\n",
      "[14,    10] loss: 38.305, speed: 35.60, accuracy: 98.73 %\n",
      "[14,    20] loss: 78.590, speed: 35.35, accuracy: 98.86 %\n",
      "[14,    30] loss: 86.708, speed: 35.19, accuracy: 99.06 %\n",
      "[14,    40] loss: 122.798, speed: 35.51, accuracy: 99.10 %\n",
      "[14,    50] loss: 155.895, speed: 35.62, accuracy: 99.06 %\n",
      "[14,    60] loss: 175.455, speed: 35.43, accuracy: 99.10 %\n",
      "[14,    70] loss: 218.167, speed: 35.56, accuracy: 99.11 %\n",
      "[14,    80] loss: 244.275, speed: 35.61, accuracy: 99.14 %\n",
      "[14,    90] loss: 255.973, speed: 35.62, accuracy: 99.15 %\n",
      "[14,   100] loss: 266.868, speed: 35.50, accuracy: 99.21 %\n",
      "[14,   110] loss: 278.615, speed: 35.54, accuracy: 99.25 %\n",
      "[14,   120] loss: 300.925, speed: 35.58, accuracy: 99.25 %\n",
      "[14,   130] loss: 326.936, speed: 35.47, accuracy: 99.24 %\n",
      "[14,   140] loss: 357.804, speed: 35.54, accuracy: 99.25 %\n",
      "[14,   150] loss: 380.117, speed: 35.56, accuracy: 99.26 %\n",
      "[14,   160] loss: 420.410, speed: 35.54, accuracy: 99.26 %\n",
      "[14,   170] loss: 455.882, speed: 35.48, accuracy: 99.26 %\n",
      "[14,   180] loss: 504.738, speed: 35.46, accuracy: 99.25 %\n",
      "[14,   190] loss: 525.777, speed: 35.46, accuracy: 99.26 %\n",
      "[14,   200] loss: 586.860, speed: 35.42, accuracy: 99.26 %\n",
      "[14,   210] loss: 609.786, speed: 35.42, accuracy: 99.27 %\n",
      "[14,   220] loss: 630.758, speed: 35.45, accuracy: 99.27 %\n",
      "[14,   230] loss: 667.365, speed: 35.46, accuracy: 99.27 %\n",
      "[14,   240] loss: 717.313, speed: 35.47, accuracy: 99.26 %\n",
      "[14,   250] loss: 742.819, speed: 35.44, accuracy: 99.27 %\n",
      "[14,   260] loss: 764.637, speed: 35.47, accuracy: 99.28 %\n",
      "[14,   270] loss: 783.623, speed: 35.46, accuracy: 99.28 %\n",
      "[14,   280] loss: 818.977, speed: 35.47, accuracy: 99.26 %\n",
      "[14,   290] loss: 844.920, speed: 35.49, accuracy: 99.26 %\n",
      "[14,   300] loss: 887.901, speed: 35.50, accuracy: 99.26 %\n",
      "[14,   310] loss: 939.313, speed: 35.52, accuracy: 99.24 %\n",
      "[14,   320] loss: 963.823, speed: 35.53, accuracy: 99.24 %\n",
      "[14,   330] loss: 1004.689, speed: 35.55, accuracy: 99.24 %\n",
      "[14,   340] loss: 1032.425, speed: 35.56, accuracy: 99.25 %\n",
      "[14,   350] loss: 1060.895, speed: 35.55, accuracy: 99.25 %\n",
      "[14,   360] loss: 1078.744, speed: 35.58, accuracy: 99.26 %\n",
      "[14,   370] loss: 1113.379, speed: 35.57, accuracy: 99.26 %\n",
      "[14,   380] loss: 1135.393, speed: 35.57, accuracy: 99.25 %\n",
      "[14,   390] loss: 1172.045, speed: 35.56, accuracy: 99.25 %\n",
      "[14,   400] loss: 1213.461, speed: 35.54, accuracy: 99.24 %\n",
      "[14,   410] loss: 1237.240, speed: 35.54, accuracy: 99.24 %\n",
      "[14,   420] loss: 1272.736, speed: 35.53, accuracy: 99.24 %\n",
      "[14,   430] loss: 1310.552, speed: 35.54, accuracy: 99.24 %\n",
      "[14,   440] loss: 1379.573, speed: 35.57, accuracy: 99.22 %\n",
      "[14,   450] loss: 1426.274, speed: 35.58, accuracy: 99.20 %\n",
      "[14,   460] loss: 1470.617, speed: 35.58, accuracy: 99.20 %\n",
      "[14,   470] loss: 1496.692, speed: 35.57, accuracy: 99.20 %\n",
      "[14,   480] loss: 1549.593, speed: 35.57, accuracy: 99.19 %\n",
      "[14,   490] loss: 1596.556, speed: 35.56, accuracy: 99.18 %\n",
      "[14,   500] loss: 1623.947, speed: 35.56, accuracy: 99.18 %\n",
      "[14,   510] loss: 1669.080, speed: 35.58, accuracy: 99.19 %\n",
      "[14,   520] loss: 1701.658, speed: 35.59, accuracy: 99.19 %\n",
      "[14,   530] loss: 1758.773, speed: 35.59, accuracy: 99.18 %\n",
      "[14,   540] loss: 1858.879, speed: 35.60, accuracy: 99.17 %\n",
      "[14,   550] loss: 1895.991, speed: 35.60, accuracy: 99.17 %\n",
      "[14,   560] loss: 1962.214, speed: 35.59, accuracy: 99.16 %\n",
      "[14,   570] loss: 2016.471, speed: 35.59, accuracy: 99.15 %\n",
      "[14,   580] loss: 2060.691, speed: 35.61, accuracy: 99.14 %\n",
      "[14,   590] loss: 2118.061, speed: 35.60, accuracy: 99.14 %\n",
      "[15,     0] loss: 4.180, speed: 32.26, accuracy: 99.00 %\n",
      "[15,    10] loss: 57.368, speed: 34.81, accuracy: 98.64 %\n",
      "[15,    20] loss: 79.261, speed: 35.35, accuracy: 98.90 %\n",
      "[15,    30] loss: 91.145, speed: 35.09, accuracy: 99.13 %\n",
      "[15,    40] loss: 104.786, speed: 35.27, accuracy: 99.20 %\n",
      "[15,    50] loss: 128.691, speed: 35.35, accuracy: 99.20 %\n",
      "[15,    60] loss: 179.771, speed: 35.35, accuracy: 99.13 %\n",
      "[15,    70] loss: 199.150, speed: 35.31, accuracy: 99.20 %\n",
      "[15,    80] loss: 225.786, speed: 35.35, accuracy: 99.22 %\n",
      "[15,    90] loss: 261.760, speed: 35.35, accuracy: 99.22 %\n",
      "[15,   100] loss: 277.534, speed: 35.31, accuracy: 99.23 %\n",
      "[15,   110] loss: 293.742, speed: 35.24, accuracy: 99.24 %\n",
      "[15,   120] loss: 310.109, speed: 35.30, accuracy: 99.23 %\n",
      "[15,   130] loss: 337.545, speed: 35.30, accuracy: 99.23 %\n",
      "[15,   140] loss: 375.404, speed: 35.15, accuracy: 99.23 %\n",
      "[15,   150] loss: 453.818, speed: 35.22, accuracy: 99.20 %\n",
      "[15,   160] loss: 494.272, speed: 35.26, accuracy: 99.20 %\n",
      "[15,   170] loss: 523.345, speed: 35.23, accuracy: 99.22 %\n",
      "[15,   180] loss: 539.108, speed: 35.26, accuracy: 99.24 %\n",
      "[15,   190] loss: 581.976, speed: 35.26, accuracy: 99.26 %\n",
      "[15,   200] loss: 622.448, speed: 35.27, accuracy: 99.25 %\n",
      "[15,   210] loss: 657.856, speed: 35.20, accuracy: 99.25 %\n",
      "[15,   220] loss: 695.746, speed: 35.22, accuracy: 99.24 %\n",
      "[15,   230] loss: 730.013, speed: 35.25, accuracy: 99.25 %\n",
      "[15,   240] loss: 773.590, speed: 35.27, accuracy: 99.24 %\n",
      "[15,   250] loss: 802.631, speed: 35.31, accuracy: 99.24 %\n",
      "[15,   260] loss: 844.322, speed: 35.28, accuracy: 99.23 %\n",
      "[15,   270] loss: 907.383, speed: 35.31, accuracy: 99.21 %\n",
      "[15,   280] loss: 958.770, speed: 35.30, accuracy: 99.18 %\n",
      "[15,   290] loss: 975.924, speed: 35.31, accuracy: 99.18 %\n",
      "[15,   300] loss: 987.230, speed: 35.32, accuracy: 99.20 %\n",
      "[15,   310] loss: 1022.095, speed: 35.29, accuracy: 99.19 %\n",
      "[15,   320] loss: 1068.900, speed: 35.32, accuracy: 99.18 %\n",
      "[15,   330] loss: 1104.446, speed: 35.32, accuracy: 99.18 %\n",
      "[15,   340] loss: 1134.114, speed: 35.29, accuracy: 99.17 %\n",
      "[15,   350] loss: 1159.281, speed: 35.30, accuracy: 99.17 %\n",
      "[15,   360] loss: 1213.913, speed: 35.32, accuracy: 99.16 %\n",
      "[15,   370] loss: 1236.331, speed: 35.33, accuracy: 99.16 %\n",
      "[15,   380] loss: 1274.099, speed: 35.30, accuracy: 99.16 %\n",
      "[15,   390] loss: 1293.631, speed: 35.31, accuracy: 99.17 %\n",
      "[15,   400] loss: 1328.907, speed: 35.32, accuracy: 99.17 %\n",
      "[15,   410] loss: 1360.685, speed: 35.32, accuracy: 99.17 %\n",
      "[15,   420] loss: 1393.169, speed: 35.31, accuracy: 99.18 %\n",
      "[15,   430] loss: 1447.559, speed: 35.32, accuracy: 99.18 %\n",
      "[15,   440] loss: 1538.645, speed: 35.34, accuracy: 99.16 %\n",
      "[15,   450] loss: 1568.448, speed: 35.32, accuracy: 99.16 %\n",
      "[15,   460] loss: 1586.974, speed: 35.32, accuracy: 99.17 %\n",
      "[15,   470] loss: 1612.582, speed: 35.35, accuracy: 99.17 %\n",
      "[15,   480] loss: 1643.802, speed: 35.34, accuracy: 99.17 %\n",
      "[15,   490] loss: 1676.990, speed: 35.32, accuracy: 99.17 %\n",
      "[15,   500] loss: 1705.605, speed: 35.32, accuracy: 99.17 %\n",
      "[15,   510] loss: 1731.427, speed: 35.32, accuracy: 99.17 %\n",
      "[15,   520] loss: 1773.789, speed: 35.31, accuracy: 99.17 %\n",
      "[15,   530] loss: 1827.711, speed: 35.30, accuracy: 99.16 %\n",
      "[15,   540] loss: 1919.025, speed: 35.30, accuracy: 99.16 %\n",
      "[15,   550] loss: 1971.600, speed: 35.28, accuracy: 99.16 %\n",
      "[15,   560] loss: 2015.102, speed: 35.25, accuracy: 99.16 %\n",
      "[15,   570] loss: 2066.534, speed: 35.26, accuracy: 99.15 %\n",
      "[15,   580] loss: 2099.731, speed: 35.28, accuracy: 99.15 %\n",
      "[15,   590] loss: 2117.428, speed: 35.28, accuracy: 99.15 %\n",
      "[16,     0] loss: 0.012, speed: 33.33, accuracy: 100.00 %\n",
      "[16,    10] loss: 10.250, speed: 36.07, accuracy: 99.64 %\n",
      "[16,    20] loss: 42.353, speed: 36.02, accuracy: 99.33 %\n",
      "[16,    30] loss: 68.871, speed: 35.47, accuracy: 99.29 %\n",
      "[16,    40] loss: 93.836, speed: 35.41, accuracy: 99.32 %\n",
      "[16,    50] loss: 131.474, speed: 35.69, accuracy: 99.33 %\n",
      "[16,    60] loss: 147.360, speed: 35.65, accuracy: 99.36 %\n",
      "[16,    70] loss: 211.007, speed: 35.61, accuracy: 99.31 %\n",
      "[16,    80] loss: 260.889, speed: 35.65, accuracy: 99.30 %\n",
      "[16,    90] loss: 273.966, speed: 35.63, accuracy: 99.34 %\n",
      "[16,   100] loss: 291.396, speed: 35.55, accuracy: 99.34 %\n",
      "[16,   110] loss: 328.171, speed: 35.50, accuracy: 99.31 %\n",
      "[16,   120] loss: 372.845, speed: 35.49, accuracy: 99.31 %\n",
      "[16,   130] loss: 398.026, speed: 35.47, accuracy: 99.31 %\n",
      "[16,   140] loss: 446.667, speed: 35.40, accuracy: 99.32 %\n",
      "[16,   150] loss: 484.592, speed: 35.41, accuracy: 99.31 %\n",
      "[16,   160] loss: 528.446, speed: 35.42, accuracy: 99.29 %\n",
      "[16,   170] loss: 561.285, speed: 35.40, accuracy: 99.27 %\n",
      "[16,   180] loss: 605.342, speed: 35.40, accuracy: 99.25 %\n",
      "[16,   190] loss: 643.027, speed: 35.38, accuracy: 99.25 %\n",
      "[16,   200] loss: 666.310, speed: 35.42, accuracy: 99.24 %\n",
      "[16,   210] loss: 715.017, speed: 35.41, accuracy: 99.24 %\n",
      "[16,   220] loss: 737.545, speed: 35.44, accuracy: 99.26 %\n",
      "[16,   230] loss: 826.127, speed: 35.43, accuracy: 99.23 %\n",
      "[16,   240] loss: 850.680, speed: 35.39, accuracy: 99.23 %\n",
      "[16,   250] loss: 872.678, speed: 35.42, accuracy: 99.24 %\n",
      "[16,   260] loss: 906.577, speed: 35.41, accuracy: 99.24 %\n",
      "[16,   270] loss: 942.498, speed: 35.43, accuracy: 99.25 %\n",
      "[16,   280] loss: 964.293, speed: 35.40, accuracy: 99.25 %\n",
      "[16,   290] loss: 995.269, speed: 35.43, accuracy: 99.24 %\n",
      "[16,   300] loss: 1037.741, speed: 35.42, accuracy: 99.24 %\n",
      "[16,   310] loss: 1083.187, speed: 35.42, accuracy: 99.25 %\n",
      "[16,   320] loss: 1110.565, speed: 35.42, accuracy: 99.25 %\n",
      "[16,   330] loss: 1161.038, speed: 35.42, accuracy: 99.25 %\n",
      "[16,   340] loss: 1205.210, speed: 35.42, accuracy: 99.24 %\n",
      "[16,   350] loss: 1220.107, speed: 35.39, accuracy: 99.25 %\n",
      "[16,   360] loss: 1243.722, speed: 35.40, accuracy: 99.24 %\n",
      "[16,   370] loss: 1274.199, speed: 35.43, accuracy: 99.23 %\n",
      "[16,   380] loss: 1322.593, speed: 35.43, accuracy: 99.23 %\n",
      "[16,   390] loss: 1333.961, speed: 35.41, accuracy: 99.24 %\n",
      "[16,   400] loss: 1389.954, speed: 35.40, accuracy: 99.22 %\n",
      "[16,   410] loss: 1427.001, speed: 35.42, accuracy: 99.22 %\n",
      "[16,   420] loss: 1456.439, speed: 35.39, accuracy: 99.22 %\n",
      "[16,   430] loss: 1511.902, speed: 35.41, accuracy: 99.22 %\n",
      "[16,   440] loss: 1546.437, speed: 35.42, accuracy: 99.22 %\n",
      "[16,   450] loss: 1595.362, speed: 35.42, accuracy: 99.22 %\n",
      "[16,   460] loss: 1629.870, speed: 35.41, accuracy: 99.21 %\n",
      "[16,   470] loss: 1708.496, speed: 35.40, accuracy: 99.20 %\n",
      "[16,   480] loss: 1764.143, speed: 35.41, accuracy: 99.20 %\n",
      "[16,   490] loss: 1826.453, speed: 35.39, accuracy: 99.18 %\n",
      "[16,   500] loss: 1907.004, speed: 35.39, accuracy: 99.17 %\n",
      "[16,   510] loss: 1944.886, speed: 35.39, accuracy: 99.15 %\n",
      "[16,   520] loss: 1971.711, speed: 35.37, accuracy: 99.16 %\n",
      "[16,   530] loss: 1992.654, speed: 35.36, accuracy: 99.16 %\n",
      "[16,   540] loss: 2038.175, speed: 35.39, accuracy: 99.16 %\n",
      "[16,   550] loss: 2063.686, speed: 35.42, accuracy: 99.16 %\n",
      "[16,   560] loss: 2078.473, speed: 35.41, accuracy: 99.17 %\n",
      "[16,   570] loss: 2149.732, speed: 35.41, accuracy: 99.15 %\n",
      "[16,   580] loss: 2183.675, speed: 35.42, accuracy: 99.15 %\n",
      "[16,   590] loss: 2235.176, speed: 35.42, accuracy: 99.15 %\n",
      "[17,     0] loss: 0.049, speed: 35.72, accuracy: 100.00 %\n",
      "[17,    10] loss: 61.296, speed: 35.60, accuracy: 99.00 %\n",
      "[17,    20] loss: 77.849, speed: 35.84, accuracy: 99.29 %\n",
      "[17,    30] loss: 105.386, speed: 35.31, accuracy: 99.26 %\n",
      "[17,    40] loss: 137.092, speed: 35.50, accuracy: 99.20 %\n",
      "[17,    50] loss: 167.729, speed: 35.56, accuracy: 99.20 %\n",
      "[17,    60] loss: 207.598, speed: 35.57, accuracy: 99.20 %\n",
      "[17,    70] loss: 230.563, speed: 35.46, accuracy: 99.23 %\n",
      "[17,    80] loss: 250.875, speed: 35.51, accuracy: 99.27 %\n",
      "[17,    90] loss: 258.410, speed: 35.56, accuracy: 99.32 %\n",
      "[17,   100] loss: 270.242, speed: 35.40, accuracy: 99.35 %\n",
      "[17,   110] loss: 277.396, speed: 35.43, accuracy: 99.39 %\n",
      "[17,   120] loss: 293.957, speed: 35.39, accuracy: 99.40 %\n",
      "[17,   130] loss: 310.734, speed: 35.34, accuracy: 99.43 %\n",
      "[17,   140] loss: 340.502, speed: 35.28, accuracy: 99.42 %\n",
      "[17,   150] loss: 341.723, speed: 35.28, accuracy: 99.46 %\n",
      "[17,   160] loss: 360.017, speed: 35.29, accuracy: 99.46 %\n",
      "[17,   170] loss: 394.320, speed: 35.30, accuracy: 99.45 %\n",
      "[17,   180] loss: 401.959, speed: 35.31, accuracy: 99.46 %\n",
      "[17,   190] loss: 415.231, speed: 35.31, accuracy: 99.47 %\n",
      "[17,   200] loss: 435.213, speed: 35.32, accuracy: 99.47 %\n",
      "[17,   210] loss: 478.338, speed: 35.28, accuracy: 99.43 %\n",
      "[17,   220] loss: 496.203, speed: 35.26, accuracy: 99.42 %\n",
      "[17,   230] loss: 531.078, speed: 35.29, accuracy: 99.41 %\n",
      "[17,   240] loss: 555.316, speed: 35.23, accuracy: 99.40 %\n",
      "[17,   250] loss: 591.506, speed: 35.21, accuracy: 99.39 %\n",
      "[17,   260] loss: 644.185, speed: 35.22, accuracy: 99.38 %\n",
      "[17,   270] loss: 654.380, speed: 35.23, accuracy: 99.38 %\n",
      "[17,   280] loss: 669.372, speed: 35.21, accuracy: 99.38 %\n",
      "[17,   290] loss: 691.055, speed: 35.21, accuracy: 99.39 %\n",
      "[17,   300] loss: 717.223, speed: 35.24, accuracy: 99.40 %\n",
      "[17,   310] loss: 745.513, speed: 35.23, accuracy: 99.39 %\n",
      "[17,   320] loss: 792.758, speed: 35.27, accuracy: 99.39 %\n",
      "[17,   330] loss: 806.726, speed: 35.30, accuracy: 99.40 %\n",
      "[17,   340] loss: 831.059, speed: 35.32, accuracy: 99.40 %\n",
      "[17,   350] loss: 850.284, speed: 35.28, accuracy: 99.40 %\n",
      "[17,   360] loss: 876.743, speed: 35.29, accuracy: 99.40 %\n",
      "[17,   370] loss: 907.834, speed: 35.29, accuracy: 99.40 %\n",
      "[17,   380] loss: 940.258, speed: 35.29, accuracy: 99.39 %\n",
      "[17,   390] loss: 963.303, speed: 35.28, accuracy: 99.38 %\n",
      "[17,   400] loss: 1008.391, speed: 35.29, accuracy: 99.38 %\n",
      "[17,   410] loss: 1041.580, speed: 35.30, accuracy: 99.38 %\n",
      "[17,   420] loss: 1064.345, speed: 35.27, accuracy: 99.38 %\n",
      "[17,   430] loss: 1099.293, speed: 35.29, accuracy: 99.37 %\n",
      "[17,   440] loss: 1157.010, speed: 35.30, accuracy: 99.36 %\n",
      "[17,   450] loss: 1183.049, speed: 35.30, accuracy: 99.36 %\n",
      "[17,   460] loss: 1220.580, speed: 35.30, accuracy: 99.35 %\n",
      "[17,   470] loss: 1262.717, speed: 35.33, accuracy: 99.34 %\n",
      "[17,   480] loss: 1302.245, speed: 35.34, accuracy: 99.33 %\n",
      "[17,   490] loss: 1343.652, speed: 35.33, accuracy: 99.33 %\n",
      "[17,   500] loss: 1399.783, speed: 35.33, accuracy: 99.32 %\n",
      "[17,   510] loss: 1455.826, speed: 35.35, accuracy: 99.32 %\n",
      "[17,   520] loss: 1533.935, speed: 35.36, accuracy: 99.30 %\n",
      "[17,   530] loss: 1673.225, speed: 35.35, accuracy: 99.29 %\n",
      "[17,   540] loss: 1686.692, speed: 35.37, accuracy: 99.29 %\n",
      "[17,   550] loss: 1741.512, speed: 35.37, accuracy: 99.29 %\n",
      "[17,   560] loss: 1791.395, speed: 35.36, accuracy: 99.29 %\n",
      "[17,   570] loss: 1867.100, speed: 35.36, accuracy: 99.27 %\n",
      "[17,   580] loss: 1893.452, speed: 35.36, accuracy: 99.27 %\n",
      "[17,   590] loss: 1917.846, speed: 35.37, accuracy: 99.28 %\n",
      "[18,     0] loss: 22.028, speed: 31.25, accuracy: 98.00 %\n",
      "[18,    10] loss: 54.983, speed: 34.92, accuracy: 99.18 %\n",
      "[18,    20] loss: 110.107, speed: 35.06, accuracy: 99.05 %\n",
      "[18,    30] loss: 171.539, speed: 34.95, accuracy: 98.84 %\n",
      "[18,    40] loss: 197.502, speed: 35.25, accuracy: 99.05 %\n",
      "[18,    50] loss: 243.434, speed: 35.47, accuracy: 99.04 %\n",
      "[18,    60] loss: 282.233, speed: 35.34, accuracy: 99.03 %\n",
      "[18,    70] loss: 326.126, speed: 35.31, accuracy: 99.04 %\n",
      "[18,    80] loss: 362.758, speed: 35.45, accuracy: 99.09 %\n",
      "[18,    90] loss: 412.801, speed: 35.56, accuracy: 99.08 %\n",
      "[18,   100] loss: 522.262, speed: 35.53, accuracy: 98.99 %\n",
      "[18,   110] loss: 579.179, speed: 35.55, accuracy: 99.00 %\n",
      "[18,   120] loss: 624.764, speed: 35.50, accuracy: 99.01 %\n",
      "[18,   130] loss: 673.338, speed: 35.51, accuracy: 99.02 %\n",
      "[18,   140] loss: 717.186, speed: 35.42, accuracy: 99.00 %\n",
      "[18,   150] loss: 749.152, speed: 35.47, accuracy: 98.99 %\n",
      "[18,   160] loss: 829.405, speed: 35.48, accuracy: 98.98 %\n",
      "[18,   170] loss: 873.838, speed: 35.44, accuracy: 99.01 %\n",
      "[18,   180] loss: 933.181, speed: 35.43, accuracy: 99.03 %\n",
      "[18,   190] loss: 999.718, speed: 35.48, accuracy: 99.00 %\n",
      "[18,   200] loss: 1010.433, speed: 35.44, accuracy: 99.03 %\n",
      "[18,   210] loss: 1069.226, speed: 35.41, accuracy: 99.02 %\n",
      "[18,   220] loss: 1144.881, speed: 35.41, accuracy: 99.01 %\n",
      "[18,   230] loss: 1179.128, speed: 35.44, accuracy: 99.02 %\n",
      "[18,   240] loss: 1196.884, speed: 35.43, accuracy: 99.03 %\n",
      "[18,   250] loss: 1254.117, speed: 35.39, accuracy: 99.03 %\n",
      "[18,   260] loss: 1266.420, speed: 35.40, accuracy: 99.06 %\n",
      "[18,   270] loss: 1293.156, speed: 35.41, accuracy: 99.07 %\n",
      "[18,   280] loss: 1359.387, speed: 35.36, accuracy: 99.07 %\n",
      "[18,   290] loss: 1376.511, speed: 35.36, accuracy: 99.08 %\n",
      "[18,   300] loss: 1388.029, speed: 35.39, accuracy: 99.10 %\n",
      "[18,   310] loss: 1469.596, speed: 35.39, accuracy: 99.09 %\n",
      "[18,   320] loss: 1494.948, speed: 35.40, accuracy: 99.09 %\n",
      "[18,   330] loss: 1549.994, speed: 35.39, accuracy: 99.08 %\n",
      "[18,   340] loss: 1615.096, speed: 35.40, accuracy: 99.07 %\n",
      "[18,   350] loss: 1675.553, speed: 35.37, accuracy: 99.07 %\n",
      "[18,   360] loss: 1723.844, speed: 35.38, accuracy: 99.06 %\n",
      "[18,   370] loss: 1765.565, speed: 35.41, accuracy: 99.06 %\n",
      "[18,   380] loss: 1830.025, speed: 35.44, accuracy: 99.06 %\n",
      "[18,   390] loss: 1876.831, speed: 35.43, accuracy: 99.07 %\n",
      "[18,   400] loss: 1929.769, speed: 35.42, accuracy: 99.06 %\n",
      "[18,   410] loss: 1955.176, speed: 35.42, accuracy: 99.08 %\n",
      "[18,   420] loss: 2018.125, speed: 35.41, accuracy: 99.08 %\n",
      "[18,   430] loss: 2097.163, speed: 35.40, accuracy: 99.07 %\n",
      "[18,   440] loss: 2163.675, speed: 35.41, accuracy: 99.07 %\n",
      "[18,   450] loss: 2230.947, speed: 35.42, accuracy: 99.06 %\n",
      "[18,   460] loss: 2281.118, speed: 35.42, accuracy: 99.07 %\n",
      "[18,   470] loss: 2325.459, speed: 35.42, accuracy: 99.07 %\n",
      "[18,   480] loss: 2363.376, speed: 35.43, accuracy: 99.07 %\n",
      "[18,   490] loss: 2405.647, speed: 35.45, accuracy: 99.07 %\n",
      "[18,   500] loss: 2465.286, speed: 35.43, accuracy: 99.07 %\n",
      "[18,   510] loss: 2569.032, speed: 35.42, accuracy: 99.06 %\n",
      "[18,   520] loss: 2584.229, speed: 35.43, accuracy: 99.07 %\n",
      "[18,   530] loss: 2624.012, speed: 35.43, accuracy: 99.07 %\n",
      "[18,   540] loss: 2649.562, speed: 35.42, accuracy: 99.08 %\n",
      "[18,   550] loss: 2671.221, speed: 35.42, accuracy: 99.08 %\n",
      "[18,   560] loss: 2726.526, speed: 35.42, accuracy: 99.08 %\n",
      "[18,   570] loss: 2773.771, speed: 35.43, accuracy: 99.08 %\n",
      "[18,   580] loss: 2838.479, speed: 35.40, accuracy: 99.07 %\n",
      "[18,   590] loss: 2867.857, speed: 35.39, accuracy: 99.08 %\n",
      "[19,     0] loss: 0.001, speed: 33.33, accuracy: 100.00 %\n",
      "[19,    10] loss: 38.219, speed: 35.83, accuracy: 99.45 %\n",
      "[19,    20] loss: 56.364, speed: 35.41, accuracy: 99.48 %\n",
      "[19,    30] loss: 74.170, speed: 35.27, accuracy: 99.48 %\n",
      "[19,    40] loss: 102.806, speed: 35.44, accuracy: 99.41 %\n",
      "[19,    50] loss: 126.137, speed: 35.52, accuracy: 99.41 %\n",
      "[19,    60] loss: 145.816, speed: 35.51, accuracy: 99.41 %\n",
      "[19,    70] loss: 170.687, speed: 35.52, accuracy: 99.37 %\n",
      "[19,    80] loss: 192.804, speed: 35.56, accuracy: 99.36 %\n",
      "[19,    90] loss: 219.279, speed: 35.49, accuracy: 99.41 %\n",
      "[19,   100] loss: 248.934, speed: 35.51, accuracy: 99.42 %\n",
      "[19,   110] loss: 253.327, speed: 35.45, accuracy: 99.45 %\n",
      "[19,   120] loss: 276.428, speed: 35.45, accuracy: 99.48 %\n",
      "[19,   130] loss: 314.733, speed: 35.38, accuracy: 99.44 %\n",
      "[19,   140] loss: 338.387, speed: 35.34, accuracy: 99.43 %\n",
      "[19,   150] loss: 367.664, speed: 35.40, accuracy: 99.44 %\n",
      "[19,   160] loss: 417.607, speed: 35.42, accuracy: 99.43 %\n",
      "[19,   170] loss: 494.909, speed: 35.46, accuracy: 99.37 %\n",
      "[19,   180] loss: 543.679, speed: 35.46, accuracy: 99.37 %\n",
      "[19,   190] loss: 579.287, speed: 35.50, accuracy: 99.38 %\n",
      "[19,   200] loss: 636.152, speed: 35.48, accuracy: 99.37 %\n",
      "[19,   210] loss: 677.448, speed: 35.46, accuracy: 99.37 %\n",
      "[19,   220] loss: 725.356, speed: 35.49, accuracy: 99.36 %\n",
      "[19,   230] loss: 746.039, speed: 35.54, accuracy: 99.36 %\n",
      "[19,   240] loss: 759.934, speed: 35.57, accuracy: 99.37 %\n",
      "[19,   250] loss: 763.238, speed: 35.56, accuracy: 99.39 %\n",
      "[19,   260] loss: 778.964, speed: 35.57, accuracy: 99.39 %\n",
      "[19,   270] loss: 797.835, speed: 35.58, accuracy: 99.40 %\n",
      "[19,   280] loss: 812.528, speed: 35.57, accuracy: 99.40 %\n",
      "[19,   290] loss: 846.622, speed: 35.57, accuracy: 99.40 %\n",
      "[19,   300] loss: 860.214, speed: 35.57, accuracy: 99.41 %\n",
      "[19,   310] loss: 874.747, speed: 35.60, accuracy: 99.42 %\n",
      "[19,   320] loss: 971.406, speed: 35.59, accuracy: 99.39 %\n",
      "[19,   330] loss: 1028.123, speed: 35.59, accuracy: 99.38 %\n",
      "[19,   340] loss: 1081.030, speed: 35.59, accuracy: 99.37 %\n",
      "[19,   350] loss: 1121.810, speed: 35.55, accuracy: 99.36 %\n",
      "[19,   360] loss: 1145.274, speed: 35.55, accuracy: 99.36 %\n",
      "[19,   370] loss: 1177.090, speed: 35.55, accuracy: 99.36 %\n",
      "[19,   380] loss: 1219.286, speed: 35.54, accuracy: 99.35 %\n",
      "[19,   390] loss: 1259.332, speed: 35.52, accuracy: 99.34 %\n",
      "[19,   400] loss: 1354.069, speed: 35.54, accuracy: 99.33 %\n",
      "[19,   410] loss: 1385.176, speed: 35.54, accuracy: 99.32 %\n",
      "[19,   420] loss: 1427.254, speed: 35.53, accuracy: 99.32 %\n",
      "[19,   430] loss: 1480.962, speed: 35.51, accuracy: 99.31 %\n",
      "[19,   440] loss: 1512.831, speed: 35.52, accuracy: 99.31 %\n",
      "[19,   450] loss: 1550.813, speed: 35.52, accuracy: 99.30 %\n",
      "[19,   460] loss: 1587.208, speed: 35.52, accuracy: 99.30 %\n",
      "[19,   470] loss: 1643.010, speed: 35.53, accuracy: 99.29 %\n",
      "[19,   480] loss: 1693.890, speed: 35.54, accuracy: 99.29 %\n",
      "[19,   490] loss: 1743.739, speed: 35.55, accuracy: 99.29 %\n",
      "[19,   500] loss: 1758.563, speed: 35.55, accuracy: 99.29 %\n",
      "[19,   510] loss: 1780.545, speed: 35.54, accuracy: 99.29 %\n",
      "[19,   520] loss: 1798.035, speed: 35.54, accuracy: 99.30 %\n",
      "[19,   530] loss: 1815.644, speed: 35.52, accuracy: 99.31 %\n",
      "[19,   540] loss: 1838.529, speed: 35.52, accuracy: 99.31 %\n",
      "[19,   550] loss: 1855.305, speed: 35.52, accuracy: 99.31 %\n",
      "[19,   560] loss: 1865.998, speed: 35.51, accuracy: 99.32 %\n",
      "[19,   570] loss: 1871.876, speed: 35.51, accuracy: 99.32 %\n",
      "[19,   580] loss: 1913.329, speed: 35.51, accuracy: 99.32 %\n",
      "[19,   590] loss: 1950.765, speed: 35.52, accuracy: 99.32 %\n",
      "[20,     0] loss: 0.002, speed: 33.33, accuracy: 100.00 %\n",
      "[20,    10] loss: 36.460, speed: 36.23, accuracy: 99.27 %\n",
      "[20,    20] loss: 54.059, speed: 36.23, accuracy: 99.43 %\n",
      "[20,    30] loss: 71.517, speed: 36.02, accuracy: 99.52 %\n",
      "[20,    40] loss: 95.347, speed: 35.73, accuracy: 99.56 %\n",
      "[20,    50] loss: 100.808, speed: 35.77, accuracy: 99.61 %\n",
      "[20,    60] loss: 117.139, speed: 35.72, accuracy: 99.59 %\n",
      "[20,    70] loss: 163.794, speed: 35.51, accuracy: 99.54 %\n",
      "[20,    80] loss: 199.805, speed: 35.47, accuracy: 99.47 %\n",
      "[20,    90] loss: 244.683, speed: 35.52, accuracy: 99.41 %\n",
      "[20,   100] loss: 290.501, speed: 35.54, accuracy: 99.40 %\n",
      "[20,   110] loss: 346.721, speed: 35.53, accuracy: 99.38 %\n",
      "[20,   120] loss: 381.493, speed: 35.53, accuracy: 99.36 %\n",
      "[20,   130] loss: 471.409, speed: 35.57, accuracy: 99.31 %\n",
      "[20,   140] loss: 492.463, speed: 35.57, accuracy: 99.33 %\n",
      "[20,   150] loss: 526.457, speed: 35.58, accuracy: 99.34 %\n",
      "[20,   160] loss: 549.342, speed: 35.60, accuracy: 99.34 %\n",
      "[20,   170] loss: 573.549, speed: 35.60, accuracy: 99.33 %\n",
      "[20,   180] loss: 609.061, speed: 35.56, accuracy: 99.33 %\n",
      "[20,   190] loss: 627.328, speed: 35.57, accuracy: 99.34 %\n",
      "[20,   200] loss: 649.609, speed: 35.56, accuracy: 99.34 %\n",
      "[20,   210] loss: 672.067, speed: 35.55, accuracy: 99.34 %\n",
      "[20,   220] loss: 733.387, speed: 35.55, accuracy: 99.33 %\n",
      "[20,   230] loss: 792.851, speed: 35.56, accuracy: 99.32 %\n",
      "[20,   240] loss: 828.027, speed: 35.56, accuracy: 99.31 %\n",
      "[20,   250] loss: 855.138, speed: 35.54, accuracy: 99.31 %\n",
      "[20,   260] loss: 873.280, speed: 35.58, accuracy: 99.32 %\n",
      "[20,   270] loss: 903.235, speed: 35.61, accuracy: 99.32 %\n",
      "[20,   280] loss: 945.710, speed: 35.60, accuracy: 99.33 %\n",
      "[20,   290] loss: 1000.092, speed: 35.54, accuracy: 99.32 %\n",
      "[20,   300] loss: 1045.895, speed: 35.52, accuracy: 99.32 %\n",
      "[20,   310] loss: 1075.191, speed: 35.54, accuracy: 99.33 %\n",
      "[20,   320] loss: 1138.325, speed: 35.51, accuracy: 99.32 %\n",
      "[20,   330] loss: 1149.186, speed: 35.48, accuracy: 99.33 %\n",
      "[20,   340] loss: 1196.836, speed: 35.48, accuracy: 99.33 %\n",
      "[20,   350] loss: 1215.710, speed: 35.51, accuracy: 99.33 %\n",
      "[20,   360] loss: 1259.027, speed: 35.51, accuracy: 99.32 %\n",
      "[20,   370] loss: 1308.365, speed: 35.50, accuracy: 99.32 %\n",
      "[20,   380] loss: 1338.819, speed: 35.48, accuracy: 99.33 %\n",
      "[20,   390] loss: 1357.182, speed: 35.49, accuracy: 99.34 %\n",
      "[20,   400] loss: 1442.205, speed: 35.49, accuracy: 99.32 %\n",
      "[20,   410] loss: 1515.986, speed: 35.48, accuracy: 99.31 %\n",
      "[20,   420] loss: 1556.156, speed: 35.47, accuracy: 99.29 %\n",
      "[20,   430] loss: 1594.708, speed: 35.44, accuracy: 99.30 %\n",
      "[20,   440] loss: 1621.905, speed: 35.41, accuracy: 99.30 %\n",
      "[20,   450] loss: 1643.445, speed: 35.39, accuracy: 99.30 %\n",
      "[20,   460] loss: 1678.524, speed: 35.38, accuracy: 99.31 %\n",
      "[20,   470] loss: 1725.210, speed: 35.38, accuracy: 99.30 %\n",
      "[20,   480] loss: 1747.173, speed: 35.38, accuracy: 99.30 %\n",
      "[20,   490] loss: 1801.411, speed: 35.38, accuracy: 99.30 %\n",
      "[20,   500] loss: 1822.636, speed: 35.37, accuracy: 99.30 %\n",
      "[20,   510] loss: 1855.916, speed: 35.36, accuracy: 99.30 %\n",
      "[20,   520] loss: 1923.105, speed: 35.36, accuracy: 99.30 %\n",
      "[20,   530] loss: 1977.525, speed: 35.35, accuracy: 99.29 %\n",
      "[20,   540] loss: 2014.435, speed: 35.36, accuracy: 99.28 %\n",
      "[20,   550] loss: 2036.785, speed: 35.37, accuracy: 99.28 %\n",
      "[20,   560] loss: 2078.883, speed: 35.36, accuracy: 99.29 %\n",
      "[20,   570] loss: 2143.555, speed: 35.35, accuracy: 99.27 %\n",
      "[20,   580] loss: 2181.193, speed: 35.35, accuracy: 99.28 %\n",
      "[20,   590] loss: 2221.839, speed: 35.35, accuracy: 99.27 %\n"
     ]
    }
   ],
   "source": [
    "# Training Model\n",
    "import time\n",
    "import numpy as np\n",
    "from torch.autograd import Variable\n",
    "\n",
    "class trainModel():\n",
    "    def __init__(self):\n",
    "        self.train_loss = 0.0\n",
    "        self.total = 0\n",
    "        self.correct = 0\n",
    "        self.start_time = 0\n",
    "        self.trainLossArray = []\n",
    "        self.TrainAccuracyArray = []\n",
    "        self.epochArray = []\n",
    "\n",
    "    def updateModel(self, data):\n",
    "        # deconstructing input\n",
    "        inputs = data[0]\n",
    "        labels = data[1]\n",
    "\n",
    "        # inputs = inputs.squeeze(1)\n",
    "        # print(inputs.size())\n",
    "        # Forward Pass\n",
    "        output = predictionModel(Variable(inputs))\n",
    "        # print(labels)\n",
    "        labels = labels.squeeze(1)\n",
    "        # print(labels)\n",
    "\n",
    "        # print(len(output))\n",
    "        # Find the Loss\n",
    "        loss = criterion(output[0],labels)\n",
    "\n",
    "        # Clear the gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Calculate gradients\n",
    "        loss.backward()\n",
    "\n",
    "        # Update Weights\n",
    "        optimizer.step()\n",
    "\n",
    "        # Calculate Loss\n",
    "        self.train_loss +=loss.item() * labels.size(0)\n",
    "\n",
    "        _, predicted = torch.max(output[0], 1)\n",
    "        self.total += labels.size(0)\n",
    "        self.correct += (predicted == labels).sum().item()\n",
    "        accuracy = 100 * self.correct / self.total\n",
    "        return accuracy, predicted\n",
    "\n",
    "    def training(self, epochs):\n",
    "        for epoch in range(epochs):\n",
    "            self.train_loss = 0.0\n",
    "            self.total = 0\n",
    "            self.correct = 0\n",
    "            self.start_time = time.time()\n",
    "            for i, data in enumerate(trainloader):\n",
    "                # train and update model\n",
    "                accuracy, predicted = self.updateModel(data)\n",
    "\n",
    "                # print every 10 batches\n",
    "                if i % 10 == 0:    \n",
    "                    batch_time = time.time()\n",
    "                    speed = (i+1)/(batch_time-self.start_time)\n",
    "                    print('[%d, %5d] loss: %.3f, speed: %.2f, accuracy: %.2f %%' % (epoch + 1, i, self.train_loss, speed, accuracy))\n",
    "\n",
    "            predictionModel.eval()    \n",
    "            self.TrainAccuracyArray.append(accuracy)\n",
    "            self.trainLossArray.append(self.train_loss)\n",
    "            self.epochArray.append(epoch)\n",
    "        return\n",
    "\n",
    "    def graphs(self):\n",
    "        import matplotlib.pyplot as plt\n",
    "        # plot the training graphs\n",
    "        figure, axis = plt.subplots(2, 2)\n",
    "        axis[0,0].plot(self.epochArray,self.trainLossArray)\n",
    "        axis[0,0].set_title(\"Training Loss\")\n",
    "\n",
    "\n",
    "        axis[1,0].plot(self.epochArray,self.TrainAccuracyArray)\n",
    "        axis[1,0].set_title(\"Training Accuracy\")\n",
    "\n",
    "        plt.show()\n",
    "        return\n",
    "\n",
    "# Training parameters\n",
    "epochs = 20\n",
    "model = trainModel()\n",
    "model.training(epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Step [100/600], Loss: 0.0631, Accuracy: 10.5200\n",
      "tensor([6, 5, 4, 9, 9, 9, 1, 6, 6, 4, 5, 7, 3, 2, 5, 2, 1, 4, 3, 7, 2, 2, 4, 6,\n",
      "        1, 9, 8, 3, 6, 2, 4, 7, 3, 4, 4, 3, 9, 9, 8, 5, 3, 7, 9, 2, 1, 6, 8, 5,\n",
      "        0, 0, 8, 4, 5, 7, 0, 4, 9, 6, 4, 8, 8, 6, 7, 0, 9, 1, 6, 8, 7, 5, 0, 3,\n",
      "        9, 1, 9, 2, 1, 9, 8, 1, 5, 3, 9, 7, 4, 8, 7, 9, 0, 1, 9, 5, 7, 8, 0, 6,\n",
      "        1, 0, 5, 9])\n",
      "Epoch [1/10], Step [200/600], Loss: 0.0168, Accuracy: 11.6600\n",
      "tensor([6, 8, 5, 6, 5, 6, 8, 9, 6, 6, 7, 1, 3, 8, 4, 4, 9, 5, 2, 6, 0, 6, 1, 3,\n",
      "        6, 1, 3, 4, 9, 3, 8, 7, 7, 3, 6, 8, 5, 8, 1, 3, 9, 8, 7, 6, 0, 2, 2, 1,\n",
      "        1, 5, 7, 0, 2, 2, 3, 3, 7, 3, 9, 8, 7, 4, 0, 3, 2, 7, 2, 8, 1, 2, 2, 2,\n",
      "        4, 3, 3, 8, 2, 7, 6, 7, 2, 9, 2, 6, 6, 6, 8, 3, 1, 3, 1, 2, 6, 7, 7, 6,\n",
      "        1, 2, 9, 0])\n",
      "Epoch [1/10], Step [300/600], Loss: 0.0233, Accuracy: 10.5800\n",
      "tensor([3, 1, 3, 5, 5, 7, 5, 4, 5, 0, 5, 0, 0, 3, 0, 8, 7, 2, 2, 0, 8, 6, 8, 2,\n",
      "        2, 7, 3, 1, 3, 0, 1, 3, 1, 9, 3, 3, 8, 3, 5, 6, 2, 1, 6, 5, 1, 3, 6, 3,\n",
      "        8, 0, 2, 0, 7, 9, 4, 5, 9, 2, 7, 5, 2, 2, 6, 5, 9, 1, 3, 8, 9, 6, 6, 2,\n",
      "        2, 4, 9, 5, 5, 5, 3, 3, 4, 7, 6, 8, 4, 6, 4, 2, 1, 4, 4, 4, 1, 6, 2, 7,\n",
      "        8, 7, 8, 7])\n",
      "Epoch [1/10], Step [400/600], Loss: 0.1022, Accuracy: 10.9400\n",
      "tensor([4, 0, 0, 2, 2, 5, 0, 7, 3, 7, 1, 5, 8, 3, 9, 8, 1, 2, 2, 3, 9, 9, 8, 0,\n",
      "        7, 1, 4, 7, 5, 5, 4, 6, 2, 8, 9, 8, 5, 3, 5, 3, 4, 0, 2, 3, 3, 5, 9, 8,\n",
      "        4, 5, 7, 7, 9, 6, 3, 5, 8, 5, 9, 5, 5, 7, 9, 8, 6, 4, 5, 6, 6, 3, 1, 4,\n",
      "        7, 6, 9, 3, 4, 1, 2, 5, 5, 4, 0, 2, 5, 0, 4, 6, 7, 1, 3, 1, 1, 4, 9, 4,\n",
      "        1, 5, 9, 5])\n",
      "Epoch [1/10], Step [500/600], Loss: 0.0030, Accuracy: 10.3600\n",
      "tensor([3, 2, 2, 6, 2, 9, 3, 6, 7, 5, 0, 2, 1, 8, 2, 6, 5, 6, 6, 3, 7, 3, 9, 8,\n",
      "        7, 7, 1, 4, 6, 7, 9, 6, 0, 5, 0, 7, 5, 4, 5, 4, 3, 1, 2, 3, 8, 7, 5, 0,\n",
      "        3, 2, 5, 4, 6, 5, 0, 9, 2, 0, 1, 8, 5, 5, 1, 2, 4, 0, 8, 3, 0, 4, 6, 3,\n",
      "        6, 7, 4, 1, 8, 7, 4, 6, 7, 4, 7, 5, 5, 4, 0, 3, 8, 1, 9, 7, 7, 0, 8, 9,\n",
      "        4, 1, 9, 6])\n",
      "Epoch [1/10], Step [600/600], Loss: 0.0774, Accuracy: 10.4500\n",
      "tensor([6, 3, 5, 2, 7, 7, 1, 5, 0, 1, 7, 2, 9, 5, 3, 4, 3, 0, 8, 1, 0, 2, 3, 6,\n",
      "        9, 8, 9, 7, 7, 5, 1, 9, 6, 9, 6, 9, 5, 1, 4, 2, 7, 8, 0, 1, 1, 1, 7, 8,\n",
      "        0, 5, 5, 5, 6, 4, 4, 5, 4, 3, 5, 5, 0, 5, 2, 1, 8, 9, 8, 1, 3, 7, 4, 8,\n",
      "        0, 5, 0, 1, 8, 2, 6, 7, 7, 5, 3, 2, 8, 1, 2, 3, 6, 9, 3, 9, 3, 9, 6, 7,\n",
      "        5, 0, 3, 1])\n",
      "Epoch [2/10], Step [100/600], Loss: 0.0183, Accuracy: 10.6500\n",
      "tensor([0, 3, 1, 9, 8, 0, 5, 7, 7, 2, 8, 2, 2, 0, 4, 6, 2, 8, 3, 1, 5, 2, 1, 1,\n",
      "        0, 8, 5, 6, 6, 6, 9, 6, 3, 9, 8, 5, 7, 8, 5, 4, 8, 2, 6, 4, 7, 1, 9, 6,\n",
      "        5, 4, 0, 7, 8, 5, 2, 2, 1, 0, 8, 0, 6, 7, 3, 7, 1, 4, 8, 7, 7, 9, 3, 5,\n",
      "        2, 7, 6, 6, 4, 0, 8, 9, 6, 7, 1, 7, 1, 1, 0, 5, 9, 5, 0, 1, 1, 0, 0, 4,\n",
      "        0, 5, 4, 7])\n",
      "Epoch [2/10], Step [200/600], Loss: 0.0052, Accuracy: 11.2400\n",
      "tensor([7, 8, 8, 8, 3, 6, 0, 6, 6, 5, 1, 1, 2, 0, 0, 7, 8, 8, 8, 5, 9, 5, 9, 3,\n",
      "        8, 7, 2, 3, 7, 3, 4, 6, 6, 3, 6, 3, 2, 8, 5, 9, 6, 4, 1, 4, 3, 1, 1, 8,\n",
      "        5, 0, 4, 8, 6, 7, 3, 9, 8, 6, 9, 2, 2, 4, 9, 3, 7, 5, 3, 9, 2, 3, 0, 4,\n",
      "        9, 3, 6, 2, 3, 7, 7, 9, 2, 9, 1, 3, 8, 5, 8, 2, 9, 6, 2, 8, 8, 1, 5, 6,\n",
      "        1, 3, 3, 6])\n",
      "Epoch [2/10], Step [300/600], Loss: 0.0305, Accuracy: 11.6200\n",
      "tensor([8, 8, 1, 1, 1, 9, 9, 8, 2, 8, 1, 6, 8, 3, 1, 0, 1, 0, 2, 7, 2, 1, 8, 1,\n",
      "        1, 6, 7, 4, 1, 7, 2, 2, 4, 4, 1, 4, 4, 3, 4, 5, 7, 8, 7, 4, 7, 1, 4, 3,\n",
      "        7, 8, 5, 7, 8, 1, 9, 4, 8, 3, 0, 9, 2, 4, 3, 3, 2, 2, 6, 0, 1, 6, 0, 8,\n",
      "        7, 9, 2, 4, 7, 7, 4, 2, 6, 7, 3, 4, 3, 7, 3, 2, 6, 6, 3, 4, 6, 5, 2, 2,\n",
      "        4, 3, 9, 7])\n",
      "Epoch [2/10], Step [400/600], Loss: 0.0345, Accuracy: 11.6500\n",
      "tensor([4, 0, 3, 8, 1, 8, 1, 1, 4, 7, 7, 2, 6, 5, 0, 7, 8, 6, 7, 7, 5, 4, 8, 9,\n",
      "        5, 0, 7, 6, 0, 0, 7, 9, 0, 8, 4, 6, 0, 3, 0, 9, 8, 4, 7, 8, 2, 3, 6, 3,\n",
      "        7, 3, 4, 8, 1, 2, 0, 3, 3, 6, 6, 8, 6, 7, 0, 8, 4, 8, 6, 9, 9, 6, 6, 4,\n",
      "        7, 2, 8, 6, 8, 5, 9, 1, 0, 4, 4, 7, 4, 2, 7, 8, 3, 9, 5, 0, 8, 7, 8, 7,\n",
      "        0, 1, 7, 2])\n",
      "Epoch [2/10], Step [500/600], Loss: 0.0130, Accuracy: 10.4800\n",
      "tensor([6, 6, 1, 3, 4, 5, 5, 9, 1, 9, 4, 6, 3, 8, 8, 6, 6, 7, 7, 9, 8, 3, 0, 1,\n",
      "        5, 8, 7, 0, 8, 1, 7, 1, 2, 4, 0, 2, 2, 8, 8, 3, 3, 3, 9, 4, 2, 4, 8, 1,\n",
      "        1, 1, 1, 4, 3, 0, 9, 2, 5, 7, 0, 7, 3, 6, 8, 9, 6, 9, 8, 2, 7, 8, 5, 7,\n",
      "        4, 0, 7, 6, 3, 0, 9, 1, 7, 6, 6, 4, 7, 8, 9, 4, 1, 1, 2, 0, 4, 9, 2, 8,\n",
      "        2, 4, 4, 2])\n",
      "Epoch [2/10], Step [600/600], Loss: 0.1090, Accuracy: 11.2600\n",
      "tensor([5, 2, 8, 3, 5, 8, 3, 9, 0, 7, 2, 5, 0, 2, 9, 5, 6, 2, 0, 7, 1, 3, 9, 8,\n",
      "        9, 7, 5, 4, 7, 2, 9, 1, 2, 5, 5, 4, 1, 1, 9, 0, 5, 5, 1, 8, 7, 9, 0, 3,\n",
      "        0, 6, 4, 5, 9, 0, 8, 8, 0, 2, 0, 2, 6, 3, 0, 1, 5, 1, 3, 5, 3, 9, 3, 7,\n",
      "        3, 2, 4, 7, 3, 9, 4, 0, 2, 9, 3, 9, 4, 2, 4, 3, 2, 3, 6, 4, 7, 2, 2, 4,\n",
      "        2, 9, 4, 4])\n",
      "Epoch [3/10], Step [100/600], Loss: 0.0120, Accuracy: 11.4200\n",
      "tensor([0, 0, 6, 7, 1, 5, 6, 1, 1, 7, 4, 6, 1, 5, 3, 8, 2, 7, 4, 6, 1, 1, 9, 4,\n",
      "        5, 8, 7, 3, 7, 1, 7, 8, 6, 2, 1, 7, 7, 9, 5, 6, 6, 3, 9, 0, 6, 9, 3, 6,\n",
      "        5, 8, 8, 1, 0, 1, 1, 3, 1, 4, 6, 4, 7, 4, 7, 8, 5, 0, 5, 5, 8, 1, 4, 1,\n",
      "        9, 1, 8, 6, 2, 6, 5, 7, 1, 0, 3, 3, 9, 0, 8, 3, 7, 0, 3, 3, 0, 6, 3, 4,\n",
      "        3, 5, 1, 7])\n",
      "Epoch [3/10], Step [200/600], Loss: 0.0669, Accuracy: 11.3100\n",
      "tensor([4, 6, 7, 4, 3, 0, 5, 6, 5, 9, 7, 3, 9, 7, 5, 5, 0, 7, 2, 6, 1, 4, 2, 5,\n",
      "        9, 7, 3, 6, 4, 6, 1, 6, 7, 1, 8, 3, 9, 9, 6, 9, 3, 0, 9, 5, 6, 0, 6, 6,\n",
      "        7, 2, 8, 5, 2, 1, 9, 2, 6, 7, 3, 9, 7, 2, 9, 9, 5, 4, 3, 4, 9, 8, 7, 9,\n",
      "        9, 5, 8, 1, 3, 0, 1, 5, 6, 8, 1, 1, 7, 5, 3, 6, 0, 5, 6, 7, 6, 0, 3, 3,\n",
      "        9, 6, 2, 1])\n",
      "Epoch [3/10], Step [300/600], Loss: 0.0053, Accuracy: 10.4400\n",
      "tensor([8, 1, 3, 8, 8, 1, 9, 9, 2, 9, 0, 6, 7, 0, 2, 4, 2, 3, 7, 1, 3, 4, 2, 0,\n",
      "        4, 0, 2, 2, 6, 7, 9, 4, 4, 9, 1, 8, 5, 4, 4, 6, 7, 2, 2, 7, 2, 3, 0, 6,\n",
      "        7, 5, 4, 4, 6, 0, 3, 8, 5, 0, 5, 9, 0, 3, 6, 6, 4, 9, 0, 0, 3, 2, 6, 2,\n",
      "        7, 3, 6, 0, 3, 7, 5, 1, 8, 7, 4, 1, 8, 3, 2, 8, 4, 1, 7, 2, 1, 3, 9, 9,\n",
      "        6, 5, 6, 6])\n",
      "Epoch [3/10], Step [400/600], Loss: 0.0312, Accuracy: 10.3200\n",
      "tensor([4, 6, 9, 6, 4, 8, 7, 7, 7, 6, 9, 0, 4, 8, 1, 7, 9, 1, 4, 9, 4, 4, 3, 2,\n",
      "        2, 5, 3, 6, 2, 3, 8, 5, 7, 3, 8, 9, 9, 5, 2, 4, 6, 8, 6, 4, 7, 0, 2, 2,\n",
      "        8, 1, 3, 1, 2, 6, 5, 0, 1, 2, 3, 8, 5, 5, 2, 8, 6, 7, 1, 7, 9, 7, 8, 3,\n",
      "        7, 1, 4, 3, 4, 0, 8, 2, 9, 0, 0, 2, 1, 2, 2, 8, 1, 0, 8, 5, 2, 4, 1, 9,\n",
      "        5, 4, 1, 0])\n",
      "Epoch [3/10], Step [500/600], Loss: 0.0166, Accuracy: 10.9300\n",
      "tensor([1, 5, 1, 7, 8, 0, 1, 2, 2, 7, 7, 9, 1, 5, 4, 8, 7, 5, 9, 3, 0, 3, 7, 0,\n",
      "        1, 4, 8, 4, 9, 2, 8, 7, 1, 2, 0, 5, 5, 9, 1, 2, 7, 5, 0, 7, 4, 4, 2, 3,\n",
      "        2, 2, 9, 2, 4, 8, 7, 5, 1, 7, 7, 5, 5, 0, 5, 6, 3, 3, 8, 4, 6, 5, 9, 2,\n",
      "        6, 1, 4, 1, 2, 3, 5, 8, 1, 7, 8, 8, 2, 1, 5, 6, 8, 4, 6, 2, 2, 6, 9, 3,\n",
      "        7, 9, 4, 5])\n",
      "Epoch [3/10], Step [600/600], Loss: 0.2088, Accuracy: 10.6300\n",
      "tensor([5, 9, 1, 5, 1, 8, 0, 9, 1, 2, 8, 2, 3, 3, 6, 7, 8, 5, 3, 9, 0, 4, 3, 1,\n",
      "        8, 7, 6, 9, 1, 0, 8, 1, 4, 3, 3, 8, 9, 7, 9, 7, 3, 0, 9, 1, 4, 7, 9, 2,\n",
      "        3, 2, 8, 1, 3, 2, 5, 7, 8, 3, 9, 8, 1, 2, 8, 6, 4, 0, 2, 7, 7, 6, 3, 2,\n",
      "        6, 7, 9, 1, 8, 5, 4, 4, 8, 0, 3, 0, 6, 6, 9, 6, 8, 9, 1, 5, 8, 3, 0, 7,\n",
      "        1, 9, 6, 0])\n",
      "Epoch [4/10], Step [100/600], Loss: 0.0005, Accuracy: 10.2400\n",
      "tensor([0, 0, 6, 9, 8, 0, 4, 8, 4, 9, 9, 0, 5, 0, 3, 0, 4, 5, 7, 4, 1, 6, 9, 4,\n",
      "        6, 9, 3, 5, 2, 9, 8, 6, 1, 3, 8, 4, 8, 2, 7, 9, 3, 3, 5, 4, 0, 1, 8, 2,\n",
      "        2, 3, 9, 5, 6, 9, 4, 9, 6, 2, 1, 7, 4, 7, 7, 5, 1, 1, 8, 5, 8, 2, 0, 9,\n",
      "        0, 2, 5, 3, 9, 7, 1, 2, 1, 7, 6, 9, 5, 8, 7, 1, 3, 8, 4, 5, 4, 7, 5, 5,\n",
      "        0, 1, 6, 2])\n",
      "Epoch [4/10], Step [200/600], Loss: 0.0353, Accuracy: 10.4700\n",
      "tensor([9, 3, 9, 2, 0, 4, 7, 1, 4, 3, 6, 4, 4, 2, 9, 1, 4, 0, 0, 6, 7, 1, 0, 0,\n",
      "        7, 7, 6, 7, 1, 8, 0, 5, 9, 4, 9, 5, 3, 2, 4, 9, 8, 1, 9, 5, 8, 8, 6, 0,\n",
      "        4, 8, 3, 1, 7, 8, 0, 3, 7, 1, 8, 1, 6, 2, 9, 8, 4, 2, 2, 7, 9, 8, 8, 1,\n",
      "        2, 5, 0, 3, 4, 5, 9, 3, 3, 4, 7, 7, 4, 3, 8, 6, 6, 4, 8, 5, 6, 5, 3, 1,\n",
      "        3, 9, 9, 4])\n",
      "Epoch [4/10], Step [300/600], Loss: 0.0067, Accuracy: 11.1800\n",
      "tensor([2, 6, 4, 4, 8, 9, 4, 3, 7, 4, 4, 0, 3, 8, 3, 4, 5, 7, 5, 6, 7, 2, 6, 4,\n",
      "        3, 7, 2, 9, 6, 6, 3, 6, 3, 7, 9, 2, 1, 7, 9, 0, 8, 9, 4, 8, 4, 5, 6, 1,\n",
      "        6, 1, 1, 1, 1, 2, 4, 4, 8, 1, 2, 0, 3, 4, 5, 1, 9, 1, 3, 0, 4, 1, 4, 6,\n",
      "        8, 7, 6, 6, 2, 8, 0, 3, 4, 4, 8, 6, 6, 5, 6, 0, 1, 0, 2, 0, 8, 3, 6, 7,\n",
      "        4, 3, 9, 9])\n",
      "Epoch [4/10], Step [400/600], Loss: 0.0075, Accuracy: 10.5400\n",
      "tensor([4, 5, 4, 4, 9, 9, 8, 4, 9, 2, 8, 1, 0, 4, 1, 4, 4, 2, 6, 7, 1, 8, 8, 6,\n",
      "        1, 5, 5, 9, 1, 4, 4, 0, 7, 9, 7, 5, 4, 5, 9, 2, 5, 3, 3, 3, 0, 5, 0, 2,\n",
      "        7, 6, 1, 1, 4, 8, 2, 2, 5, 6, 3, 5, 5, 0, 1, 2, 0, 1, 9, 2, 3, 9, 2, 3,\n",
      "        8, 3, 7, 0, 2, 1, 8, 7, 9, 5, 2, 8, 0, 8, 9, 5, 2, 0, 3, 7, 7, 2, 0, 6,\n",
      "        5, 0, 1, 8])\n",
      "Epoch [4/10], Step [500/600], Loss: 0.0115, Accuracy: 10.7100\n",
      "tensor([1, 5, 4, 3, 7, 3, 5, 0, 3, 6, 3, 2, 8, 6, 3, 4, 0, 0, 1, 8, 9, 5, 1, 5,\n",
      "        0, 9, 9, 4, 2, 6, 7, 0, 8, 2, 9, 3, 7, 2, 3, 6, 1, 5, 1, 2, 2, 0, 2, 7,\n",
      "        0, 9, 2, 1, 9, 0, 0, 3, 3, 7, 2, 7, 2, 8, 8, 5, 9, 9, 5, 1, 6, 5, 7, 1,\n",
      "        7, 8, 9, 9, 1, 9, 9, 8, 5, 0, 0, 1, 4, 5, 6, 1, 8, 1, 3, 9, 6, 7, 6, 2,\n",
      "        8, 2, 9, 1])\n",
      "Epoch [4/10], Step [600/600], Loss: 0.0025, Accuracy: 10.8600\n",
      "tensor([1, 3, 2, 1, 3, 4, 4, 4, 3, 5, 1, 9, 8, 2, 0, 2, 6, 4, 1, 6, 7, 9, 5, 8,\n",
      "        3, 4, 2, 0, 1, 9, 6, 1, 6, 0, 8, 1, 9, 7, 7, 7, 2, 0, 6, 3, 4, 2, 8, 9,\n",
      "        2, 6, 1, 9, 0, 7, 8, 8, 4, 4, 1, 1, 0, 3, 2, 6, 7, 8, 2, 6, 6, 7, 6, 6,\n",
      "        6, 3, 2, 7, 9, 0, 5, 3, 5, 4, 3, 6, 3, 2, 5, 7, 1, 6, 8, 6, 1, 1, 8, 7,\n",
      "        7, 1, 2, 0])\n",
      "Epoch [5/10], Step [100/600], Loss: 0.0492, Accuracy: 10.3500\n",
      "tensor([3, 3, 6, 1, 7, 8, 5, 4, 2, 9, 7, 1, 9, 0, 9, 1, 5, 1, 4, 0, 2, 0, 2, 9,\n",
      "        5, 3, 2, 2, 5, 5, 6, 9, 2, 3, 2, 8, 4, 1, 7, 9, 8, 8, 6, 3, 2, 5, 7, 4,\n",
      "        5, 8, 2, 5, 9, 3, 8, 4, 4, 5, 5, 9, 1, 6, 0, 8, 5, 0, 6, 5, 7, 8, 1, 0,\n",
      "        0, 8, 7, 6, 2, 6, 6, 9, 2, 3, 0, 4, 3, 5, 1, 9, 4, 5, 2, 0, 2, 7, 3, 9,\n",
      "        3, 4, 7, 1])\n",
      "Epoch [5/10], Step [200/600], Loss: 0.0110, Accuracy: 10.7400\n",
      "tensor([9, 1, 2, 4, 1, 6, 2, 1, 5, 7, 4, 9, 2, 8, 1, 5, 6, 6, 0, 9, 0, 1, 6, 4,\n",
      "        8, 1, 2, 3, 3, 4, 6, 6, 1, 7, 9, 9, 9, 8, 1, 9, 2, 5, 4, 8, 6, 0, 3, 7,\n",
      "        3, 5, 8, 6, 3, 9, 1, 2, 0, 6, 8, 1, 3, 3, 4, 1, 0, 5, 6, 9, 1, 2, 9, 3,\n",
      "        4, 7, 3, 3, 1, 5, 0, 0, 2, 8, 2, 2, 3, 4, 0, 7, 6, 4, 8, 4, 6, 9, 3, 5,\n",
      "        2, 1, 4, 3])\n",
      "Epoch [5/10], Step [300/600], Loss: 0.0736, Accuracy: 10.4900\n",
      "tensor([0, 6, 4, 7, 4, 3, 8, 6, 0, 5, 9, 1, 8, 8, 8, 1, 2, 3, 7, 3, 3, 5, 9, 6,\n",
      "        4, 6, 8, 3, 7, 9, 7, 5, 7, 2, 7, 3, 3, 5, 5, 1, 1, 7, 5, 2, 4, 3, 3, 2,\n",
      "        0, 4, 4, 3, 2, 0, 9, 1, 9, 0, 1, 4, 5, 7, 7, 1, 4, 7, 6, 0, 0, 8, 4, 8,\n",
      "        1, 3, 2, 7, 2, 7, 2, 4, 9, 8, 0, 5, 8, 9, 7, 8, 9, 2, 0, 3, 9, 2, 5, 1,\n",
      "        7, 3, 2, 1])\n",
      "Epoch [5/10], Step [400/600], Loss: 0.0964, Accuracy: 11.3300\n",
      "tensor([1, 6, 8, 9, 1, 3, 6, 5, 8, 8, 2, 6, 3, 8, 1, 9, 3, 4, 2, 1, 3, 1, 1, 2,\n",
      "        9, 7, 3, 4, 8, 3, 0, 7, 5, 1, 3, 6, 4, 9, 7, 1, 3, 7, 1, 1, 4, 3, 8, 2,\n",
      "        9, 1, 8, 7, 1, 4, 9, 9, 9, 6, 9, 5, 7, 1, 0, 5, 3, 4, 5, 2, 7, 2, 0, 7,\n",
      "        3, 2, 7, 1, 5, 3, 3, 8, 9, 7, 9, 7, 5, 2, 1, 8, 6, 6, 0, 7, 1, 7, 4, 6,\n",
      "        4, 1, 5, 9])\n",
      "Epoch [5/10], Step [500/600], Loss: 0.0669, Accuracy: 10.7400\n",
      "tensor([9, 7, 4, 8, 1, 4, 6, 6, 7, 9, 3, 8, 2, 2, 6, 7, 1, 8, 2, 7, 8, 9, 8, 9,\n",
      "        7, 4, 0, 0, 9, 5, 7, 0, 3, 0, 4, 1, 3, 7, 0, 2, 2, 7, 1, 8, 6, 9, 6, 1,\n",
      "        6, 7, 9, 5, 6, 4, 1, 5, 9, 7, 7, 7, 2, 8, 4, 7, 1, 7, 9, 5, 0, 2, 6, 5,\n",
      "        0, 7, 4, 2, 1, 5, 9, 4, 6, 3, 9, 1, 6, 4, 9, 5, 2, 3, 0, 4, 6, 0, 0, 9,\n",
      "        2, 7, 1, 8])\n",
      "Epoch [5/10], Step [600/600], Loss: 0.0009, Accuracy: 11.5800\n",
      "tensor([9, 2, 9, 4, 8, 9, 5, 4, 7, 3, 8, 8, 7, 1, 9, 9, 2, 8, 2, 6, 3, 0, 1, 1,\n",
      "        0, 8, 3, 1, 7, 8, 3, 0, 3, 1, 2, 1, 9, 0, 3, 2, 7, 8, 1, 8, 7, 7, 1, 4,\n",
      "        5, 8, 3, 0, 8, 2, 1, 7, 1, 2, 6, 8, 5, 1, 1, 6, 6, 6, 7, 0, 2, 6, 7, 6,\n",
      "        9, 0, 8, 1, 1, 0, 4, 3, 1, 3, 1, 6, 5, 1, 8, 7, 2, 7, 8, 9, 1, 9, 6, 2,\n",
      "        2, 2, 2, 4])\n",
      "Epoch [6/10], Step [100/600], Loss: 0.0011, Accuracy: 10.9600\n",
      "tensor([6, 7, 5, 8, 3, 2, 5, 1, 5, 1, 9, 4, 1, 5, 2, 4, 7, 4, 1, 1, 3, 4, 7, 2,\n",
      "        4, 5, 2, 8, 0, 5, 4, 1, 8, 1, 0, 7, 3, 2, 8, 0, 1, 7, 7, 6, 3, 1, 0, 1,\n",
      "        2, 1, 6, 5, 9, 4, 0, 9, 9, 4, 2, 8, 2, 9, 1, 6, 3, 4, 3, 9, 0, 0, 0, 4,\n",
      "        9, 7, 1, 1, 2, 6, 2, 7, 3, 1, 5, 0, 2, 6, 1, 7, 5, 3, 6, 3, 5, 8, 3, 3,\n",
      "        1, 1, 9, 0])\n",
      "Epoch [6/10], Step [200/600], Loss: 0.0434, Accuracy: 11.5200\n",
      "tensor([2, 3, 0, 0, 9, 3, 7, 7, 4, 8, 8, 1, 4, 1, 8, 7, 3, 4, 5, 8, 1, 1, 1, 7,\n",
      "        5, 9, 5, 9, 1, 6, 4, 3, 4, 2, 3, 6, 9, 5, 2, 4, 1, 2, 4, 0, 7, 5, 9, 3,\n",
      "        3, 0, 1, 9, 9, 7, 8, 5, 8, 3, 7, 8, 5, 7, 7, 9, 1, 2, 3, 7, 5, 1, 4, 1,\n",
      "        9, 3, 1, 4, 1, 7, 2, 1, 7, 4, 2, 7, 7, 4, 4, 1, 6, 5, 5, 3, 2, 4, 1, 6,\n",
      "        8, 7, 5, 8])\n",
      "Epoch [6/10], Step [300/600], Loss: 0.0003, Accuracy: 10.5400\n",
      "tensor([5, 1, 0, 4, 4, 1, 3, 0, 0, 2, 2, 9, 2, 9, 5, 5, 8, 6, 7, 6, 5, 7, 1, 7,\n",
      "        6, 9, 1, 5, 3, 1, 2, 0, 2, 7, 9, 0, 4, 3, 6, 3, 0, 1, 8, 2, 1, 4, 8, 6,\n",
      "        6, 7, 8, 5, 0, 6, 6, 4, 7, 7, 4, 4, 3, 1, 4, 8, 4, 9, 3, 8, 5, 2, 3, 6,\n",
      "        6, 0, 8, 4, 6, 1, 6, 7, 9, 8, 8, 0, 2, 4, 6, 4, 6, 8, 2, 3, 7, 7, 6, 3,\n",
      "        5, 4, 5, 3])\n",
      "Epoch [6/10], Step [400/600], Loss: 0.1452, Accuracy: 11.3900\n",
      "tensor([6, 3, 7, 5, 6, 2, 1, 3, 8, 0, 2, 5, 3, 3, 6, 6, 6, 3, 1, 6, 6, 4, 7, 3,\n",
      "        4, 6, 8, 8, 7, 0, 7, 0, 7, 7, 8, 4, 2, 4, 5, 3, 7, 3, 9, 1, 6, 4, 7, 5,\n",
      "        2, 8, 0, 9, 1, 6, 9, 3, 2, 8, 6, 5, 4, 0, 5, 7, 1, 4, 9, 9, 2, 3, 6, 9,\n",
      "        9, 3, 0, 3, 9, 6, 8, 3, 9, 3, 0, 3, 3, 2, 9, 5, 7, 4, 6, 9, 1, 6, 6, 7,\n",
      "        3, 4, 5, 4])\n",
      "Epoch [6/10], Step [500/600], Loss: 0.0085, Accuracy: 10.6800\n",
      "tensor([2, 5, 9, 5, 8, 9, 5, 4, 6, 7, 9, 3, 2, 1, 8, 4, 0, 4, 4, 4, 1, 3, 4, 4,\n",
      "        5, 9, 9, 2, 3, 3, 8, 6, 4, 1, 5, 9, 6, 0, 8, 8, 8, 7, 4, 0, 6, 1, 2, 1,\n",
      "        2, 1, 4, 1, 1, 9, 3, 5, 4, 5, 8, 9, 9, 7, 5, 5, 0, 3, 6, 9, 2, 7, 2, 7,\n",
      "        7, 7, 9, 0, 5, 2, 3, 9, 7, 0, 7, 5, 8, 3, 7, 5, 1, 3, 2, 1, 4, 6, 8, 2,\n",
      "        7, 9, 9, 9])\n",
      "Epoch [6/10], Step [600/600], Loss: 0.0142, Accuracy: 11.1000\n",
      "tensor([2, 9, 6, 3, 4, 0, 9, 4, 7, 7, 7, 6, 1, 2, 6, 7, 1, 5, 6, 7, 5, 2, 4, 2,\n",
      "        0, 8, 3, 1, 0, 7, 8, 5, 3, 7, 7, 4, 7, 4, 7, 8, 3, 2, 5, 9, 6, 2, 7, 0,\n",
      "        7, 1, 2, 6, 7, 2, 2, 4, 9, 4, 4, 8, 8, 9, 2, 0, 1, 6, 1, 8, 1, 3, 2, 3,\n",
      "        8, 0, 7, 4, 3, 5, 7, 8, 2, 4, 0, 3, 5, 8, 3, 4, 8, 4, 7, 4, 3, 7, 8, 5,\n",
      "        2, 0, 5, 9])\n",
      "Epoch [7/10], Step [100/600], Loss: 0.0247, Accuracy: 11.6100\n",
      "tensor([7, 7, 0, 1, 0, 3, 0, 4, 0, 3, 3, 2, 1, 2, 7, 9, 8, 7, 7, 0, 6, 3, 9, 1,\n",
      "        8, 8, 9, 0, 1, 3, 9, 7, 0, 3, 3, 6, 4, 6, 3, 4, 6, 7, 5, 0, 4, 3, 2, 5,\n",
      "        7, 0, 4, 3, 3, 1, 6, 6, 4, 0, 7, 0, 9, 5, 3, 6, 1, 1, 7, 0, 9, 6, 2, 8,\n",
      "        8, 1, 8, 9, 0, 3, 7, 2, 5, 5, 0, 0, 2, 8, 0, 8, 1, 7, 1, 1, 7, 5, 3, 3,\n",
      "        0, 8, 0, 4])\n",
      "Epoch [7/10], Step [200/600], Loss: 0.0284, Accuracy: 10.5300\n",
      "tensor([3, 1, 5, 9, 0, 2, 9, 3, 1, 2, 4, 7, 9, 2, 9, 4, 6, 4, 1, 5, 9, 4, 6, 8,\n",
      "        7, 4, 0, 0, 9, 5, 1, 6, 7, 3, 0, 7, 7, 1, 7, 8, 4, 2, 4, 6, 6, 6, 3, 2,\n",
      "        9, 6, 4, 5, 2, 7, 2, 3, 2, 4, 5, 3, 0, 1, 6, 8, 6, 0, 8, 8, 4, 9, 4, 4,\n",
      "        5, 9, 4, 8, 7, 8, 3, 8, 6, 0, 9, 0, 7, 7, 0, 6, 3, 4, 8, 2, 4, 0, 5, 0,\n",
      "        3, 2, 5, 3])\n",
      "Epoch [7/10], Step [300/600], Loss: 0.0024, Accuracy: 10.8600\n",
      "tensor([3, 7, 0, 9, 7, 1, 8, 2, 6, 7, 7, 0, 7, 1, 0, 3, 2, 5, 6, 8, 1, 3, 8, 3,\n",
      "        5, 9, 3, 4, 6, 7, 8, 6, 6, 9, 5, 3, 6, 4, 0, 2, 4, 8, 1, 4, 2, 6, 2, 4,\n",
      "        9, 6, 4, 8, 6, 6, 9, 8, 1, 3, 9, 2, 9, 3, 5, 3, 0, 9, 5, 8, 6, 3, 2, 9,\n",
      "        2, 8, 4, 9, 5, 3, 4, 5, 1, 0, 3, 9, 1, 6, 8, 7, 6, 1, 1, 0, 6, 8, 3, 9,\n",
      "        9, 1, 6, 9])\n",
      "Epoch [7/10], Step [400/600], Loss: 0.0558, Accuracy: 11.3800\n",
      "tensor([8, 9, 3, 1, 6, 4, 8, 0, 1, 0, 0, 1, 4, 9, 1, 6, 0, 0, 9, 3, 8, 6, 5, 7,\n",
      "        2, 8, 4, 3, 4, 1, 4, 2, 1, 5, 6, 3, 4, 9, 7, 7, 9, 3, 1, 6, 3, 6, 8, 6,\n",
      "        1, 9, 6, 4, 6, 6, 7, 2, 5, 6, 3, 5, 0, 5, 1, 2, 3, 3, 5, 8, 6, 0, 5, 6,\n",
      "        9, 4, 6, 1, 2, 1, 3, 1, 5, 6, 8, 1, 0, 9, 1, 0, 9, 3, 1, 9, 8, 6, 0, 9,\n",
      "        0, 6, 2, 2])\n",
      "Epoch [7/10], Step [500/600], Loss: 0.1465, Accuracy: 11.3300\n",
      "tensor([5, 0, 5, 4, 6, 4, 3, 9, 1, 2, 9, 4, 5, 5, 3, 0, 2, 1, 9, 6, 1, 0, 4, 4,\n",
      "        6, 6, 2, 2, 7, 7, 7, 2, 2, 6, 4, 8, 6, 9, 8, 8, 7, 0, 2, 9, 5, 7, 5, 5,\n",
      "        0, 8, 4, 1, 1, 9, 3, 2, 2, 2, 1, 2, 0, 1, 9, 5, 1, 4, 8, 0, 0, 0, 4, 5,\n",
      "        1, 7, 2, 1, 0, 1, 6, 4, 2, 0, 8, 1, 7, 4, 0, 0, 4, 7, 7, 5, 2, 9, 5, 1,\n",
      "        5, 0, 0, 2])\n",
      "Epoch [7/10], Step [600/600], Loss: 0.0007, Accuracy: 10.5400\n",
      "tensor([7, 3, 6, 9, 3, 0, 5, 0, 9, 3, 2, 6, 8, 7, 6, 9, 2, 7, 1, 7, 2, 8, 1, 4,\n",
      "        9, 8, 1, 4, 9, 2, 9, 3, 9, 1, 9, 8, 6, 1, 6, 0, 2, 8, 4, 1, 2, 3, 5, 7,\n",
      "        9, 0, 0, 0, 5, 4, 8, 8, 6, 3, 0, 1, 0, 7, 7, 5, 5, 9, 6, 1, 4, 9, 3, 2,\n",
      "        8, 8, 9, 4, 6, 6, 5, 2, 5, 3, 3, 8, 7, 7, 9, 5, 6, 3, 5, 3, 9, 3, 0, 0,\n",
      "        2, 9, 0, 6])\n",
      "Epoch [8/10], Step [100/600], Loss: 0.0138, Accuracy: 11.5300\n",
      "tensor([3, 9, 1, 0, 8, 4, 7, 0, 2, 9, 7, 8, 1, 6, 1, 9, 3, 2, 6, 4, 0, 8, 7, 1,\n",
      "        3, 9, 0, 7, 7, 1, 5, 7, 9, 5, 6, 9, 8, 4, 6, 6, 6, 8, 0, 9, 0, 1, 0, 8,\n",
      "        8, 4, 0, 0, 3, 8, 6, 0, 2, 0, 1, 3, 7, 9, 1, 7, 1, 2, 0, 5, 6, 8, 1, 6,\n",
      "        0, 7, 7, 7, 6, 8, 3, 6, 5, 7, 8, 7, 1, 7, 9, 9, 7, 4, 1, 2, 1, 1, 5, 3,\n",
      "        9, 2, 1, 0])\n",
      "Epoch [8/10], Step [200/600], Loss: 0.0228, Accuracy: 10.4200\n",
      "tensor([7, 0, 6, 2, 1, 6, 4, 1, 1, 5, 2, 4, 5, 6, 3, 9, 9, 0, 5, 6, 9, 5, 8, 5,\n",
      "        5, 6, 2, 8, 1, 8, 6, 6, 8, 7, 4, 4, 9, 8, 7, 4, 9, 3, 9, 9, 0, 4, 4, 5,\n",
      "        6, 7, 7, 7, 7, 9, 7, 2, 5, 9, 0, 1, 8, 0, 6, 8, 8, 0, 1, 0, 0, 9, 4, 0,\n",
      "        7, 4, 7, 0, 2, 3, 1, 2, 8, 8, 2, 5, 3, 1, 6, 8, 0, 2, 9, 7, 5, 1, 6, 2,\n",
      "        9, 3, 0, 5])\n",
      "Epoch [8/10], Step [300/600], Loss: 0.0280, Accuracy: 12.2400\n",
      "tensor([2, 2, 2, 2, 0, 2, 4, 2, 3, 1, 7, 3, 4, 7, 4, 2, 3, 1, 2, 9, 8, 0, 4, 8,\n",
      "        6, 1, 9, 0, 2, 9, 1, 5, 5, 9, 9, 1, 2, 9, 6, 0, 9, 8, 9, 7, 8, 9, 2, 6,\n",
      "        5, 9, 3, 0, 6, 7, 9, 0, 3, 4, 8, 6, 5, 6, 7, 1, 7, 3, 8, 2, 2, 0, 2, 7,\n",
      "        9, 5, 7, 4, 4, 2, 9, 0, 2, 2, 5, 2, 6, 4, 1, 1, 6, 5, 2, 3, 2, 9, 2, 1,\n",
      "        2, 1, 5, 2])\n",
      "Epoch [8/10], Step [400/600], Loss: 0.0861, Accuracy: 10.4500\n",
      "tensor([0, 0, 1, 5, 8, 7, 8, 4, 5, 7, 8, 9, 7, 5, 3, 9, 6, 6, 3, 8, 7, 2, 7, 5,\n",
      "        6, 7, 0, 3, 1, 2, 7, 4, 4, 9, 9, 6, 9, 1, 7, 7, 0, 0, 6, 2, 9, 4, 9, 5,\n",
      "        6, 4, 5, 3, 4, 1, 6, 7, 0, 4, 9, 2, 9, 4, 5, 1, 0, 1, 3, 3, 8, 4, 9, 3,\n",
      "        4, 0, 7, 0, 2, 7, 2, 1, 3, 9, 5, 8, 7, 6, 3, 5, 5, 9, 0, 1, 4, 2, 8, 7,\n",
      "        6, 6, 9, 5])\n",
      "Epoch [8/10], Step [500/600], Loss: 0.0059, Accuracy: 11.1000\n",
      "tensor([8, 1, 1, 9, 2, 3, 9, 3, 3, 4, 9, 6, 7, 5, 3, 3, 9, 5, 1, 6, 3, 2, 0, 0,\n",
      "        3, 8, 6, 8, 6, 9, 6, 6, 9, 8, 7, 2, 3, 2, 2, 7, 8, 6, 8, 1, 3, 0, 3, 0,\n",
      "        0, 9, 5, 5, 6, 4, 3, 4, 2, 0, 6, 4, 2, 9, 0, 0, 1, 4, 1, 9, 1, 4, 3, 9,\n",
      "        7, 5, 0, 6, 2, 0, 3, 7, 5, 0, 8, 1, 4, 3, 5, 9, 3, 5, 5, 0, 3, 3, 4, 1,\n",
      "        2, 5, 6, 3])\n",
      "Epoch [8/10], Step [600/600], Loss: 0.0137, Accuracy: 11.9700\n",
      "tensor([2, 1, 9, 2, 9, 8, 7, 7, 2, 9, 3, 8, 9, 6, 9, 9, 9, 0, 4, 2, 8, 4, 1, 4,\n",
      "        6, 2, 3, 6, 0, 6, 9, 8, 6, 1, 0, 7, 4, 2, 2, 9, 4, 1, 7, 0, 8, 1, 0, 9,\n",
      "        4, 6, 8, 7, 3, 8, 0, 8, 9, 0, 4, 9, 6, 1, 9, 4, 4, 1, 9, 8, 9, 1, 5, 1,\n",
      "        1, 7, 5, 1, 2, 7, 6, 4, 9, 7, 9, 7, 1, 1, 4, 8, 1, 9, 6, 2, 2, 9, 3, 7,\n",
      "        4, 3, 0, 7])\n",
      "Epoch [9/10], Step [100/600], Loss: 0.1293, Accuracy: 10.7900\n",
      "tensor([2, 5, 1, 6, 9, 5, 3, 3, 7, 1, 1, 9, 6, 9, 4, 7, 6, 4, 4, 9, 4, 5, 2, 3,\n",
      "        5, 9, 6, 5, 0, 1, 4, 2, 3, 6, 8, 9, 4, 0, 6, 1, 9, 7, 2, 4, 0, 8, 9, 3,\n",
      "        2, 7, 4, 7, 4, 4, 6, 5, 2, 2, 5, 8, 0, 0, 8, 5, 1, 7, 0, 8, 0, 1, 4, 8,\n",
      "        7, 4, 3, 8, 4, 3, 6, 4, 6, 4, 4, 8, 3, 3, 8, 1, 6, 8, 3, 2, 4, 6, 6, 8,\n",
      "        7, 2, 1, 9])\n",
      "Epoch [9/10], Step [200/600], Loss: 0.0015, Accuracy: 10.8800\n",
      "tensor([2, 7, 9, 3, 7, 7, 6, 3, 6, 1, 4, 2, 2, 3, 5, 7, 9, 0, 9, 2, 3, 7, 2, 1,\n",
      "        6, 7, 3, 4, 0, 7, 2, 3, 7, 8, 2, 0, 7, 3, 4, 2, 3, 6, 9, 4, 0, 6, 7, 9,\n",
      "        2, 6, 8, 4, 9, 3, 8, 4, 7, 0, 9, 3, 6, 4, 6, 0, 5, 8, 8, 0, 9, 2, 6, 8,\n",
      "        0, 5, 1, 9, 6, 5, 0, 6, 8, 9, 9, 3, 4, 5, 8, 3, 6, 3, 7, 7, 2, 4, 7, 8,\n",
      "        1, 4, 5, 2])\n",
      "Epoch [9/10], Step [300/600], Loss: 0.0376, Accuracy: 11.1100\n",
      "tensor([1, 2, 1, 1, 1, 1, 4, 6, 7, 8, 2, 1, 4, 9, 2, 6, 8, 4, 3, 1, 8, 4, 6, 4,\n",
      "        1, 0, 4, 5, 8, 9, 8, 9, 1, 1, 2, 5, 8, 8, 7, 8, 2, 1, 3, 8, 8, 3, 5, 1,\n",
      "        1, 7, 3, 4, 8, 7, 3, 7, 4, 0, 6, 3, 6, 5, 9, 3, 1, 8, 1, 6, 3, 9, 3, 0,\n",
      "        4, 7, 1, 6, 7, 8, 2, 2, 0, 6, 2, 9, 4, 0, 0, 5, 3, 7, 7, 9, 0, 7, 5, 3,\n",
      "        1, 3, 7, 8])\n",
      "Epoch [9/10], Step [400/600], Loss: 0.0457, Accuracy: 10.4900\n",
      "tensor([1, 0, 9, 1, 6, 1, 2, 6, 3, 8, 7, 6, 0, 6, 0, 2, 7, 5, 7, 2, 4, 2, 8, 8,\n",
      "        1, 8, 7, 3, 1, 9, 8, 7, 5, 6, 7, 5, 2, 4, 4, 5, 8, 6, 1, 4, 9, 8, 0, 7,\n",
      "        1, 6, 6, 0, 6, 9, 0, 9, 8, 5, 2, 7, 4, 9, 3, 8, 5, 0, 5, 0, 0, 2, 2, 5,\n",
      "        4, 4, 5, 3, 2, 9, 6, 5, 9, 2, 7, 9, 0, 8, 6, 0, 5, 0, 2, 5, 7, 1, 3, 1,\n",
      "        6, 1, 0, 8])\n",
      "Epoch [9/10], Step [500/600], Loss: 0.0563, Accuracy: 10.6200\n",
      "tensor([0, 1, 4, 6, 0, 1, 8, 6, 7, 6, 4, 6, 7, 5, 8, 1, 7, 8, 2, 2, 3, 8, 5, 4,\n",
      "        7, 8, 7, 7, 9, 1, 3, 9, 7, 5, 0, 1, 4, 8, 0, 1, 6, 1, 8, 9, 7, 9, 8, 3,\n",
      "        8, 3, 6, 0, 4, 9, 9, 1, 8, 3, 0, 8, 6, 5, 6, 5, 1, 4, 4, 1, 1, 5, 9, 3,\n",
      "        2, 2, 3, 5, 0, 5, 8, 3, 1, 7, 3, 9, 3, 8, 1, 2, 4, 0, 6, 0, 1, 5, 2, 0,\n",
      "        7, 0, 3, 8])\n",
      "Epoch [9/10], Step [600/600], Loss: 0.1087, Accuracy: 10.5900\n",
      "tensor([3, 8, 7, 4, 7, 2, 3, 9, 8, 0, 3, 4, 6, 2, 0, 4, 6, 2, 7, 2, 5, 0, 6, 6,\n",
      "        3, 8, 9, 2, 9, 0, 9, 8, 8, 9, 1, 8, 5, 1, 7, 8, 1, 2, 8, 7, 0, 1, 6, 1,\n",
      "        0, 6, 2, 3, 0, 3, 5, 6, 5, 9, 1, 7, 0, 8, 6, 3, 9, 4, 3, 3, 7, 8, 8, 2,\n",
      "        7, 7, 3, 6, 1, 2, 7, 4, 0, 0, 7, 1, 2, 2, 7, 1, 9, 2, 7, 9, 9, 1, 5, 8,\n",
      "        4, 2, 4, 6])\n",
      "Epoch [10/10], Step [100/600], Loss: 0.0624, Accuracy: 10.3500\n",
      "tensor([5, 3, 9, 3, 9, 9, 8, 0, 9, 2, 0, 2, 5, 9, 6, 4, 1, 6, 8, 9, 7, 1, 7, 9,\n",
      "        4, 0, 3, 7, 4, 7, 9, 1, 7, 6, 2, 1, 0, 2, 1, 3, 9, 3, 1, 5, 8, 5, 3, 7,\n",
      "        5, 8, 0, 1, 9, 1, 1, 9, 5, 4, 4, 3, 5, 4, 9, 3, 2, 2, 4, 0, 6, 3, 4, 1,\n",
      "        2, 2, 8, 6, 2, 0, 7, 1, 7, 8, 3, 3, 5, 2, 6, 7, 8, 8, 8, 8, 6, 0, 4, 2,\n",
      "        3, 5, 9, 7])\n",
      "Epoch [10/10], Step [200/600], Loss: 0.0227, Accuracy: 11.7000\n",
      "tensor([7, 6, 0, 9, 9, 2, 7, 6, 8, 8, 6, 1, 5, 6, 5, 5, 9, 9, 4, 9, 9, 0, 5, 0,\n",
      "        5, 3, 6, 5, 3, 8, 7, 4, 0, 9, 4, 0, 3, 9, 4, 3, 9, 9, 8, 3, 8, 0, 6, 7,\n",
      "        6, 9, 9, 0, 3, 1, 8, 9, 6, 4, 4, 1, 3, 9, 7, 1, 9, 0, 4, 7, 6, 2, 9, 7,\n",
      "        8, 2, 6, 9, 8, 8, 9, 9, 4, 3, 7, 1, 1, 1, 1, 7, 8, 8, 1, 9, 4, 1, 1, 8,\n",
      "        6, 8, 5, 3])\n",
      "Epoch [10/10], Step [300/600], Loss: 0.0730, Accuracy: 10.9600\n",
      "tensor([1, 8, 7, 2, 0, 9, 9, 9, 2, 1, 8, 1, 1, 0, 1, 1, 3, 3, 0, 5, 8, 8, 5, 6,\n",
      "        6, 1, 1, 8, 9, 0, 1, 4, 8, 7, 9, 8, 4, 5, 6, 1, 3, 2, 6, 4, 6, 4, 1, 7,\n",
      "        1, 6, 3, 0, 6, 3, 4, 0, 2, 8, 7, 1, 8, 2, 7, 9, 4, 9, 2, 7, 2, 8, 0, 6,\n",
      "        5, 1, 7, 3, 6, 9, 4, 3, 1, 0, 6, 5, 0, 7, 8, 8, 5, 7, 9, 7, 1, 1, 6, 6,\n",
      "        9, 5, 9, 0])\n",
      "Epoch [10/10], Step [400/600], Loss: 0.0142, Accuracy: 10.7800\n",
      "tensor([7, 9, 7, 0, 7, 3, 6, 5, 8, 3, 3, 7, 7, 6, 8, 2, 6, 9, 7, 5, 6, 0, 0, 2,\n",
      "        2, 7, 9, 3, 6, 2, 6, 5, 0, 7, 3, 8, 0, 4, 6, 9, 3, 9, 3, 8, 8, 5, 9, 0,\n",
      "        1, 6, 4, 3, 0, 2, 7, 1, 8, 7, 8, 6, 5, 9, 7, 7, 4, 6, 4, 6, 9, 6, 4, 0,\n",
      "        3, 5, 9, 9, 1, 5, 6, 5, 3, 5, 4, 4, 2, 8, 0, 8, 3, 6, 9, 0, 7, 1, 1, 8,\n",
      "        7, 8, 2, 7])\n",
      "Epoch [10/10], Step [500/600], Loss: 0.0278, Accuracy: 11.4200\n",
      "tensor([2, 3, 2, 9, 2, 5, 3, 8, 7, 6, 9, 0, 1, 1, 3, 1, 2, 6, 7, 2, 7, 6, 2, 9,\n",
      "        7, 7, 0, 6, 9, 1, 3, 3, 5, 2, 0, 9, 0, 7, 1, 3, 5, 3, 8, 4, 4, 1, 1, 3,\n",
      "        6, 5, 0, 5, 0, 2, 1, 4, 2, 5, 3, 7, 3, 1, 0, 5, 3, 4, 7, 0, 3, 7, 3, 0,\n",
      "        7, 3, 3, 2, 4, 6, 7, 1, 5, 7, 0, 5, 3, 0, 0, 9, 5, 0, 4, 2, 6, 6, 5, 5,\n",
      "        6, 8, 7, 1])\n",
      "Epoch [10/10], Step [600/600], Loss: 0.0582, Accuracy: 10.6700\n",
      "tensor([6, 9, 4, 5, 8, 3, 6, 5, 7, 9, 8, 9, 3, 7, 9, 6, 3, 9, 5, 4, 1, 1, 3, 2,\n",
      "        1, 8, 1, 3, 7, 7, 0, 0, 8, 8, 1, 5, 0, 4, 2, 1, 4, 0, 1, 1, 5, 7, 7, 0,\n",
      "        4, 5, 9, 4, 0, 8, 7, 0, 4, 3, 5, 1, 1, 7, 6, 9, 9, 2, 0, 2, 8, 2, 8, 5,\n",
      "        7, 5, 5, 1, 7, 8, 5, 6, 1, 8, 3, 5, 8, 6, 0, 0, 3, 7, 0, 7, 0, 8, 5, 8,\n",
      "        2, 6, 2, 0])\n"
     ]
    }
   ],
   "source": [
    "from torch.autograd import Variable\n",
    "import math\n",
    "\n",
    "loaders = {\n",
    "    'train' : trainloader,\n",
    "    \n",
    "    'test'  : testloader,\n",
    "}\n",
    "\n",
    "num_epochs = 10\n",
    "\n",
    "def getaccuracy(outputs, labels):\n",
    "    _, preds = torch.max(outputs, dim=1)\n",
    "    return torch.tensor(torch.sum(preds == labels).item() / len(preds))\n",
    "\n",
    "def train(num_epochs, cnn, loaders):\n",
    "    \n",
    "    cnn.train()\n",
    "        \n",
    "    # Train the model\n",
    "    total_step = len(loaders['train'])\n",
    "        \n",
    "    for epoch in range(num_epochs):\n",
    "        total, correct = 0,0\n",
    "  \n",
    "        for i, (images, labels) in enumerate(loaders['train']):\n",
    "            \n",
    "            # gives batch data, normalize x when iterate train_loader\n",
    "            b_x = Variable(images)   # batch x\n",
    "            b_y = Variable(labels.squeeze(1))   # batch y\n",
    "            # print(b_x)\n",
    "            output = cnn(b_x)[0]  \n",
    "            # print(output.shape) \n",
    "            # print(b_y.shape)            \n",
    "            loss = criterion(output, b_y)\n",
    "            \n",
    "            # clear gradients for this training step   \n",
    "            optimizer.zero_grad()           \n",
    "            \n",
    "            # backpropagation, compute gradients \n",
    "            loss.backward()    \n",
    "            # apply gradients             \n",
    "            optimizer.step()            \n",
    "\n",
    "            _, predicted = torch.max(output, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += math.floor(labels.size(0) * torch.mean(torch.eq(predicted,labels).float()))\n",
    "            # accuracy = torch.mean(torch.eq(predicted,labels).float())\n",
    "            accuracy = getaccuracy(output,labels)\n",
    " \n",
    "            if (i+1) % 100 == 0:\n",
    "                # accuracy = 100 *correct / total\n",
    "                print ('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}, Accuracy: {:.4f}' \n",
    "                       .format(epoch + 1, num_epochs, i + 1, total_step, loss.item(),accuracy))\n",
    "                print(predicted)\n",
    "                pass\n",
    "        pass\n",
    "    \n",
    "    \n",
    "    pass\n",
    "# train(num_epochs, predictionModel, loaders)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAjsAAAGzCAYAAADJ3dZzAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjYuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8o6BhiAAAACXBIWXMAAA9hAAAPYQGoP6dpAAB6bklEQVR4nO3deVxUVf8H8M/MwAz7IjuIIrjgjqISri0kmlmWPa655ZIKldKi5p4lZubjkxtlmWaatqilGGok+lNxwyVXFEUxZFhU9n3m/P5ARkdAAYFhxs/79ZqXzrnn3vle4F6+nHsWiRBCgIiIiMhASXUdABEREVFtYrJDREREBo3JDhERERk0JjtERERk0JjsEBERkUFjskNEREQGjckOERERGTQmO0RERGTQmOwQERGRQWOyQ1UyevRoeHh4VGvfefPmQSKR1GxAREREj8Fkx0BIJJJKvaKionQdqk6MHj0aFhYWug6DiIh0QMK1sQzDjz/+qPX+hx9+wN69e7Fhwwat8hdffBFOTk7V/pyioiKo1WooFIoq71tcXIzi4mKYmJhU+/Ora/To0fj111+RnZ1d559NRES6ZaTrAKhmvPnmm1rvjxw5gr1795Ypf1hubi7MzMwq/TnGxsbVig8AjIyMYGTEHzkiIqpbfIz1FHn22WfRpk0bxMTEoGfPnjAzM8PHH38MAPj999/Rr18/uLq6QqFQwMvLCwsWLIBKpdI6xsN9dq5fvw6JRIIlS5bgm2++gZeXFxQKBTp37ozjx49r7Vtenx2JRILg4GBs374dbdq0gUKhQOvWrREREVEm/qioKHTq1AkmJibw8vLC119/XeP9gH755Rf4+vrC1NQU9vb2ePPNN5GYmKhVR6lUYsyYMWjYsCEUCgVcXFzw6quv4vr165o6J06cQGBgIOzt7WFqaoomTZrgrbfeqrE4iYio8vhn9lPm9u3b6Nu3L4YMGYI333xT80hr3bp1sLCwQEhICCwsLPD3339jzpw5yMzMxBdffPHY427atAlZWVl4++23IZFIsHjxYrz++uu4du3aY1uDDh48iK1bt2Ly5MmwtLTEV199hYEDByIhIQF2dnYAgFOnTqFPnz5wcXHB/PnzoVKp8Mknn8DBweHJvyj3rFu3DmPGjEHnzp0RGhqK5ORk/O9//8OhQ4dw6tQp2NjYAAAGDhyI8+fP45133oGHhwdSUlKwd+9eJCQkaN737t0bDg4OmD59OmxsbHD9+nVs3bq1xmIlIqIqEGSQgoKCxMPf3l69egkAIiwsrEz93NzcMmVvv/22MDMzE/n5+ZqyUaNGicaNG2vex8fHCwDCzs5O3LlzR1P++++/CwBix44dmrK5c+eWiQmAkMvlIi4uTlN25swZAUAsX75cU9a/f39hZmYmEhMTNWVXrlwRRkZGZY5ZnlGjRglzc/MKtxcWFgpHR0fRpk0bkZeXpynfuXOnACDmzJkjhBDi7t27AoD44osvKjzWtm3bBABx/Pjxx8ZFRES1j4+xnjIKhQJjxowpU25qaqr5f1ZWFtLS0tCjRw/k5ubi0qVLjz3u4MGDYWtrq3nfo0cPAMC1a9ceu29AQAC8vLw079u1awcrKyvNviqVCn/99RcGDBgAV1dXTb2mTZuib9++jz1+ZZw4cQIpKSmYPHmyVgfqfv36wdvbG+Hh4QBKvk5yuRxRUVG4e/duuccqbQHauXMnioqKaiQ+IiKqPiY7Txk3NzfI5fIy5efPn8drr70Ga2trWFlZwcHBQdO5OSMj47HHbdSokdb70sSnooTgUfuW7l+6b0pKCvLy8tC0adMy9corq44bN24AAFq0aFFmm7e3t2a7QqHA559/jj///BNOTk7o2bMnFi9eDKVSqanfq1cvDBw4EPPnz4e9vT1effVVfP/99ygoKKiRWImIqGqY7DxlHmzBKZWeno5evXrhzJkz+OSTT7Bjxw7s3bsXn3/+OQBArVY/9rgymazcclGJmQ2eZF9dmDJlCi5fvozQ0FCYmJhg9uzZaNmyJU6dOgWgpNP1r7/+iujoaAQHByMxMRFvvfUWfH19OfSdiEgHmOwQoqKicPv2baxbtw7vvfceXn75ZQQEBGg9ltIlR0dHmJiYIC4ursy28sqqo3HjxgCA2NjYMttiY2M120t5eXnh/fffx549e3Du3DkUFhbiyy+/1KrzzDPP4LPPPsOJEyewceNGnD9/Hps3b66ReImIqPKY7JCmZeXBlpTCwkKsWrVKVyFpkclkCAgIwPbt23Hr1i1NeVxcHP78888a+YxOnTrB0dERYWFhWo+b/vzzT1y8eBH9+vUDUDIvUX5+vta+Xl5esLS01Ox39+7dMq1SPj4+AMBHWUREOsCh54SuXbvC1tYWo0aNwrvvvguJRIINGzbUq8dI8+bNw549e9CtWzdMmjQJKpUKK1asQJs2bXD69OlKHaOoqAiffvppmfIGDRpg8uTJ+PzzzzFmzBj06tULQ4cO1Qw99/DwwNSpUwEAly9fxgsvvIBBgwahVatWMDIywrZt25CcnIwhQ4YAANavX49Vq1bhtddeg5eXF7KysrBmzRpYWVnhpZdeqrGvCRERVQ6THYKdnR127tyJ999/H7NmzYKtrS3efPNNvPDCCwgMDNR1eAAAX19f/Pnnn/jggw8we/ZsuLu745NPPsHFixcrNVoMKGmtmj17dplyLy8vTJ48GaNHj4aZmRkWLVqEadOmwdzcHK+99ho+//xzzQgrd3d3DB06FJGRkdiwYQOMjIzg7e2Nn3/+GQMHDgRQ0kH52LFj2Lx5M5KTk2FtbY0uXbpg48aNaNKkSY19TYiIqHK4NhbptQEDBuD8+fO4cuWKrkMhIqJ6in12SG/k5eVpvb9y5Qp27dqFZ599VjcBERGRXmDLDukNFxcXjB49Gp6enrhx4wZWr16NgoICnDp1Cs2aNdN1eEREVE+xzw7pjT59+uCnn36CUqmEQqGAv78/Fi5cyESHiIgeiY+xSG98//33uH79OvLz85GRkYGIiAh07NhR12FRLTlw4AD69+8PV1dXSCQSbN++/bH7REVFoWPHjlAoFGjatCnWrVtX63ESUf3HZIeI6qWcnBy0b98eK1eurFT9+Ph49OvXD8899xxOnz6NKVOmYNy4cdi9e3ctR0pE9R377BBRvSeRSLBt2zYMGDCgwjrTpk1DeHg4zp07pykbMmQI0tPTERERUQdRElF9ZbB9dtRqNW7dugVLS0tIJBJdh0P01BFCICsrC66urpBKa78ROTo6GgEBAVplgYGBmDJlSoX7FBQUaM1qrVarcefOHdjZ2fG+QaQjtXHvMNhk59atW3B3d9d1GERPvZs3b6Jhw4a1/jlKpRJOTk5aZU5OTsjMzEReXl65i+CGhoZi/vz5tR4bEVVdTd47DDbZsbS0BFDyxbKystJxNERPn8zMTLi7u2uuxfpoxowZCAkJ0bzPyMhAo0aNeN8g0qHauHcYbLJT2gRtZWXFmxaRDtXV4yBnZ2ckJydrlSUnJ8PKyqrcVh0AUCgUUCgUZcp53yDSvZq8d3A0FhEZBH9/f0RGRmqV7d27F/7+/jqKiIjqCyY7RFQvZWdn4/Tp05pV7ePj43H69GkkJCQAKHkENXLkSE39iRMn4tq1a/joo49w6dIlrFq1Cj///LNmxXoienox2SGieunEiRPo0KEDOnToAAAICQlBhw4dMGfOHABAUlKSJvEBgCZNmiA8PBx79+5F+/bt8eWXX+Lbb79FYGCgTuInovrDYOfZyczMhLW1NTIyMip89q5WC0zaGIPUrAJ8P7oLrM2M6zhKIsNVmWuwvtHHmIkMTW1chwbbQbkypFIJDl+9jaz8YqRmFzDZISIiMkBP/WMse4uSkRhp2QWPqUlERET6iMmOhRwAcDu7UMeREBERUW146pMdO3O27BARERmypz7ZsbcsbdlhskNERGSInvpkp7RlJ5WPsYiIiAzSU5/s2FuWJDts2SEiIjJMVUp2VCoVZs+ejSZNmsDU1BReXl5YsGABHpyqRwiBOXPmwMXFBaampggICMCVK1e0jnPnzh0MHz4cVlZWsLGxwdixY5Gdna1V559//kGPHj1gYmICd3d3LF68+AlOs2L25iWPsdhnh4iIyDBVKdn5/PPPsXr1aqxYsQIXL17E559/jsWLF2P58uWaOosXL8ZXX32FsLAwHD16FObm5ggMDER+fr6mzvDhw3H+/Hns3bsXO3fuxIEDBzBhwgTN9szMTPTu3RuNGzdGTEwMvvjiC8ybNw/ffPNNDZyyNk3LTg4fYxERERmiKk0qePjwYbz66qvo168fAMDDwwM//fQTjh07BqCkVWfZsmWYNWsWXn31VQDADz/8ACcnJ2zfvh1DhgzBxYsXERERgePHj6NTp04AgOXLl+Oll17CkiVL4Orqio0bN6KwsBBr166FXC5H69atcfr0aSxdulQrKaoJdqUtO1ls2SEiIjJEVWrZ6dq1KyIjI3H58mUAwJkzZ3Dw4EH07dsXQMlCfUqlEgEBAZp9rK2t4efnh+joaABAdHQ0bGxsNIkOAAQEBEAqleLo0aOaOj179oRcLtfUCQwMRGxsLO7evVtubAUFBcjMzNR6VUZpy05OoQp5harKfimIiIhIT1SpZWf69OnIzMyEt7c3ZDIZVCoVPvvsMwwfPhwAoFQqAQBOTk5a+zk5OWm2KZVKODo6agdhZIQGDRpo1WnSpEmZY5Rus7W1LRNbaGgo5s+fX5XTAQBYKowgl0lRqFIjLbsA7g3MqnwMIiIiqr+q1LLz888/Y+PGjdi0aRNOnjyJ9evXY8mSJVi/fn1txVdpM2bMQEZGhuZ18+bNSu0nkUjuz6LMfjtEREQGp0otOx9++CGmT5+OIUOGAADatm2LGzduIDQ0FKNGjYKzszMAIDk5GS4uLpr9kpOT4ePjAwBwdnZGSkqK1nGLi4tx584dzf7Ozs5ITk7WqlP6vrTOwxQKBRQKRVVOR8POQoFbGfnst0NERGSAqtSyk5ubC6lUexeZTAa1Wg0AaNKkCZydnREZGanZnpmZiaNHj8Lf3x8A4O/vj/T0dMTExGjq/P3331Cr1fDz89PUOXDgAIqKijR19u7dixYtWpT7COtJ3W/ZYbJDRERkaKqU7PTv3x+fffYZwsPDcf36dWzbtg1Lly7Fa6+9BqDkkdCUKVPw6aef4o8//sDZs2cxcuRIuLq6YsCAAQCAli1bok+fPhg/fjyOHTuGQ4cOITg4GEOGDIGrqysAYNiwYZDL5Rg7dizOnz+PLVu24H//+x9CQkJq9uzvsdOsfM7HWERERIamSo+xli9fjtmzZ2Py5MlISUmBq6sr3n77bcyZM0dT56OPPkJOTg4mTJiA9PR0dO/eHRERETAxMdHU2bhxI4KDg/HCCy9AKpVi4MCB+OqrrzTbra2tsWfPHgQFBcHX1xf29vaYM2dOjQ87L2VvwcVAiYiIDJVEPDj9sQHJzMyEtbU1MjIyYGVl9ci63/7fNXwafhH927ti+dAOdRQhkWGryjVYX+hjzESGpjauw6d+bSwAsLPgyudERESGiskO7j/Gus0+O0RERAaHyQ4AO3P22SEiIjJUTHYA2FuWPMa6k1sIldoguzARERE9tZjsAGhgJodEAggB3OEsykT1ysqVK+Hh4QETExP4+flpFh6uyLJly9CiRQuYmprC3d0dU6dORX5+fh1FS0T1EZMdAEYyKWzNOLEgUX2zZcsWhISEYO7cuTh58iTat2+PwMDAMrOwl9q0aROmT5+OuXPn4uLFi/juu++wZcsWfPzxx3UcORHVJ0x27rEzL0l20rLYskNUXyxduhTjx4/HmDFj0KpVK4SFhcHMzAxr164tt/7hw4fRrVs3DBs2DB4eHujduzeGDh362NYgIjJsTHbu0YzIYssOUb1QWFiImJgYBAQEaMqkUikCAgIQHR1d7j5du3ZFTEyMJrm5du0adu3ahZdeeqnc+gUFBcjMzNR6EZHhqdIMyoasdK6dVC4GSlQvpKWlQaVSwcnJSavcyckJly5dKnefYcOGIS0tDd27d4cQAsXFxZg4cWKFj7FCQ0Mxf/78Go+diOoXtuzcc79lh4+xiPRVVFQUFi5ciFWrVuHkyZPYunUrwsPDsWDBgnLrz5gxAxkZGZrXzZs36zhiIqoLbNm5p3Tl8zS27BDVC/b29pDJZEhOTtYqT05OhrOzc7n7zJ49GyNGjMC4ceMAAG3bttWs1Tdz5kxIpdp/3ykUCigUito5ASKqN9iycw9bdojqF7lcDl9fX0RGRmrK1Go1IiMj4e/vX+4+ubm5ZRIamUwGADDQZQCJqBLYsnOPHVc+J6p3QkJCMGrUKHTq1AldunTBsmXLkJOTgzFjxgAARo4cCTc3N4SGhgIA+vfvj6VLl6JDhw7w8/NDXFwcZs+ejf79+2uSHiJ6+jDZucdesxgoW3aI6ovBgwcjNTUVc+bMgVKphI+PDyIiIjSdlhMSErRacmbNmgWJRIJZs2YhMTERDg4O6N+/Pz777DNdnQIR1QMSYaBtu1VdIv7mnVz0WLwPciMpYhf0gUQiqYMoiQxXVa/B+kAfYyYyNLVxHbLPzj2lQ88Li9XILijWcTRERERUU5js3GMmN4KZvOSZfhofZRERERkMJjsP0IzIYidlIiIig8Fk5wGlj7I4IouIiMhwMNl5gL1m+DkfYxERERkKJjsPsGfLDhERkcFhsvOA+3122LJDRERkKJjsPMDOnC07REREhobJzgPsLdmyQ0REZGiY7DzAzpzrYxERERkaJjsPcLDkYywiIiJDw2TnAaUtO5n5xSgsVus4GiIiIqoJTHYeYG1qDCNpyQKgt3PYukNERGQImOw8QCqVoMG9EVnspExERGQYmOw8pHSunVT22yEiIjIITHYeUro+Flt2iIiIDAOTnYc4WHD4ORERkSFhsvOQ+y07THaIiIgMAZOdh3DlcyIiIsPCZOchdnyMRUREZFCY7Dyk9DEWW3aIiIgMQ5WTncTERLz55puws7ODqakp2rZtixMnTmi2CyEwZ84cuLi4wNTUFAEBAbhy5YrWMe7cuYPhw4fDysoKNjY2GDt2LLKzs7Xq/PPPP+jRowdMTEzg7u6OxYsXV/MUq6a0gzL77BARERmGKiU7d+/eRbdu3WBsbIw///wTFy5cwJdffglbW1tNncWLF+Orr75CWFgYjh49CnNzcwQGBiI/P19TZ/jw4Th//jz27t2LnTt34sCBA5gwYYJme2ZmJnr37o3GjRsjJiYGX3zxBebNm4dvvvmmBk750TQdlHMKoVaLWv88IiIiqmWiCqZNmya6d+9e4Xa1Wi2cnZ3FF198oSlLT08XCoVC/PTTT0IIIS5cuCAAiOPHj2vq/Pnnn0IikYjExEQhhBCrVq0Stra2oqCgQOuzW7RoUelYMzIyBACRkZFR6X2EEKKgSCUaT9spGk/bKe5kFzx+ByIqV3WvwYetWLFCNG7cWCgUCtGlSxdx9OjRR9a/e/eumDx5snB2dhZyuVw0a9ZMhIeH12nMRFR9tXEdVqll548//kCnTp3wn//8B46OjujQoQPWrFmj2R4fHw+lUomAgABNmbW1Nfz8/BAdHQ0AiI6Oho2NDTp16qSpExAQAKlUiqNHj2rq9OzZE3K5XFMnMDAQsbGxuHv3brmxFRQUIDMzU+tVHXIjKaxMjACwkzKRrm3ZsgUhISGYO3cuTp48ifbt2yMwMBApKSnl1i8sLMSLL76I69ev49dff0VsbCzWrFkDNze3Oo6ciOqTKiU7165dw+rVq9GsWTPs3r0bkyZNwrvvvov169cDAJRKJQDAyclJaz8nJyfNNqVSCUdHR63tRkZGaNCggVad8o7x4Gc8LDQ0FNbW1pqXu7t7VU5Ni70lh58T1QdLly7F+PHjMWbMGLRq1QphYWEwMzPD2rVry62/du1a3LlzB9u3b0e3bt3g4eGBXr16oX379nUcORHVJ1VKdtRqNTp27IiFCxeiQ4cOmDBhAsaPH4+wsLDaiq/SZsyYgYyMDM3r5s2b1T6WvTmHnxPpWmFhIWJiYrRaiqVSKQICAjQtxQ/7448/4O/vj6CgIDg5OaFNmzZYuHAhVCpVufVrqkWYiOq3KiU7Li4uaNWqlVZZy5YtkZCQAABwdnYGACQnJ2vVSU5O1mxzdnYu0wRdXFyMO3fuaNUp7xgPfsbDFAoFrKystF7VZW/JWZSJdC0tLQ0qleqRLcUPu3btGn799VeoVCrs2rULs2fPxpdffolPP/203Po12SJMRPVXlZKdbt26ITY2Vqvs8uXLaNy4MQCgSZMmcHZ2RmRkpGZ7ZmYmjh49Cn9/fwCAv78/0tPTERMTo6nz999/Q61Ww8/PT1PnwIEDKCoq0tTZu3cvWrRooTXyq7bYmfMxFpE+UqvVcHR0xDfffANfX18MHjwYM2fOrLD1uSZbhImo/qpSsjN16lQcOXIECxcuRFxcHDZt2oRvvvkGQUFBAACJRIIpU6bg008/xR9//IGzZ89i5MiRcHV1xYABAwCUtAT16dMH48ePx7Fjx3Do0CEEBwdjyJAhcHV1BQAMGzYMcrkcY8eOxfnz57Flyxb873//Q0hISM2efQVKl4y4ncOWHSJdsbe3h0wme2RL8cNcXFzQvHlzyGQyTVnLli2hVCpRWFj2j5eabBEmovqrSslO586dsW3bNvz0009o06YNFixYgGXLlmH48OGaOh999BHeeecdTJgwAZ07d0Z2djYiIiJgYmKiqbNx40Z4e3vjhRdewEsvvYTu3btrzaFjbW2NPXv2ID4+Hr6+vnj//fcxZ84crbl4alPpXDupWWzZIdIVuVwOX19frZZitVqNyMhITUvxw7p164a4uDio1WpN2eXLl+Hi4qI1upOIni4SIYRBzpyXmZkJa2trZGRkVPmvtYhzSkz8MQYdGtlg2+RutRQhkWF7kmuw1JYtWzBq1Ch8/fXX6NKlC5YtW4aff/4Zly5dgpOTE0aOHAk3NzeEhoYCAG7evInWrVtj1KhReOedd3DlyhW89dZbePfddzFz5sw6iZmInkxtXIdGNXIUA2OvWR+Lj7GIdGnw4MFITU3FnDlzoFQq4ePjg4iICE2n5YSEBEil9xuo3d3dsXv3bkydOhXt2rWDm5sb3nvvPUybNk1Xp0BE9QCTnXJo+uywgzKRzgUHByM4OLjcbVFRUWXK/P39ceTIkVqOioj0CVc9L0dpn53cQhVyC4t1HA0RERE9CSY75bBQGEFhVPKlYesOERGRfmOyUw6JRKJ5lMV+O0RERPqNyU4F7ndSZssOERGRPmOyUwE7TSdltuwQERHpMyY7FeDwcyIiIsPAZKcCdhZcH4uIiMgQMNmpADsoExERGQYmOxUofYzFoedERET6jclOBdiyQ0REZBiY7FSgdBbl2zls2SEiItJnTHYqUNqycze3EMUqtY6jISIioupislMBWzM5pBJACOBOLlt3iIiI9BWTnQrIpBI0ML83104Wkx0iIiJ9xWTnEezM782inMNOykRERPqKyc4j2FtyFmUiIiJ9x2TnETQtO5xrh4iISG8x2XmE0hFZqWzZISIi0ltMdh7BjrMoExER6T0mO4/gwFmUiYiI9B6TnUdgyw4REZH+Y7LzCHZs2SEiItJ7THYe4cGVz4UQOo6GiIiIqoPJziOUjsYqVKmRmV+s42iInk4rV66Eh4cHTExM4Ofnh2PHjlVqv82bN0MikWDAgAG1GyAR1XtMdh7BxFgGC4URAOA2H2UR1bktW7YgJCQEc+fOxcmTJ9G+fXsEBgYiJSXlkftdv34dH3zwAXr06FFHkRJRfcZk5zFKOymnsZMyUZ1bunQpxo8fjzFjxqBVq1YICwuDmZkZ1q5dW+E+KpUKw4cPx/z58+Hp6fnI4xcUFCAzM1PrRUSGh8nOY5Q+ymLLDlHdKiwsRExMDAICAjRlUqkUAQEBiI6OrnC/Tz75BI6Ojhg7duxjPyM0NBTW1taal7u7e43ETkT1C5Odx7Az5/pYRLqQlpYGlUoFJycnrXInJycolcpy9zl48CC+++47rFmzplKfMWPGDGRkZGheN2/efOK4iaj+MdJ1APWdvWXp8HM+xiKqz7KysjBixAisWbMG9vb2ldpHoVBAoVDUcmREpGtMdh7D/l7Lzu0ctuwQ1SV7e3vIZDIkJydrlScnJ8PZ2blM/atXr+L69evo37+/pkytVgMAjIyMEBsbCy8vr9oNmojqJT7GegxNy04WW3aI6pJcLoevry8iIyM1ZWq1GpGRkfD39y9T39vbG2fPnsXp06c1r1deeQXPPfccTp8+zf44RE8xtuw8hp35vQ7KbNkhqnMhISEYNWoUOnXqhC5dumDZsmXIycnBmDFjAAAjR46Em5sbQkNDYWJigjZt2mjtb2NjAwBlyono6cJk5zHsOfScSGcGDx6M1NRUzJkzB0qlEj4+PoiIiNB0Wk5ISIBUygZqIno0JjuPwfWxiHQrODgYwcHB5W6Liop65L7r1q2r+YCISO880Z9EixYtgkQiwZQpUzRl+fn5CAoKgp2dHSwsLDBw4MAyHQwTEhLQr18/mJmZwdHRER9++CGKi7WXY4iKikLHjh2hUCjQtGlTnd20HO4lO1n5xcgvUukkBiIiIqq+aic7x48fx9dff4127dpplU+dOhU7duzAL7/8gv379+PWrVt4/fXXNdtVKhX69euHwsJCHD58GOvXr8e6deswZ84cTZ34+Hj069dP07FwypQpGDduHHbv3l3dcKvNytQIxjIJAOBODh9lERER6ZtqJTvZ2dkYPnw41qxZA1tbW015RkYGvvvuOyxduhTPP/88fH198f333+Pw4cM4cuQIAGDPnj24cOECfvzxR/j4+KBv375YsGABVq5cicLCkmQiLCwMTZo0wZdffomWLVsiODgYb7zxBv773//WwClXjUQi0XRS5qMsIiIi/VOtZCcoKAj9+vXTmsYdAGJiYlBUVKRV7u3tjUaNGmmmd4+Ojkbbtm21ZkUNDAxEZmYmzp8/r6nz8LEDAwMfOUV8ba5xU7o+1m12UiYiItI7Ve6gvHnzZpw8eRLHjx8vs02pVEIul2uGe5Z6cHp3pVJZ7vTvpdseVSczMxN5eXkwNTUt89mhoaGYP39+VU+nUkrXx0plyw4REZHeqVLLzs2bN/Hee+9h48aNMDExqa2YqqU217hhyw4REZH+qlKyExMTg5SUFHTs2BFGRkYwMjLC/v378dVXX8HIyAhOTk4oLCxEenq61n4PTu/u7Oxc7vTvpdseVcfKyqrcVh2gZI0bKysrrVdNceDwcyIiIr1VpWTnhRdeKDMde6dOnTB8+HDN/42NjbWmd4+NjUVCQoJmend/f3+cPXsWKSkpmjp79+6FlZUVWrVqpanz4DFK65Q3RXxduN+yw2SHiIhI31Spz46lpWWZadfNzc1hZ2enKR87dixCQkLQoEEDWFlZ4Z133oG/vz+eeeYZAEDv3r3RqlUrjBgxAosXL4ZSqcSsWbMQFBSkWX144sSJWLFiBT766CO89dZb+Pvvv/Hzzz8jPDy8Js65yuwtuPI5ERGRvqrxGZT/+9//QiqVYuDAgSgoKEBgYCBWrVql2S6TybBz505MmjQJ/v7+MDc3x6hRo/DJJ59o6jRp0gTh4eGYOnUq/ve//6Fhw4b49ttvERgYWNPhVgpnUSYiItJfEiGE0HUQtSEzMxPW1tbIyMh44v47529loN9XB2FvocCJWQGP34GIavQarCv6GDORoamN65Ar6FVC6WOsOzkFUKsNMjckIiIyWEx2KqGBeUkHZbUA7uay3w4REZE+YbJTCcYyKWzMjAEAt7k+FhERkV5hslNJmhFZWeykTEREpE+Y7FSS3b1HWWls2SEiItIrTHYqyd6SLTtERET6iMlOJdnfa9m5ncNkh4iISJ8w2amk+312+BiLiIhInzDZqaTSWZTZskNERKRfmOxUkv29xUC5PhYREZF+YbJTSaUtO7fS81BYrNZxNERERFRZTHYqqamDBczkMqRkFSB400kUqZjwENWFlStXwsPDAyYmJvDz88OxY8cqrLtmzRr06NEDtra2sLW1RUBAwCPrE9HTgclOJVmbGSPsTV/IjaTYcyEZUzafRjETHqJatWXLFoSEhGDu3Lk4efIk2rdvj8DAQKSkpJRbPyoqCkOHDsW+ffsQHR0Nd3d39O7dG4mJiXUcORHVJ1z1vIr2XUrB2xtiUKhS45X2rvjvYB/IpJIaOz6RoaiJa9DPzw+dO3fGihUrAABqtRru7u545513MH369Mfur1KpYGtrixUrVmDkyJF1EjMRPRmuel4PPOftiFXDO8JIKsEfZ27hw1/OQMWV0IlqXGFhIWJiYhAQEKApk0qlCAgIQHR0dKWOkZubi6KiIjRo0KDc7QUFBcjMzNR6EZHhYbJTDQGtnLBiWAfIpBJsPZWIGVv/gZoJD1GNSktLg0qlgpOTk1a5k5MTlEplpY4xbdo0uLq6aiVMDwoNDYW1tbXm5e7u/sRxE1H9w2Snmvq0ccGywT6QSoCfT/yLWb+fg4E+ESTSS4sWLcLmzZuxbds2mJiYlFtnxowZyMjI0Lxu3rxZx1ESUV1gsvME+rd3xdJBPpBIgE1HEzDvj/NMeKjeuJychSmbT+F6Wo6uQ6kWe3t7yGQyJCcna5UnJyfD2dn5kfsuWbIEixYtwp49e9CuXbsK6ykUClhZWWm9iMjwMNl5QgM6uOGLN9pDIgHWR9/Ap+EXmfCQzgkhMO23f7D99C3M+eO8rsOpFrlcDl9fX0RGRmrK1Go1IiMj4e/vX+F+ixcvxoIFCxAREYFOnTrVRahEVM8x2akBb/g2ROhrbQEA3x2Mx6KIS0x4SKcOxd3GqYR0AMCBy6k4l5ih24CqKSQkBGvWrMH69etx8eJFTJo0CTk5ORgzZgwAYOTIkZgxY4am/ueff47Zs2dj7dq18PDwgFKphFKpRHZ2tq5OgYjqASY7NWRIl0ZYMKANAODr/dewdO9lHUdET7OvIq8AABRGJZf4qqg4XYZTbYMHD8aSJUswZ84c+Pj44PTp04iIiNB0Wk5ISEBSUpKm/urVq1FYWIg33ngDLi4umteSJUt0dQpEVA9wnp0a9v2heMzfcQFASZ+eCT080bahdZ19PtGRa7cx5JsjkMukWDOqE0atPQaJBPgrpBe8HCzqLA59nLNGH2MmMjScZ0cPjOnWBDNfagkA2HHmFvqvOIg3Vh9G+D9JnHGZ6kRpq86gzg3Rq7kDAlo6QQggLOqqjiMjItINJju1YHxPT/we1A2v+rjCSCrBiRt3EbTpJHos3odVUXG4m8OV06l2HL9+B4ev3oaxTIJJzzYFAEx+zgsAsO1UIhLT83QZHhGRTjDZqSXt3W3wvyEdcGj683j3+aawM5cjKSMfiyNi8UxoJKb/9g8uKTlbK9Ws0ladN3wbws3GFADQsZEt/D3tUKwWWHPgmi7DIyLSCSY7tczJygQhvVvg0PTnseQ/7dHa1QoFxWpsPn4TfZb9H4Z+cwS7zibh/K0M3Lidg7TsAuQXqTiai6rsVMJd/N+VNMikEkzq1VRrW9BzJe83H0/A7ewCXYRHRKQzRroO4GlhYizDG74NMbCjG07cuIvvD8Vj9/lkRF+7jehrt8vUl0klMJfLYKEwgvm9l4XCCK42JpjQ0wtNHeuuoynph+V/l4y4eq2DGxrZmWlt69bUDu0aWuOffzPw/aHr+CCwhS5CJCLSCSY7dUwikaCzRwN09miAxPQ8bIi+gb0XlMjML0ZOQTFyC1UAAJVaIDO/GJn5xWWO8dvJRAz3a4QpAc3RwFxe16dA9dDZfzPw96UUSCX3W3EeJJFIMPnZppj4YwzWR1/H2708YWlirINIiYjqHpMdHXKzMcX0vt6Y3tdbU6ZSC+QWFiOnQIXsgpIEKKegGNn3XrvOKvHXxWT8EH0D204lIvi5phjdzQMKI5kOz4R07au/S/rqvOrjhib25uXW6d3KCU0dLRCXko0fjyRg0rNedRkiEZHOsM9OPSOTSmBpYgxnaxM0dbRAe3cbdG1qj96tnfF6x4b4dlQnbBrvh1YuVsjKL0bon5cQsHQ/wv9JYj+fp9SFW5nYeyEZkgpadUpJpRJM6lWS4Hx38Bryi1R1FSIRkU4x2dFDXb3sseOd7vjijXZwslLg5p08BG06iTfConEy4a6uw6M6tmJfSatOv7Yuj+3L9YqPK9xsTJGWXYhfTnCFbyJ6OjDZ0VMyqQT/6eSOfR88iykBzWBqLEPMjbt4fdVhvPPTKdy8k6vrEKkOxCqzsOusEgDwzvPNHlvfWCbF2708AQBh+6+hiBNdEtFTgMmOnjOTG2FKQHNEffgs/uPbEBJJyczNLyzdj/k7zuNcYoZBPN7KyC1C+D9JmLntLBbuulgjcxQJIXAs/g7e+ekU2s3bjYGrD2PF31dwLjEDanXdfc2EELicnIXUrKoPCV+xr2QEVt82zmjhbFmpfQZ1coe9hRyJ6XnYceZWlT8zMT0Pey8kV3k/IiJd4dpYBub8rQx8Fn4Rh6/eH87u6WCO/u1c8YqPa7XXRsouKMbJG3dxISkTTR0s0LWpHczktde/vVilxpl/07H/choOXE7FP/+m4+H8o727DQZ3ckf/9i5VGlmUU1CM7acTsSH6Bi4ps8qtY2+hwLMtHPBsCwf0aOoAa7OaHbkkhMA//2Yg/GwSdp1Nwr938yA3kmJSLy9MetYLJsaP73Ael5KNF/+7H0IA4e92R2vXyq/BtioqDosjYtHU0QJ7pvSEVCqpVMybj9/EZ+EXoVILREzpgcZ25XeGBvTzGtTHmIkMTW1ch0x2DJAQAvsvp+LnEzcReTEFBcX3H1W0crHCKz6u6N/eVTPDbnnScwtx/PpdHIu/jWPxd3DuViZUD2QbcpkUXZo00CQEXg4WkEge/wvzUW7eycWBK6n4v8tpOHQ1DVkPDbtv5miB7s3soczIx94LySi+F4+psQz92rlgSGd3+Da2rTCOklFIN/BbzL/IKig5tomxFAN83PBaBzdcTc3BvtgUHIpL00wBAJQ8MuzYyAbPtnBEr+YOaO1qVa1zLS/BKWUsk6BIVXI+jRqYYd4rrfC8t9Mjjxey5TS2nkrEi62csGZkpyrFkplfhG6hfyOroBhfj/BFYGvnR9a/lZ6Hab/9g/+7kgYA8G1si6WD2jPZIaIax2SnCnjTKpGVX4S9F5Kx48wt/N+VNE2CAACdGtviFR9X9G3jAiEEjsbfwbF7r9jksi0eDW1N0cbVGuduZWj9ogZKhtGXJD6O6OplB3NF+a0+WflFuJWej8T0XCSm5yPxbh5upefhbGIG4tNytOpamxqjezN79Gxmjx7NHOD6QHKWll2AbScTsfl4Aq6m3t/P08Ecgzu54/WODeFgqUCxSo2/LiZjw5EbOBR3v7Wrib053nymMd7o2LBMq01BsQonrt9FVGwKomJTcSUlW2t7A3M53BuYwdXaBC7WpnC1KfnXxcYErtamcLBUQHavpeRRCY6ZXIbnvR3Rr60Lnm3hiMhLyViw8wKSM0seZ73Yyglz+7dCQ1vtCQIB4HpaDp7/MgpqAewI7o62DSvfqlPqi92XsHLfVbRvaI3tQd3KTeCEEPj5xE18uvMisgqKoTCS4sPAFhjTrYnmHCuij9egPsZMZGh0nuyEhoZi69atuHTpEkxNTdG1a1d8/vnnaNHi/mys+fn5eP/997F582YUFBQgMDAQq1atgpPT/b9SExISMGnSJOzbtw8WFhYYNWoUQkNDYWR0/xdkVFQUQkJCcP78ebi7u2PWrFkYPXp0pU+MN62y7uQU4s9zSfjj9C0cu34Hpd95iQQo76fAy8EcXZrYwa9JA3Ru0kDTEiSEwNXUHETFpmD/5VQcvXYHhQ90dJXLpOjcxBa+jRsgPbcQt9Lz8O+9pKa8SRJLlbag9GjmgJ7NHdDWzfqxv1CFEDiZcBdbjt/EjjNJyLs3nNpIKkGPZva4pMxCUkY+AEAqAZ73dsJI/8bo3tS+Uo9uAODfu7mIik1FVGwqDl/VbvWp6DycLBVwsTFFcmZ+uQnOy+1c0Ku5I0zl2o+rsguK8VXkFaw9GI9itYCJsRTvPN8M43o00ZpL6cNfzuCXmH/xXAsHfD+mS6XO42Fp2QXotuhvFBSr8eNYP3RvZq+1PSkjD9N/O4v9l1MBAB0a2WDJf9pX+lGoPl6D+hgzkaHRebLTp08fDBkyBJ07d0ZxcTE+/vhjnDt3DhcuXIC5eUlz9qRJkxAeHo5169bB2toawcHBkEqlOHToEABApVLBx8cHzs7O+OKLL5CUlISRI0di/PjxWLhwIQAgPj4ebdq0wcSJEzFu3DhERkZiypQpCA8PR2BgYKVi5U3r0ZQZ+dj5zy3sOHMLZ/7NgERS8oirS5MG6OJRktzYWygqdazcwmJEX71dkhBcTsHNO49eWdvGzBiu1qZwszWFm03Jy8PeHH6eDWD1BLP6ZhcUY+eZW9h8/CZO30zXlDcwl2NIZ3cM82tUbitJVRQUq3BZmY1bGXlISs9DUkY+bmXka/6vzMzXetwHPD7BKc/l5CzM2n4Ox+LvAAA87c0x/9XW6NHMATfv5OLZJVFQqQW2Tu6Kjo1sq30+8/44j3WHr6Orlx02jX8GQEkC+UvMv1iw4wKyCoohN5Lig97NMba752OTzwfp4zWojzETGRqdJzsPS01NhaOjI/bv34+ePXsiIyMDDg4O2LRpE9544w0AwKVLl9CyZUtER0fjmWeewZ9//omXX34Zt27d0rT2hIWFYdq0aUhNTYVcLse0adMQHh6Oc+fOaT5ryJAhSE9PR0RERKVi402r8pIz82FiLIO16ZN3whVC4FpaDqJiU3EpKRMOlgq42ZrC1cYUDW1M4WJjCosKHnHVpFhlFvacV8K9gRn6tnWusxmmVWqB1KyCe8lQPhRGUnRral+pBOdhQghsP52Iz8IvIe3e4p392roAEiD8nyT0aGaPDWP9nijexPQ89Fq8D8VqgW2Tu8LF2hQztv6DfbElrTk+7iWtOdVZi00fr0F9jJnI0NTGdfhEv3UyMjIAAA0aNAAAxMTEoKioCAEBAZo63t7eaNSokSbZiY6ORtu2bbUeawUGBmLSpEk4f/48OnTogOjoaK1jlNaZMmVKhbEUFBSgoOD+0N3MzCcfmvy0cLIyqbFjSSQSeDlYVHvUV01p4WxZ6aHYNUkmlcDZ2gTO1iZAoyc7lkQiwWsdGuKFlk5Yuucyfoi+jvCzSZrt773w+Hl1HsfNxhQDOrjh15h/MWPrWSSm5yErv6Q1J+TF5hjfo2qtOURE9VG159lRq9WYMmUKunXrhjZt2gAAlEol5HI5bGxstOo6OTlBqVRq6jyY6JRuL932qDqZmZnIyyv/EUloaCisra01L3d39+qeGlG9YmVijHmvtMaOd7qjYyMbAEDP5g7o5NGgRo4/sZcXJBLgkjILWfnFaO9ug/B3umNiLy8mOkRkEKrdshMUFIRz587h4MGDNRlPtc2YMQMhISGa95mZmUx4yKC0drXGrxO74p/EDDR3qrmWs6aOFhjxTGP8GvMvgp9vigk9PGEk43yjRGQ4qpXsBAcHY+fOnThw4AAaNmyoKXd2dkZhYSHS09O1WneSk5Ph7OysqXPs2DGt4yUnJ2u2lf5bWvZgHSsrK5ialj83jEKhgEJRuQ61RPpKKpXAx92mxo87/5XWmNe/daVHqBER6ZMq/fkmhEBwcDC2bduGv//+G02aNNHa7uvrC2NjY0RGRmrKYmNjkZCQAH9/fwCAv78/zp49i5SUFE2dvXv3wsrKCq1atdLUefAYpXVKj0FENUsikTDRISKDVaWWnaCgIGzatAm///47LC0tNX1srK2tYWpqCmtra4wdOxYhISFo0KABrKys8M4778Df3x/PPFMyrLV3795o1aoVRowYgcWLF0OpVGLWrFkICgrStMxMnDgRK1aswEcffYS33noLf//9N37++WeEh4fX8OkTERGRwRNVAKDc1/fff6+pk5eXJyZPnixsbW2FmZmZeO2110RSUpLWca5fvy769u0rTE1Nhb29vXj//fdFUVGRVp19+/YJHx8fIZfLhaenp9ZnVEZGRoYAIDIyMqq0HxHVjJq6BlesWCEaN24sFAqF6NKlizh69Ogj6//888+iRYsWQqFQiDZt2ojw8PA6j5mIqq82rkMuF0FEtaImrsEtW7Zg5MiRCAsLg5+fH5YtW4ZffvkFsbGxcHR0LFP/8OHD6NmzJ0JDQ/Hyyy9j06ZN+Pzzz3Hy5EnNqNHajpmInky9m1SwPuNNi0i3auIa9PPzQ+fOnbFixQoAJVNeuLu745133sH06dPL1B88eDBycnKwc+dOTdkzzzwDHx8fhIWF1UnMRPRk6t2kgvVZaQ7HyQWJdKP02qvu31OFhYWIiYnBjBkzNGVSqRQBAQGIjo4ud5/o6GitKSiAkglJt2/fXm79hycjLZ0olfcNIt150ntHeQw22cnKKlm1m3PtEOlWVlYWrK2rvip7WloaVCpVuROMXrp0qdx9KpqQtHQwxcNCQ0Mxf/78MuW8bxDp3u3bt6t17yiPwSY7rq6uuHnzJiwtLSGRVDyktnTywZs3bxpUszXPS78Y4nkJIZCVlQVXV1ddh1KhhycjTU9PR+PGjZGQkFBjN9napq8/O/oYN2OuGxkZGWjUqJFmKaqaYLDJjlQq1Zrw8HGsrKz05gehKnhe+sXQzutJEgZ7e3vIZLJyJxgtnYD0YRVNSFpR/YomI7W2tta774O+/uzoY9yMuW5IpTU3kzvnhCeiekkul8PX11drglG1Wo3IyMgKJxjlhKREVB6DbdkhIv0XEhKCUaNGoVOnTujSpQuWLVuGnJwcjBkzBgAwcuRIuLm5ITQ0FADw3nvvoVevXvjyyy/Rr18/bN68GSdOnMA333yjy9MgIh176pMdhUKBuXPnGty6Wjwv/WKo5/WkBg8ejNTUVMyZMwdKpRI+Pj6IiIjQdEJOSEjQauru2rUrNm3ahFmzZuHjjz9Gs2bNsH379krNsQPo5/dBH2MG9DNuxlw3aiNmg51nh4iIiAhgnx0iIiIycEx2iIiIyKAx2SEiIiKDxmSHiIiIDNpTn+ysXLkSHh4eMDExgZ+fH44dO6brkJ7IvHnzIJFItF7e3t66DqvKDhw4gP79+8PV1RUSiaTM2kZCCMyZMwcuLi4wNTVFQEAArly5optgq+Bx5zV69Ogy378+ffroJlgDVdVr/pdffoG3tzdMTEzQtm1b7Nq1q44iva8qMa9ZswY9evSAra0tbG1tERAQoLP7WnXvr5s3b4ZEIsGAAQNqN8ByVDXm9PR0BAUFwcXFBQqFAs2bN6/zn5Gqxrxs2TK0aNECpqamcHd3x9SpU5Gfn18nsT7uHlieqKgodOzYEQqFAk2bNsW6deuq/sHiKbZ582Yhl8vF2rVrxfnz58X48eOFjY2NSE5O1nVo1TZ37lzRunVrkZSUJJKSksSgQYOEu7t7tY+lqx+RXbt2iZkzZ4qtW7cKAGLbtm1a2xctWiSsra3F9u3bxZkzZ8Qrr7wimjRpIvLy8nQSb2U97rxGjRol+vTpo/n+JSUliTt37ugmWANU1Wv+0KFDQiaTicWLF4sLFy6IWbNmCWNjY3H27Nl6G/OwYcPEypUrxalTp8TFixfF6NGjhbW1tfj333/rLObqxF0qPj5euLm5iR49eohXX321boK9p6oxFxQUiE6dOomXXnpJHDx4UMTHx4uoqChx+vTpehvzxo0bhUKhEBs3bhTx8fFi9+7dwsXFRUydOrVO4n3cPfBh165dE2ZmZiIkJERcuHBBLF++XMhkMhEREVGlz32qk50uXbqIoKAgzXuVSiVcXV1FaGhorX82gEq99u3bV6Xjzp07V7Rv317zftSoUaJx48bVilGXyc6DSr8WH330kRBCCLVaLZydncUXX3yhqZOeni4UCoX46aefdBVmlVWU7NT1Df5pUtVrftCgQaJfv35aZX5+fuLtt9+u1Tgf9KT3qeLiYmFpaSnWr19fWyGWqzpxFxcXi65du4pvv/1WJ9dCVWNevXq18PT0FIWFhXUVYhlVjTkoKEg8//zzWmUhISGiW7dutRpneSqT7Hz00UeidevWWmWDBw8WgYGBVfqsp/YxVmFhIWJiYhAQEKApk0qlCAgIQHR0dK1//oYNG7ReL774YrnlLVu2rPKxr1y5AldXV3h6eqKgoAB79+6tVoyzZs1CXl5etfatKZmZmQAAR0dH/PTTTxBCID4+HkqlUut7Z21tDT8/vzr53tW2qKgoODo6okWLFpg0aRJu376t65AMQnWu+ejoaK36ABAYGFhnP2c1cZ/Kzc1FUVFRjS6q+DjVjfuTTz6Bo6Mjxo4dWxdhaqlOzH/88Qf8/f0RFBQEJycntGnTBgsXLoRKpaq3MXft2hUxMTGaR13Xrl3Drl278NJLL9VJzFVVU9fgUzuDclpaGlQqlWYm1lJOTk64dOlSrX/+m2++qfX+yJEj2Lt3b5nyh+Xm5sLMzKzC7X5+fli3bh1atGiBpKQkzJ8/HwEBATh37hwsLS2rFKORkRGMjHT7I/Lbb78BAIKDgzFnzhwcOHAAxsbGAFDu906pVNZ5jEBJH6L8/HyYmpo+0XH69OmD119/HU2aNMHVq1fx8ccfo2/fvoiOjoZMJquhaJ9O1bnmlUqlTn/OauI+NW3aNLi6upb5hVGbqhP3wYMH8d133+H06dN1EGFZ1Yn52rVr+PvvvzF8+HDs2rULcXFxmDx5MoqKijB37tx6GfOwYcOQlpaG7t27QwiB4uJiTJw4ER9//HGtx1sdFV2DmZmZyMvLq/Q996lt2dEHzz77LNq0aYOYmBj07NkTZmZmmh/I33//Hf369YOrqysUCgW8vLywYMEC9O7dG//5z3/Qrl07BAYGwsPDAzdv3sTPP/8MALh+/TokEgmWLFmCb775Bl5eXlAoFOjcuTOOHz+u9fmlnZ0fJJFIEBwcrJmCX6FQoHXr1oiIiCgTf1RUFDp16gQTExN4eXnh66+/LveYj7Jx40YAQNu2bdGyZUvN+4ddunQJ0dHR+P3332FqaooWLVpg5syZWnUSExMxduxYzdesSZMmmDRpEgoLCys8XwBYt24dJBIJrl+/rinz8PDAyy+/jN27d6NTp04wNTXF119/DQD4/vvv8fzzz8PR0REKhQKtWrXC6tWry407JiYGvXr1gqWlJaysrPDll18iOzsbbdu2xalTp3DlyhUcP34cUVFRWvtNmDABNjY2ddapkPTTokWLsHnzZmzbtg0mJia6DqdCWVlZGDFiBNasWQN7e3tdh1NparUajo6O+Oabb+Dr64vBgwdj5syZCAsL03VoFYqKisLChQuxatUqnDx5Elu3bkV4eDgWLFig69Bq1VPbsmNvbw+ZTIbk5GSt8uTkZDg7O+soqrJu376Nvn37YsiQIXjzzTc1Ge66detgYWGBkJAQWFhY4O+//8acOXOQmZmJL774QrO/XC6HsbEx4uLitI67adMmZGVl4e2334ZEIsHixYvx+uuv49q1a5qWk4ocPHgQW7duxeTJk2FpaYmvvvoKAwcOREJCAuzs7AAAp06dQp8+feDi4oL58+dDpVLhk08+gYODQ6XP/datW9i3b5/m/dChQ/Hf//4XISEhAEq+Vy4uLvjnn3/Qo0cP5ObmwsfHB+PHj8fVq1exY8cOfPbZZ5pjdenSBenp6ZgwYQK8vb2RmJiIX3/9Fbm5uZDL5ZWOq1RsbCyGDh2Kt99+G+PHj0eLFi0AAKtXr0br1q3xyiuvwMjICDt27MDkyZOhVqsRFBSkdYzPPvsMrVu3xowZM2BjY4NTp04hIiICw4YNw4gRI/DJJ5/A3NwccXFxeOGFFwCUNF3/+uuvGDhwYL3+BVbfVOead3Z21uk94knuU0uWLMGiRYvw119/oV27drUZZhlVjfvq1au4fv06+vfvrylTq9UASlqYY2Nj4eXlVa9iBgAXFxcYGxtrtbq2bNkSSqUShYWF1bqv1HbMs2fPxogRIzBu3DgAJX9I5uTkYMKECZg5c6bWWnP1QUXXoJWVVdVa0qvUw8fAdOnSRQQHB2veq1Qq4ebmVicdlB8WFBRUpjNwr169BAARFhZWpn5ubm6ZsrfffluYmZmJ/Px8TdmwYcOEVCoV//vf/4QQJSMdAAg7OzutUT6///67ACB27NihKSuvgzIAIZfLRVxcnKbszJkzAoBYvny5pqx///7CzMxMJCYmasquXLkijIyMKt3pecmSJcLU1FTTie3y5csCgNi6datwdnYWS5YsEUII0bNnT2FpaSnkcrlWB2W1Wq35/8iRI4VUKhXHjx8v8zml9SrqkP39998LACI+Pl5T1rhxYwGg3BEB5X1vAgMDhaenp+Z9enq6ACCaN29eZgTZg3H7+voKAOL333/XlJWOYqhq53Wq+jU/aNAg8fLLL2uV+fv713kH5arepz7//HNhZWUloqOj6yLEclUl7ry8PHH27Fmt16uvviqef/55cfbsWVFQUFDvYhZCiBkzZojGjRsLlUqlKVu2bJlwcXGp9VhLVTXmjh07agZ7lNq0aZMwNTUVxcXFtRrrw1DJDspt2rTRKhs6dGiVOyg/1cnO5s2bhUKhEOvWrRMXLlwQEyZMEDY2NkKpVNZ5LBUlOwqF4rEXemZmpkhNTRU//vijACC+/fZbER8fLw4dOiRcXFyEVCoVKSkpQoj7yc7kyZO1jnHnzh0BQJMUCVFxsvPSSy+VicHKykozdLG4uFiYmpqKYcOGlanXv3//SiU7WVlZwtvbWwQEBAgAYunSpeLUqVOibdu24o033hCLFi0SNjY24ocffhAAhKenZ4VDz1UqlbCysnrsyI6qJjtNmjR57Hmkp6eL1NRUsXDhQgFA/Pvvv+LUqVNi8eLFAoAYM2aMOHXqlLhx44bIysoSH3zwgYiOjhbx8fHir7/+Eu7u7gKAOH/+vOaYAwcOFO7u7lpJEVXO4675ESNGiOnTp2vqHzp0SBgZGYklS5aIixcvirlz5+pk6HlVYl60aJGQy+Xi119/1ZrCICsrq85irk7cD9PFaKyqxpyQkCAsLS1FcHCwiI2NFTt37hSOjo7i008/rbcxz507V1haWoqffvpJXLt2TezZs0d4eXmJQYMG1Um8WVlZ4tSpU+LUqVNa9/YbN24IIYSYPn26GDFihKZ+6dDzDz/8UFy8eFGsXLmSQ8+rY/ny5aJRo0ZCLpeLLl26iCNHjugkjoqSnQdbAx507tw5MWDAAGFlZVVmuHqDBg2EXC4Xbm5uwsPDQ7i5uWn2K012Fi1aVOaYAMS8efM07ytKdiZOnFhm38aNG4vRo0cLIYS4deuWACDmzJlTpt7UqVMrleysW7eu3KH4rVq1EiYmJiI9PV3Mnj1bNGjQQAAQ3t7eIjY2ttxjKZVKAUDMnDnzkZ9Z1WTn4eGbpQ4ePCheeOEFYWZmVib+zZs3l3teo0aNErm5uaJ3797CwcFBGBsbi8aNG4uRI0cKhUIh5s+fL4S4P8T+Ub8k6NEedc336tVLjBo1Sqv+zz//LJo3by7kcrlo3bq1CA8Pr+OIqxZzaavjw6+5c+fW67gfpqtpGKoa8+HDh4Wfn59QKBTC09NTfPbZZ3XeQlKVmIuKisS8efOEl5eXMDExEe7u7mLy5Mni7t27dRLrvn37KrwHClHyfe/Vq1eZfXx8fIRcLheenp7i+++/r/LnPvXJTn1RUbLz8PwCQghx9+5dYWdnJ5o0aSKWLVsmduzYIfbu3Ss+//zzMo83Hp5npzTZeXCOmlIP3xArSnYenNOhVOPGjTU/rDWR7Hz88cePnH9o7dq1Qgghjhw5IgCINWvWVHisyiY78+bNKze2b7/9ttxk5+H5V4QQIi4uTigUCtG+fXsRFhYmwsPDxd69ezXnXXqMRYsWCQDiypUrj/1aDBw4UDRv3lwrlnPnzj12PyIiKvHUdlDWZ1FRUbh9+za2bt2Knj17asrj4+N1GNV9jo6OMDExKdMpGkC5ZQ8TQmDTpk147rnnMHny5DLbFyxYgI0bN2LMmDHw9PQEAJw7d67C4zk4OMDKyuqRdQDA1tYWQMn07zY2NpryGzduPDbmUjt27EBBQQH++OMPNGrUSFP+YEdrAJrOlufOnUPTpk0fecyRI0fi1VdfxfHjx7Fx40Z06NABrVu3rnRMRERPu/rV7ZoqpbTnvxBCU1ZYWIhVq1bpKiQtMpkMAQEB2L59O27duqUpj4uLw59//vnY/Q8dOoTr169jzJgxeOONN8q8Bg8ejH379uHWrVtwcHBAz549sXbtWiQkJGgdp/TrI5VKMWDAAOzYsQMnTpwo83ml9UoTkAMHDmi25eTkYP369VU69wePCQAZGRn4/vvvter17t0blpaWCA0NLTN8/MF9AaBv376wt7fH559/jv379z92LiYiItLGlh091LVrV9ja2mLUqFF49913IZFIsGHDhjK/JHVp3rx52LNnD7p164ZJkyZBpVJhxYoVaNOmzWMnDdu4cSNkMhn69etX7vZXXnkFM2fOxObNmxESEoKvvvoK3bt3R8eOHTFhwgQ0adIE169fR3h4uOazFi5ciD179qBXr16YMGECWrZsiaSkJPzyyy84ePAgbGxs0Lt3bzRq1Ahjx47Fhx9+CJlMhrVr18LBwaFMIlWR3r17Qy6Xo3///nj77beRnZ2NNWvWwNHREUlJSZp6VlZW+O9//4tx48ahc+fOGDZsGGxtbXHmzBnk5uZqJVjGxsYYMmQIVqxYAZlMhqFDh1YqFiIiKsGWHT1kZ2eHnTt3wsXFBbNmzcKSJUvw4osvYvHixboOTcPX1xd//vknbG1tMXv2bHz33Xf45JNP8MILLzxybpiioiL88ssv6Nq1a4XT27dp0wZNmjTBjz/+CABo3749jhw5gp49e2L16tV499138dtvv+GVV17R7OPm5oajR4/ijTfewMaNG/Huu+/ihx9+wLPPPquZkdrY2Bjbtm2Dl5cXZs+eja+++grjxo1DcHBwpc+7RYsW+PXXXyGRSPDBBx8gLCwMEyZMwHvvvVem7tixY/HHH3/AysoKCxYswLRp03Dy5En07du3TN2RI0cCAF544QW4uLhUOh4iIgIkoj41B5DBGzBgAM6fP48rV67oOhS9cubMGfj4+OCHH37AiBEjdB0OEZFeYcsO1ZqHFxG9cuUKdu3ahWeffVY3AemxNWvWwMLCAq+//rquQyEi0jvss0O1xtPTE6NHj4anpydu3LiB1atXQy6X46OPPtJ1aHpjx44duHDhAr755hsEBwfD3Nxc1yEREekdtuxQrenTpw9++uknvPPOO1i+fDk6d+6MAwcOoFmzZroOTW+88847mDdvHl566SXMnz9f1+HUqQMHDqB///5wdXWFRCLB9u3bH7tPVFQUOnbsCIVCgaZNm2LdunW1HicR1X9s2aFa8/Bwa6q6B1daf9rk5OSgffv2eOuttyr1+C4+Ph79+vXDxIkTsXHjRkRGRmLcuHFwcXFBYGBgHURMRPUVOygTUb0nkUiwbds2DBgwoMI606ZNQ3h4uNbkkUOGDEF6ejoiIiLqIEoiqq8MtmVHrVbj1q1bsLS0hEQi0XU4RE8dIQSysrLg6uoKqbT2n5hHR0cjICBAqywwMBBTpkypcJ+CggIUFBRo3qvVaty5cwd2dna8bxDpSG3cOww22bl16xbc3d11HQbRU+/mzZto2LBhrX+OUqmEk5OTVpmTkxMyMzORl5cHU1PTMvuEhoY+dX2hiPRFTd47DDbZsbS0BFDyxbKystJxNERPn8zMTLi7u2uuxfpoxowZCAkJ0bzPyMhAo0aNeN8g0qHauHcYbLJT2gRtZWXFmxaRDtXV4yBnZ2ckJydrlSUnJ8PKyqrcVh0AUCgUUCgUZcp53yDSvZq8d3DoOREZBH9/f0RGRmqV7d27F/7+/jqKiIjqCyY7RFQvZWdn4/Tp05rFXOPj43H69GnNoqwzZszQrBkGABMnTsS1a9fw0Ucf4dKlS1i1ahV+/vlnTJ06VRfhE1E9wmSHiOqlEydOoEOHDujQoQMAICQkBB06dMCcOXMAAElJSVqr0Tdp0gTh4eHYu3cv2rdvjy+//BLffvst59ghIsOdZyczMxPW1tbIyMjgs3eix1CrBTLzi2BjJq+xY+rjNaiPMRMZmtq4Dg22gzIRVU5WfhHG/3ACR67dgXsDU3T2aIAuHg3QuUkDeNqbc74ZItJ7THaInmJ3cwox6vtj+OffDADAzTt5uHknEVtPJgIA7Mzl6ORhi84eDdDZowFau1rBSMan30SkX5jsEOmRuJRs7DhzCz2a2aOTR4MnOlZKZj7e/O4oLidno4G5HF+P8EVuoQrH4+/g2PU7OH0zHbdzCrH7fDJ2ny8Z0m0ml6FjI1t08rDFML9GcLQ0qYnTIiKqVUx2iOo5IQQOXEnD2oPx2H85FQDw1d9XMKmXF6YENIfcqOotLf/ezcXwb4/ixu1cOFkpsHGcH5o6lkzg1au5AwCgoFiFc4kZOBZ/Fyeu38Hx63eQmV+Mg3FpOBiXhv904gzlRKQfmOwQ1VO5hcXYejIR3x+Kx9XUHACARAK0cbXG2cQMrIq6ioNxaVg22AeeDhaVPu611GwM//YokjLy4d7AFBvHPoNGdmZl6imMZPBt3AC+jRsA8IJaLXA5JQvHr9/FZWUW3GzKn6iPiKi+YbJDVM8kpufhh+jr+OloAjLziwEAFgojDOrkjlFdG6OxnTl2nU3CjK1n8c+/Gej31UHMe6UVBnVyf2xn4otJmRjx3VGkZRfCy8EcG8c9A2fryj2Kkkol8Ha2grczRykRkX5hskNUg4QQyMwvxq30PCRl5CG7QAVzuQymchnM5UYwe+D/pnIZFEZSSCQSCCEQc+Mu1h6Kx+7zyVCpS2aEaGxnhtFdPfCGb0NYmhhrPuelti7o0MgGIVvOIPrabUz77Sz2XUpF6OttYWte/vDxUwl3MWrtMWTmF6OVixU2jO0CO4uySyUQERkaJjtEVaBWCyTcycWt9DzcysjXJDWJ6flISs/DrfQ85BSqKn08mVQCM2MZ5EZS3M4p1JR39bLDW92a4DlvR8ik5bfWuFibYuM4P6z5v2tYsicWEeeVOHXzLpYO8kG3pvZadaOv3sa49ceRU6hCx0Y2+H5MF1ibGpd7XCIiQ8Nkh+gxCovVOHLtNnafV2LvhWSkZBU8dp8G5nK4WJvAQmGE/CIVcgpVyCtUIaewGLmFKhQWqwEAKrVAVkExUADIjaR4zccNY7p7VPpRkVQqwdu9vNCtqT3e3XwK11JzMPzbo5jQ0xPv924OhZEM+y6lYOKPMSgoVqNbUzt8M6ITzBW89Ino6cE7HhmcuzmFWHsoHj8dS4DCSAYfdxu0d7eGj7st2rhZwUz++B/77IJi7I9Nxe7zSuy7lFKSkNyjMJLCzdYUbjamcLE2gauNKVytTUv+tTGBi7UpTOWyRx6/WKVGbpEKuQUq5N5LgBramlZ7BuM2btYIf6cHFoRfwKajCfjmwDUcvJKGgb4NsejPiyhSCQS0dMSKYR1hYvzo2IiIDA2XiyCDkZpVgG//7xo2HLmB3AoeJUklQHMny3sJkA183G3QzNECRjIpUrMKEHkxGXsuJONgXJqm9QUAHCwVeLGVE3q3coK/lx0URvU3YdhzXolpv/2Du7lFmrL+7V2xdFB7GNfhhID6eA3qY8xEhsZglovIysrC7NmzsW3bNqSkpKBDhw743//+h86dOwMAkpOTMW3aNOzZswfp6eno2bMnli9fjmbNmukiXKrnlBn5CNt/FT8dS0DBvQSltasVgp9rCmtTY5z+Nx1nbqbjzM0MKDPzcUmZhUvKLGw+fhMAYGosg3sDU1xJycaDqb+HnRkCWzujd2tndHC3gbSCvjP1Te/WzvBxt8H7v5zB/11Jw9Au7vh0QNsK+/4QERk6nSQ748aNw7lz57Bhwwa4urrixx9/REBAAC5cuABXV1cMGDAAxsbG+P3332FlZYWlS5dqtpubm+siZKphRSo1Eu7k4mpKNq6l5SC3UAUvB3M0d7KEp4N5pVpObt7Jxer9V/HriX9RqCpJcnzcbfDuC03xXAtHzTDsrg901lVm5OPMveTn9M10/PNvBrILinE5ORsA0K6hNXq3ckJga2c0dbTQ23WhHK1M8MNbXZCaXcBZjonoqVfnj7Hy8vJgaWmJ33//Hf369dOU+/r6om/fvhg5ciRatGiBc+fOoXXr1gAAtVoNZ2dnLFy4EOPGjSv3uAUFBSgouN9xNDMzE+7u7myO1rE7OYW4lpqNa6k5uJqajaupObiWmo2EO7koVpf/oyeVAB525mjmZIFmjpZo5mSB5k6WaGJvDhNjGeLTcrBqXxy2nUrUHKNLkwZ49/lm6NbUrkoJilotcC2tJK62btZw5UR5NUYfHwnpY8xEhsYgHmMVFxdDpVLBxET7r01TU1McPHgQgwcPBgCt7VKpFAqFAgcPHqww2QkNDcX8+fNrL3B6LCEErqbm4FBcGv7vShpibtzR6jfyMFNjGTwdzOHlYAEzuQxxKdm4nJyFzPxiXEvLwbW0HM2aTEBJEuTewAw37+SiNE/q0cwewc81hZ+nXbVilkolaOpoqVkqgYiIDE+dJzuWlpbw9/fHggUL0LJlSzg5OeGnn35CdHQ0mjZtCm9vbzRq1AgzZszA119/DXNzc/z3v//Fv//+i6SkpAqPO2PGDISEhGjel7bsUO26nV1QslbSlTQcikvDrYz8MnXcbEzh6WAOT3tzeDlawNPeAl6O5nC2MinTCiOEQEpWAa4klyQ+V1KyNP/PzC/Gjdu5AIDnvR0R/HxTdGxkWyfnSURE+ksnfXY2bNiAt956C25ubpDJZOjYsSOGDh2KmJgYGBsbY+vWrRg7diwaNGgAmUyGgIAA9O3bF4964qZQKKBQcDbYqlKrBdKyC1BQrIaRTAKZRAKptORf2b33Mum9l0SCQpUax6/fwcErJa03F5IytY4nl0nRycMW3ZvZo6uXPZo7WVRqqHcpiUQCJysTOFmZoHuz+31tSpOguJRsOFoq0MyJLTFERFQ5Okl2vLy8sH//fuTk5CAzMxMuLi4YPHgwPD09AZT03zl9+jQyMjJQWFgIBwcH+Pn5oVOnTroIV68Vq9RIyshHYnoeEu/m4d+7eUhMz9W8v5Wer+ncW10tXazQo5k9ujW1RxePBo+dY6Y6HkyCiIiIqkKnkwqam5vD3Nwcd+/exe7du7F48WKt7dbW1gCAK1eu4MSJE1iwYIEuwtQ7t7MLsHDXJRy5dhtJGXmooB+whlQCGMukUAsBlVo8tr7zvVaXHvdabxws2aJGRET1l06Snd27d0MIgRYtWiAuLg4ffvghvL29MWbMGADAL7/8AgcHBzRq1Ahnz57Fe++9hwEDBqB37966CFevRJxTYua2s1rrLMllUrjamKChrRncbEw1s/+W/utsbaI12Zy4l/SoSv9VC6jVgEoICCHQwFyut0OyiYjo6aOTZCcjIwMzZszAv//+iwYNGmDgwIH47LPPYGxcsjBhUlISQkJCkJycDBcXF4wcORKzZ8/WRah6IyO3CPN2nMe2U4kAAG9nS8zq1wrNnSxgb6Go0oR4EokERjIJ1xIhIiKDwOUiDEBUbAqm/fYPkjMLIJUAk571wrsvNKvXSxqQ4dPHa1AfYyYyNAYxzw7VnOyCYnwWfhE/HUsAAHjam+PLQe3RgcOxiYiINOpuVUCqUdFXb6PPsgOaROetbk0Q/m4PJjpkcFauXAkPDw+YmJjAz88Px44de2T9ZcuWoUWLFjA1NYW7uzumTp2K/Pyy8z8R0dODLTt6Jq9QhcW7L+H7Q9cBAA1tTfHFG+3h71W9GYSJ6rMtW7YgJCQEYWFh8PPzw7JlyxAYGIjY2Fg4OjqWqb9p0yZMnz4da9euRdeuXXH58mWMHj0aEokES5cu1cEZEFF9wGRHD+QWFiMuJRuXkrIQtv8qrqXlAACGdmmEmf1awkLBbyMZpqVLl2L8+PGakZphYWEIDw/H2rVrMX369DL1Dx8+jG7dumHYsGEAAA8PDwwdOhRHjx6t07iJqH7hb8l6pEilRnxaDmKVWbicnIVL9/5NuJOLB7uRO1kp8PnAdni2Rdm/bIkMRWFhIWJiYjBjxgxNmVQqRUBAAKKjo8vdp2vXrvjxxx9x7NgxdOnSBdeuXcOuXbswYsSIcuuXt4AwERkeJjs6lFeowp4LSvx9KQWxyixcTc1Gkar8wXH2FnK0cLZE+4Y2eLunF6zNjOs4WqK6lZaWBpVKBScnJ61yJycnXLp0qdx9hg0bhrS0NHTv3h1CCBQXF2PixIn4+OOPy63PBYSJng5MduqYSi1w+Goatp1KxO5zSuQUqrS2WyiM0NzJAi2cLdHcyVLzr70FZykmepyoqCgsXLgQq1atgp+fH+Li4vDee+9hwYIF5c7VxQWEiZ4OTHbqgBAC529lYtupRPxx5hZSs+43m7s3MMUr7V3h29gWzZ0s4WZjytmJiQDY29tDJpMhOTlZqzw5ORnOzs7l7jN79myMGDEC48aNAwC0bdsWOTk5mDBhAmbOnAmpVHsAKhcQJno6MNmpRTfv5OKPM7ew7VQi4lKyNeW2ZsZ4uZ0rBnRwRcdGtkxuiMohl8vh6+uLyMhIDBgwAACgVqsRGRmJ4ODgcvfJzc0tk9DIZCWTaxro/KlEVAlMdmrB9bQcTN/6D45cu6MpUxhJEdDKCa/5uKFncwfIjTjFEdHjhISEYNSoUejUqRO6dOmCZcuWIScnRzM6a+TIkXBzc0NoaCgAoH///li6dCk6dOigeYw1e/Zs9O/fX5P0ENHTh8lODbukzMSI744hNasAEgnQ1csOA3zc0KeNMyxN2KmYqCoGDx6M1NRUzJkzB0qlEj4+PoiIiNB0Wk5ISNBqyZk1axYkEglmzZqFxMREODg4oH///vjss890dQpEVA9wbawadPpmOkatPYaMvCJ4O1tizchOcG9gViefTVTf6OM6U/oYM5Gh4dpY9djhq2kYv/4EcgpV6NDIButGd+HwcCIionqAyU4N+OtCMiZvOonCYjW6NbXDNyM6wZyzGhMREdUL/I38hH4/nYj3fz6DYrXAi62csHxoB5gYsyMkERFRfcFk5wlsOpqAmdvPQghggI8rvvhPexjLOMqKiIioPmGyU01f77+K0D9Lpqwf7tcIC15tA6mU8+UQERHVN0x2qkgIgS/3XMaKfXEAgIm9vDCtTwtODEhERFRPMdmpArVa4JOdF7Du8HUAwEd9WmDys011GxQRERE9EpOdKpix9Sy2nLgJAFjwamuM8PfQbUBERET0WEx2Kulqaja2nLgJqQRY8p/2eL1jQ12HRERERJXAoUOVlHg3DwDQ1NGCiQ4REZEeYbJTSalZBQAAR0sTHUdCREREVcFkp5JSNMmOQseREBERUVUw2amklKx8AICDFZMdIiIifcJkp5JKW3YcLJjsEBER6RMmO5WUmnnvMZYV++wQERHpEyY7lVT6GIt9doiIiPQLk51KSmUHZSIiIr3EZKcScgqKkVOoAsDHWERERPpGJ8lOVlYWpkyZgsaNG8PU1BRdu3bF8ePHNduzs7MRHByMhg0bwtTUFK1atUJYWJguQgVwv3OymVwGCwUnnSYiItInOvnNPW7cOJw7dw4bNmyAq6srfvzxRwQEBODChQtwc3NDSEgI/v77b/z444/w8PDAnj17MHnyZLi6uuKVV16p83hTMu8NO+cjLCIiIr1T5y07eXl5+O2337B48WL07NkTTZs2xbx589C0aVOsXr0aAHD48GGMGjUKzz77LDw8PDBhwgS0b98ex44dq+twAXBCQSIiIn1W58lOcXExVCoVTEy0+76Ympri4MGDAICuXbvijz/+QGJiIoQQ2LdvHy5fvozevXtXeNyCggJkZmZqvWpKCpeKICIi0lt1nuxYWlrC398fCxYswK1bt6BSqfDjjz8iOjoaSUlJAIDly5ejVatWaNiwIeRyOfr06YOVK1eiZ8+eFR43NDQU1tbWmpe7u3uNxVw6EouPsYiIiPSPTjoob9iwAUIIuLm5QaFQ4KuvvsLQoUMhlZaEs3z5chw5cgR//PEHYmJi8OWXXyIoKAh//fVXhcecMWMGMjIyNK+bN2/WWLyaOXa4VARRnVu5ciU8PDxgYmICPz+/xz7OTk9PR1BQEFxcXKBQKNC8eXPs2rWrjqIlovpIJx2Uvby8sH//fuTk5CAzMxMuLi4YPHgwPD09kZeXh48//hjbtm1Dv379AADt2rXD6dOnsWTJEgQEBJR7TIVCAYWidpIRrnhOpBtbtmxBSEgIwsLC4Ofnh2XLliEwMBCxsbFwdHQsU7+wsBAvvvgiHB0d8euvv8LNzQ03btyAjY1N3QdPRPWGTsdRm5ubw9zcHHfv3sXu3buxePFiFBUVoaioSNPKU0omk0GtVuskzpRMPsYi0oWlS5di/PjxGDNmDAAgLCwM4eHhWLt2LaZPn16m/tq1a3Hnzh0cPnwYxsbGAAAPD48Kj19QUICCggLN+5rs60dE9YdOHmPt3r0bERERiI+Px969e/Hcc8/B29sbY8aMgZWVFXr16oUPP/wQUVFRiI+Px7p16/DDDz/gtdde00W4XCqCSAcKCwsRExOj1ZorlUoREBCA6Ojocvf5448/4O/vj6CgIDg5OaFNmzZYuHAhVCpVufVrs68fEdUfOkl2MjIyEBQUBG9vb4wcORLdu3fH7t27NX+Jbd68GZ07d8bw4cPRqlUrLFq0CJ999hkmTpxY57EWFqtxN7cIAJMdorqUlpYGlUoFJycnrXInJycolcpy97l27Rp+/fVXqFQq7Nq1C7Nnz8aXX36JTz/9tNz6tdnXj4jqD508xho0aBAGDRpU4XZnZ2d8//33dRhRxdKyS5q4jaQS2JrJdRwNET2KWq2Go6MjvvnmG8hkMvj6+iIxMRFffPEF5s6dW6Z+bfb1I6L6g2sfPEbKA8POpVKJjqMhenrY29tDJpMhOTlZqzw5ORnOzs7l7uPi4gJjY2PIZDJNWcuWLaFUKlFYWAi5nH+wED2NuBDoY5QuFcFHWER1Sy6Xw9fXF5GRkZoytVqNyMhI+Pv7l7tPt27dEBcXpzWY4fLly3BxcWGiQ/QUY7LzGPdbdjjsnKiuhYSEYM2aNVi/fj0uXryISZMmIScnRzM6a+TIkZgxY4am/qRJk3Dnzh289957uHz5MsLDw7Fw4UIEBQXp6hSIqB7gY6zHSOHsyUQ6M3jwYKSmpmLOnDlQKpXw8fFBRESEptNyQkKC1jQV7u7u2L17N6ZOnYp27drBzc0N7733HqZNm6arUyCieoDJzmOkctg5kU4FBwcjODi43G1RUVFlyvz9/XHkyJFajoqI9AkfYz2GZvZkLhVBRESkl5jsPAZXPCciItJvTHYeo3SpCD7GIiIi0k9Mdh5BrRaaSQX5GIuIiEg/Mdl5hDu5hShWCwCAnTmTHSIiIn3EZOcRSh9hNTCXQ27ELxUREZE+4m/wR0jNZn8dIiIifcdk5xFKl4rghIJERET6i8nOI3DYORERkf5jsvMInFCQiIhI/zHZeYSUe0tFOFgw2SEiItJXTHYeQTOhIFt2iIiI9BaTnUe4PxqLfXaIiIj0FZOdCgghuFQEERGRAWCyU4HsgmLkFakA8DEWERGRPmOyU4HSYecWCiOYyY10HA0RERFVF5OdCpQ+wuKEgkRERPqNyU4FNMPOmewQERHpNSY7FdBMKMhkh4iISK8x2alAKpeKICIiMghMdiqQwqUiiIiIDAKTnQqU9tnhYywiIiL9xmSnAhyNRVQ/rFy5Eh4eHjAxMYGfnx+OHTtWqf02b94MiUSCAQMG1G6ARFTvMdmpQAr77BDp3JYtWxASEoK5c+fi5MmTaN++PQIDA5GSkvLI/a5fv44PPvgAPXr0qKNIiag+Y7JTjoJiFTLyigDwMRaRLi1duhTjx4/HmDFj0KpVK4SFhcHMzAxr166tcB+VSoXhw4dj/vz58PT0rMNoiai+0kmyk5WVhSlTpqBx48YwNTVF165dcfz4cc12iURS7uuLL76ok/hKR2LJZVLYmBnXyWcSkbbCwkLExMQgICBAUyaVShEQEIDo6OgK9/vkk0/g6OiIsWPHPvYzCgoKkJmZqfUiIsOjk2Rn3Lhx2Lt3LzZs2ICzZ8+id+/eCAgIQGJiIgAgKSlJ67V27VpIJBIMHDiwTuIrfYTlYKmARCKpk88kIm1paWlQqVRwcnLSKndycoJSqSx3n4MHD+K7777DmjVrKvUZoaGhsLa21rzc3d2fOG4iqn/qPNnJy8vDb7/9hsWLF6Nnz55o2rQp5s2bh6ZNm2L16tUAAGdnZ63X77//jueee67OmqTZOZlI/2RlZWHEiBFYs2YN7O3tK7XPjBkzkJGRoXndvHmzlqMkIl2o8xUui4uLoVKpYGKi3fHX1NQUBw8eLFM/OTkZ4eHhWL9+/SOPW1BQgIKCAs37J2mOTuWwcyKds7e3h0wmQ3JyslZ5cnIynJ2dy9S/evUqrl+/jv79+2vK1Go1AMDIyAixsbHw8vLS2kehUECh4HVOZOjqvGXH0tIS/v7+WLBgAW7dugWVSoUff/wR0dHRSEpKKlN//fr1sLS0xOuvv/7I49Zkc/SDj7GISDfkcjl8fX0RGRmpKVOr1YiMjIS/v3+Z+t7e3jh79ixOnz6teb3yyit47rnncPr0aT6iInqK6aTPzoYNGyCEgJubGxQKBb766isMHToUUmnZcNauXYvhw4eXaQl6WE02R3OpCKL6ISQkBGvWrMH69etx8eJFTJo0CTk5ORgzZgwAYOTIkZgxYwYAwMTEBG3atNF62djYwNLSEm3atIFcLtflqRCRDtX5YywA8PLywv79+5GTk4PMzEy4uLhg8ODBZfrk/N///R9iY2OxZcuWxx6zJpujuVQEUf0wePBgpKamYs6cOVAqlfDx8UFERISm03JCQkK5fyQRET1IJ8lOKXNzc5ibm+Pu3bvYvXs3Fi9erLX9u+++g6+vL9q3b1+ncXGpCKL6Izg4GMHBweVui4qKeuS+69atq/mAiEjv6CTZ2b17N4QQaNGiBeLi4vDhhx/C29tb0zQNlHQw/uWXX/Dll1/WeXylo7H4GIuIiEj/6aT9NyMjA0FBQfD29sbIkSPRvXt37N69G8bG9yfw27x5M4QQGDp0aJ3GplILpGXzMRYREZGh0EnLzqBBgzBo0KBH1pkwYQImTJhQRxHddzunAGoBSCSAnTk7NBIREek79ux7SOlILDtzOYxk/PIQERHpO/42f8j9OXbYX4eIiMgQMNl5SKqmczL76xARERkCJjsP4bBzIiIiw8Jk5yGcUJCIiMiwMNl5SGkHZQcLJjtERESGgMnOQ+637LCDMhERkSFgsvMQ9tkhIiIyLEx2HiCE4FIRREREBobJzgMy84tRUKwGwA7KREREhoLJzgNS7z3CsjQxgomxTMfREBERUU1gsvOA+7Mns1WHiIjIUDDZeUDpsHN2TiYiIjIcTHYewM7JREREhofJzgM47JyIiMjwMNl5AJeKICIiMjxMdh7Ax1hERESGh8nOA1Kz2UGZiIjI0DDZeUBKZkmfHQ49JyIiMhxMdu7JL1IhM78YAB9jERERGRImO/eUzrEjN5LCytRIx9EQUamVK1fCw8MDJiYm8PPzw7Fjxyqsu2bNGvTo0QO2trawtbVFQEDAI+sT0dOByc49Dw47l0gkOo6GiABgy5YtCAkJwdy5c3Hy5Em0b98egYGBSElJKbd+VFQUhg4din379iE6Ohru7u7o3bs3EhMT6zhyIqpPmOzcc38kFvvrENUXS5cuxfjx4zFmzBi0atUKYWFhMDMzw9q1a8utv3HjRkyePBk+Pj7w9vbGt99+C7VajcjIyDqOnIjqEyY799wficX+OkT1QWFhIWJiYhAQEKApk0qlCAgIQHR0dKWOkZubi6KiIjRo0KDc7QUFBcjMzNR6EZHhYbJzT2nLDkdiEdUPaWlpUKlUcHJy0ip3cnKCUqms1DGmTZsGV1dXrYTpQaGhobC2tta83N3dnzhuIqp/mOzcw6UiiAzLokWLsHnzZmzbtg0mJuW32M6YMQMZGRma182bN+s4SiKqCxx2dA+XiiCqX+zt7SGTyZCcnKxVnpycDGdn50fuu2TJEixatAh//fUX2rVrV2E9hUIBhYLXPJGhY8vOPVwqgqh+kcvl8PX11epcXNrZ2N/fv8L9Fi9ejAULFiAiIgKdOnWqi1CJqJ5jy849pS077LNDVH+EhIRg1KhR6NSpE7p06YJly5YhJycHY8aMAQCMHDkSbm5uCA0NBQB8/vnnmDNnDjZt2gQPDw9N3x4LCwtYWFjo7DyISLeY7ABQqQXu5PAxFlF9M3jwYKSmpmLOnDlQKpXw8fFBRESEptNyQkICpNL7DdSrV69GYWEh3njjDa3jzJ07F/PmzavL0ImoHmGyA+B2dgHUApBKADtzJjtE9UlwcDCCg4PL3RYVFaX1/vr167UfEBHpHZ302cnKysKUKVPQuHFjmJqaomvXrjh+/LhWnYsXL+KVV16BtbU1zM3N0blzZyQkJNRKPKWPsOwsFJBJOXsyERGRIdFJsjNu3Djs3bsXGzZswNmzZ9G7d28EBARopnS/evUqunfvDm9vb0RFReGff/7B7NmzKxw++qQ47JyIiMhw1fljrLy8PPz222/4/fff0bNnTwDAvHnzsGPHDqxevRqffvopZs6ciZdeegmLFy/W7Ofl5VVrMXGpCCIiIsNV5y07xcXFUKlUZVppTE1NcfDgQajVaoSHh6N58+YIDAyEo6Mj/Pz8sH379kce90mmfdfMscNh50RERAanzpMdS0tL+Pv7Y8GCBbh16xZUKhV+/PFHREdHIykpCSkpKcjOzsaiRYvQp08f7NmzB6+99hpef/117N+/v8LjPsm076mcUJCIiMhg6aTPzoYNGyCEgJubGxQKBb766isMHToUUqkUarUaAPDqq69i6tSp8PHxwfTp0/Hyyy8jLCyswmM+ybTvpX12OMcOERGR4dFJsuPl5YX9+/cjOzsbN2/exLFjx1BUVARPT0/Y29vDyMgIrVq10tqnZcuWjxyNpVAoYGVlpfWqrPuPsZjsEBERGRqdLhdhbm4OFxcX3L17F7t378arr74KuVyOzp07IzY2Vqvu5cuX0bhx41qJ4/6K5+yzQ0REZGh0Mqng7t27IYRAixYtEBcXhw8//BDe3t6aKeA//PBDDB48GD179sRzzz2HiIgI7Nixo8wEYjVBCHG/zw5bdoiIiAyOTlp2MjIyEBQUBG9vb4wcORLdu3fH7t27YWxsDAB47bXXEBYWhsWLF6Nt27b49ttv8dtvv6F79+41H0teEQpVJf2E2GeHiIjI8OikZWfQoEEYNGjQI+u89dZbeOutt2o9ltJWHWtTY5gYy2r984iIiKhu6bTPTn3A1c6JiIgMG5MdLhVBRERk0JjscKkIIiIig8ZkRzN7MoedExERGSImOxx2TkREZNCe+mQnlUtFEBERGbSnPtnhiudERESG7alPdlIzOfSciIjIkOlkUsH6QqUWeLG1E1KzCuBkxWSHiIjIED3VyY5MKsHSQT66DoOIiIhq0VP/GIuIiIgMG5MdIqrXVq5cCQ8PD5iYmMDPzw/Hjh17ZP1ffvkF3t7eMDExQdu2bbFr1646ipSI6ismO0RUb23ZsgUhISGYO3cuTp48ifbt2yMwMBApKSnl1j98+DCGDh2KsWPH4tSpUxgwYAAGDBiAc+fO1XHkRFSfSIQQQtdB1IbMzExYW1sjIyMDVlZWug6H6KlTE9egn58fOnfujBUrVgAA1Go13N3d8c4772D69Oll6g8ePBg5OTnYuXOnpuyZZ56Bj48PwsLC6iRmInoytXEdGmwH5dIcLjMzU8eRED2dSq+96v49VVhYiJiYGMyYMUNTJpVKERAQgOjo6HL3iY6ORkhIiFZZYGAgtm/fXm79goICFBQUaN5nZGRoxU5Ede9J7x3lMdhkJysrCwDg7u6u40iInm5ZWVmwtrau8n5paWlQqVRwcnLSKndycsKlS5fK3UepVJZbX6lUlls/NDQU8+fPL1PO+waR7t2+fbta947yGGyy4+rqips3b8LS0hISiaTCepmZmXB3d8fNmzcNqtma56VfDPG8hBDIysqCq6urrkOp0IwZM7RagtLT09G4cWMkJCTU2E22tunrz44+xs2Y60ZGRgYaNWqEBg0a1NgxDTbZkUqlaNiwYaXrW1lZ6c0PQlXwvPSLoZ3XkyQM9vb2kMlkSE5O1ipPTk6Gs7Nzufs4OztXqb5CoYBCUXZCUWtra737Pujrz44+xs2Y64ZUWnNjqDgai4jqJblcDl9fX0RGRmrK1Go1IiMj4e/vX+4+/v7+WvUBYO/evRXWJ6Kng8G27BCR/gsJCcGoUaPQqVMndOnSBcuWLUNOTg7GjBkDABg5ciTc3NwQGhoKAHjvvffQq1cvfPnll+jXrx82b96MEydO4JtvvtHlaRCRjj31yY5CocDcuXPLbcrWZzwv/WKo5/WkBg8ejNTUVMyZMwdKpRI+Pj6IiIjQdEJOSEjQauru2rUrNm3ahFmzZuHjjz9Gs2bNsH37drRp06ZSn6eP3wd9jBnQz7gZc92ojZgNdp4dIiIiIoB9doiIiMjAMdkhIiIig8Zkh4iIiAwakx0iIiIyaEx2iIiIyKA99cnOypUr4eHhARMTE/j5+eHYsWO6DumJzJs3DxKJROvl7e2t67Cq7MCBA+jfvz9cXV0hkUjKLOQohMCcOXPg4uICU1NTBAQE4MqVK7oJtgoed16jR48u8/3r06ePboI1UFW95n/55Rd4e3vDxMQEbdu2xa5du+oo0vuqEvOaNWvQo0cP2NrawtbWFgEBATq7r1X3/rp582ZIJBIMGDCgdgMsR1VjTk9PR1BQEFxcXKBQKNC8efM6/xmpaszLli1DixYtYGpqCnd3d0ydOhX5+fl1Euvj7oHliYqKQseOHaFQKNC0aVOsW7eu6h8snmKbN28WcrlcrF27Vpw/f16MHz9e2NjYiOTkZF2HVm1z584VrVu3FklJSZpXamqqrsOqsl27domZM2eKrVu3CgBi27ZtWtsXLVokrK2txfbt28WZM2fEK6+8Ipo0aSLy8vJ0E3AlPe68Ro0aJfr06aP1/btz545ugjVAVb3mDx06JGQymVi8eLG4cOGCmDVrljA2NhZnz56ttzEPGzZMrFy5Upw6dUpcvHhRjB49WlhbW4t///23zmKuTtyl4uPjhZubm+jRo4d49dVX6ybYe6oac0FBgejUqZN46aWXxMGDB0V8fLyIiooSp0+frrcxb9y4USgUCrFx40YRHx8vdu/eLVxcXMTUqVPrJN7H3QMfdu3aNWFmZiZCQkLEhQsXxPLly4VMJhMRERFV+tynOtnp0qWLCAoK0rxXqVTC1dVVhIaG6jCqJzN37lzRvn17XYdRox6+INRqtXB2dhZffPGFpiw9PV0oFArx008/6SDC6qko2anrG/zTpKrX/KBBg0S/fv20yvz8/MTbb79dq3E+6EnvU8XFxcLS0lKsX7++tkIsV3XiLi4uFl27dhXffvutTq6Fqsa8evVq4enpKQoLC+sqxDKqGnNQUJB4/vnntcpCQkJEt27dajXO8lQm2fnoo49E69attcoGDx4sAgMDq/RZT+1jrMLCQsTExCAgIEBTJpVKERAQgOjoaB1G9uSuXLkCV1dXeHp6Yvjw4UhISNB1SDUqPj4eSqVS63tnbW0NPz8/vf/eASVNto6OjmjRogUmTZqE27dv6zokg1Cdaz46OlqrPgAEBgbW2c9ZTdyncnNzUVRUVKMrSD9OdeP+5JNP4OjoiLFjx9ZFmFqqE/Mff/wBf39/BAUFwcnJCW3atMHChQuhUqnqbcxdu3ZFTEyM5lHXtWvXsGvXLrz00kt1EnNV1dQ1+NQuF5GWlgaVSqWZdr6Uk5MTLl26pKOonpyfnx/WrVuHFi1aICkpCfPnz0ePHj1w7tw5WFpa6jq8GqFUKgGg3O9d6TZ91adPH7z++uto0qQJrl69io8//hh9+/ZFdHQ0ZDKZrsPTa9W55pVKpU5/zmriPjVt2jS4urqW+YVRm6oT98GDB/Hdd9/h9OnTdRBhWdWJ+dq1a/j7778xfPhw7Nq1C3FxcZg8eTKKioowd+7cehnzsGHDkJaWhu7du0MIgeLiYkycOBEff/xxrcdbHRVdg5mZmcjLy4OpqWmljvPUJjuGqm/fvpr/t2vXDn5+fmjcuDF+/vlnnfy1RFUzZMgQzf/btm2Ldu3awcvLC1FRUXjhhRd0GBnpo0WLFmHz5s2IioqCiYmJrsOpUFZWFkaMGIE1a9bA3t5e1+FUmlqthqOjI7755hvIZDL4+voiMTERX3zxRZ0kO9URFRWFhQsXYtWqVfDz80NcXBzee+89LFiwALNnz9Z1eLXmqU127O3tIZPJkJycrFWenJwMZ2dnHUVV82xsbNC8eXPExcXpOpQaU/r9SU5OhouLi6Y8OTkZPj4+Ooqqdnh6esLe3h5xcXFMdp5Qda55Z2dnnd4jnuQ+tWTJEixatAh//fUX2rVrV5thllHVuK9evYrr16+jf//+mjK1Wg0AMDIyQmxsLLy8vOpVzADg4uICY2NjrVbXli1bQqlUorCwEHK5vN7FPHv2bIwYMQLjxo0DUPJHVU5ODiZMmICZM2dqLaxbH1R0DVpZWVW6VQd4ioeey+Vy+Pr6IjIyUlOmVqsRGRkJf39/HUZWs7Kzs3H16lWtpEDfNWnSBM7Ozlrfu8zMTBw9etSgvncA8O+//+L27dsG9f3Tlepc8/7+/lr1AWDv3r119nNW3fvU4sWLsWDBAkRERKBTp051EaqWqsbt7e2Ns2fP4vTp05rXK6+8gueeew6nT5+Gu7t7vYsZALp164a4uDhNYgYAly9fhouLS60nOtWNOTc3t0xCU5qsiXq4LniNXYNV6zttWDZv3iwUCoVYt26duHDhgpgwYYKwsbERSqVS16FV2/vvvy+ioqJEfHy8OHTokAgICBD29vYiJSVF16FVSVZWljh16pQ4deqUACCWLl0qTp06JW7cuCGEKBl6bmNjI37//Xfxzz//iFdffVUvhp4/6ryysrLEBx98IKKjo0V8fLz466+/RMeOHUWzZs1Efn6+rkM3CI+75keMGCGmT5+uqX/o0CFhZGQklixZIi5evCjmzp2rk6HnVYl50aJFQi6Xi19//VVrCoOsrKw6i7k6cT9MF6OxqhpzQkKCsLS0FMHBwSI2Nlbs3LlTODo6ik8//bTexjx37lxhaWkpfvrpJ3Ht2jWxZ88e4eXlJQYNGlQn8T7u3j59+nQxYsQITf3SoecffvihuHjxoli5ciWHnlfH8uXLRaNGjYRcLhddunQRR44c0XVIT2Tw4MHCxcVFyOVy4ebmJgYPHizi4uJ0HVaV7du3TwAo8xo1apQQomT4+ezZs4WTk5NQKBTihRdeELGxsboNuhIedV65ubmid+/ewsHBQRgbG4vGjRuL8ePH63XyXR896prv1auX5mes1M8//yyaN28u5HK5aN26tQgPD6/jiKsWc+PGjcv9GZs7d269jvthupqGoaoxHz58WPj5+QmFQiE8PT3FZ599JoqLi+ttzEVFRWLevHnCy8tLmJiYCHd3dzF58mRx9+7dOon1cff2UaNGiV69epXZx8fHR8jlcuHp6Sm+//77Kn+uRIh62G5FREREVEOe2j47RERE9HRgskNEREQGjckOERERGTQmO0RERGTQmOwQERGRQWOyQ0RERAaNyQ4REREZNCY7REREZNCY7BAREZFBY7JDREREBo3JDhERERm0/weAfG9ewq/JYgAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 640x480 with 4 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "# generate graphs\n",
    "model.graphs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 150.0000\n",
      "Accuracy: 162.5000\n",
      "Accuracy: 154.1667\n",
      "Accuracy: 159.3750\n",
      "Accuracy: 162.5000\n",
      "Accuracy: 160.4167\n",
      "Accuracy: 162.5000\n",
      "Accuracy: 157.8125\n",
      "Accuracy: 159.7222\n",
      "Accuracy: 161.2500\n",
      "Accuracy: 162.5000\n",
      "Accuracy: 162.5000\n",
      "Accuracy: 159.6154\n",
      "Accuracy: 159.8214\n",
      "Accuracy: 162.5000\n",
      "Accuracy: 161.7188\n",
      "Accuracy: 162.5000\n",
      "Accuracy: 161.8056\n",
      "Accuracy: 160.5263\n",
      "Accuracy: 161.2500\n",
      "Accuracy: 166.6667\n",
      "Accuracy: 165.9091\n",
      "Accuracy: 169.5652\n",
      "Accuracy: 169.7917\n",
      "Accuracy: 168.0000\n",
      "Accuracy: 169.2308\n",
      "Accuracy: 169.4444\n",
      "Accuracy: 169.6429\n",
      "Accuracy: 168.9655\n",
      "Accuracy: 168.3333\n",
      "Accuracy: 166.1290\n",
      "Accuracy: 165.6250\n",
      "Accuracy: 165.5303\n",
      "Accuracy: 165.8088\n",
      "Accuracy: 166.7857\n",
      "Accuracy: 165.9722\n",
      "Accuracy: 164.8649\n",
      "Accuracy: 164.4737\n",
      "Accuracy: 164.7436\n",
      "Accuracy: 163.7500\n",
      "Accuracy: 164.0244\n",
      "Accuracy: 164.8810\n",
      "Accuracy: 163.9535\n",
      "Accuracy: 164.7727\n",
      "Accuracy: 164.4444\n",
      "Accuracy: 164.6739\n",
      "Accuracy: 163.8298\n",
      "Accuracy: 164.0625\n",
      "Accuracy: 164.2857\n",
      "Accuracy: 164.0000\n",
      "Accuracy: 163.7255\n",
      "Accuracy: 165.3846\n",
      "Accuracy: 166.5094\n",
      "Accuracy: 166.6667\n",
      "Accuracy: 165.9091\n",
      "Accuracy: 166.5179\n",
      "Accuracy: 168.4211\n",
      "Accuracy: 169.8276\n",
      "Accuracy: 169.9153\n",
      "Accuracy: 169.1667\n",
      "Accuracy: 168.4426\n",
      "Accuracy: 167.3387\n",
      "Accuracy: 166.6667\n",
      "Accuracy: 166.4062\n",
      "Accuracy: 166.9231\n",
      "Accuracy: 166.6667\n",
      "Accuracy: 167.1642\n",
      "Accuracy: 167.6471\n",
      "Accuracy: 167.7536\n",
      "Accuracy: 167.5000\n",
      "Accuracy: 167.9577\n",
      "Accuracy: 168.0556\n",
      "Accuracy: 168.6644\n",
      "Accuracy: 167.7365\n",
      "Accuracy: 168.1667\n",
      "Accuracy: 168.7500\n",
      "Accuracy: 169.1558\n",
      "Accuracy: 168.2692\n",
      "Accuracy: 167.8797\n",
      "Accuracy: 168.2812\n",
      "Accuracy: 168.3642\n",
      "Accuracy: 167.8354\n",
      "Accuracy: 167.3193\n",
      "Accuracy: 167.1131\n",
      "Accuracy: 167.2059\n",
      "Accuracy: 167.2965\n",
      "Accuracy: 166.8103\n",
      "Accuracy: 166.9034\n",
      "Accuracy: 166.7135\n",
      "Accuracy: 166.8056\n",
      "Accuracy: 166.3462\n",
      "Accuracy: 166.1685\n",
      "Accuracy: 166.2634\n",
      "Accuracy: 165.8245\n",
      "Accuracy: 165.9211\n",
      "Accuracy: 166.0156\n",
      "Accuracy: 165.8505\n",
      "Accuracy: 165.9439\n",
      "Accuracy: 165.9091\n",
      "Accuracy: 165.7500\n",
      "Accuracy: 166.3366\n",
      "Accuracy: 166.4216\n",
      "Accuracy: 166.2621\n",
      "Accuracy: 166.5865\n",
      "Accuracy: 166.0714\n",
      "Accuracy: 166.1557\n",
      "Accuracy: 166.4720\n",
      "Accuracy: 166.5509\n",
      "Accuracy: 166.3991\n",
      "Accuracy: 168.2955\n",
      "Accuracy: 167.9054\n",
      "Accuracy: 168.1920\n",
      "Accuracy: 167.8097\n",
      "Accuracy: 167.6535\n",
      "Accuracy: 167.2826\n",
      "Accuracy: 168.6422\n",
      "Accuracy: 168.2692\n",
      "Accuracy: 168.7500\n",
      "Accuracy: 168.4874\n",
      "Accuracy: 168.3333\n",
      "Accuracy: 168.5950\n",
      "Accuracy: 168.8525\n",
      "Accuracy: 168.6992\n",
      "Accuracy: 168.7500\n",
      "Accuracy: 168.8000\n",
      "Accuracy: 168.4524\n",
      "Accuracy: 168.3071\n",
      "Accuracy: 167.7734\n",
      "Accuracy: 168.0233\n",
      "Accuracy: 167.7885\n",
      "Accuracy: 167.4618\n",
      "Accuracy: 167.5189\n",
      "Accuracy: 167.7632\n",
      "Accuracy: 168.0037\n",
      "Accuracy: 167.6852\n",
      "Accuracy: 167.5551\n",
      "Accuracy: 167.4270\n",
      "Accuracy: 167.1196\n",
      "Accuracy: 167.1763\n",
      "Accuracy: 166.9643\n",
      "Accuracy: 166.9326\n",
      "Accuracy: 166.8134\n",
      "Accuracy: 167.2203\n",
      "Accuracy: 167.1007\n",
      "Accuracy: 167.1552\n",
      "Accuracy: 167.3801\n",
      "Accuracy: 167.6020\n",
      "Accuracy: 167.5676\n",
      "Accuracy: 167.4497\n",
      "Accuracy: 167.3333\n",
      "Accuracy: 167.2185\n",
      "Accuracy: 167.5987\n",
      "Accuracy: 167.6471\n",
      "Accuracy: 167.4513\n",
      "Accuracy: 167.2581\n",
      "Accuracy: 166.9872\n",
      "Accuracy: 166.7197\n",
      "Accuracy: 166.4557\n",
      "Accuracy: 166.3522\n",
      "Accuracy: 166.4062\n",
      "Accuracy: 166.3043\n",
      "Accuracy: 166.2809\n",
      "Accuracy: 166.0276\n",
      "Accuracy: 166.5396\n",
      "Accuracy: 166.7424\n",
      "Accuracy: 166.9428\n",
      "Accuracy: 166.6916\n",
      "Accuracy: 166.8899\n",
      "Accuracy: 167.2337\n",
      "Accuracy: 167.1324\n",
      "Accuracy: 167.1784\n",
      "Accuracy: 167.2238\n",
      "Accuracy: 166.9798\n",
      "Accuracy: 166.7385\n",
      "Accuracy: 166.6429\n",
      "Accuracy: 166.5483\n",
      "Accuracy: 166.7373\n",
      "Accuracy: 166.9242\n",
      "Accuracy: 166.8296\n",
      "Accuracy: 166.5972\n",
      "Accuracy: 166.2983\n",
      "Accuracy: 166.3462\n",
      "Accuracy: 166.2568\n",
      "Accuracy: 166.4402\n",
      "Accuracy: 166.4865\n",
      "Accuracy: 166.3978\n",
      "Accuracy: 166.4439\n",
      "Accuracy: 167.0213\n",
      "Accuracy: 166.9312\n",
      "Accuracy: 166.8421\n",
      "Accuracy: 166.6885\n",
      "Accuracy: 166.7318\n",
      "Accuracy: 166.6451\n",
      "Accuracy: 166.5593\n",
      "Accuracy: 166.7308\n",
      "Accuracy: 167.1556\n",
      "Accuracy: 167.3223\n",
      "Accuracy: 167.4874\n",
      "Accuracy: 167.6508\n",
      "Accuracy: 167.8125\n",
      "Accuracy: 168.2214\n",
      "Accuracy: 168.6262\n",
      "Accuracy: 168.6576\n",
      "Accuracy: 168.4436\n",
      "Accuracy: 168.5976\n",
      "Accuracy: 168.8714\n",
      "Accuracy: 168.7802\n",
      "Accuracy: 168.6899\n",
      "Accuracy: 168.6005\n",
      "Accuracy: 168.5119\n",
      "Accuracy: 168.4242\n",
      "Accuracy: 168.1014\n",
      "Accuracy: 167.8991\n",
      "Accuracy: 167.7570\n",
      "Accuracy: 167.7907\n",
      "Accuracy: 167.7083\n",
      "Accuracy: 167.6267\n",
      "Accuracy: 167.4312\n",
      "Accuracy: 167.1233\n",
      "Accuracy: 167.2159\n",
      "Accuracy: 167.2511\n",
      "Accuracy: 167.1734\n",
      "Accuracy: 166.9843\n",
      "Accuracy: 166.9085\n",
      "Accuracy: 166.9444\n",
      "Accuracy: 166.8695\n",
      "Accuracy: 166.5749\n",
      "Accuracy: 166.6118\n",
      "Accuracy: 166.4301\n",
      "Accuracy: 166.9022\n",
      "Accuracy: 166.9372\n",
      "Accuracy: 166.9720\n",
      "Accuracy: 166.7918\n",
      "Accuracy: 166.7201\n",
      "Accuracy: 166.5426\n",
      "Accuracy: 166.5784\n",
      "Accuracy: 166.6139\n",
      "Accuracy: 166.3866\n",
      "Accuracy: 166.4226\n",
      "Accuracy: 166.2500\n",
      "Accuracy: 166.1826\n",
      "Accuracy: 166.1157\n",
      "Accuracy: 165.9465\n",
      "Accuracy: 165.7787\n",
      "Accuracy: 165.6122\n",
      "Accuracy: 165.3963\n",
      "Accuracy: 165.5364\n",
      "Accuracy: 165.7762\n",
      "Accuracy: 165.8133\n",
      "Accuracy: 165.7500\n",
      "Accuracy: 165.9861\n",
      "Accuracy: 166.2202\n",
      "Accuracy: 166.0573\n",
      "Accuracy: 166.0925\n",
      "Accuracy: 165.8824\n",
      "Accuracy: 165.6250\n",
      "Accuracy: 165.6128\n",
      "Accuracy: 165.6492\n",
      "Accuracy: 165.3958\n",
      "Accuracy: 165.7212\n",
      "Accuracy: 165.9483\n",
      "Accuracy: 165.9828\n",
      "Accuracy: 165.9696\n",
      "Accuracy: 166.0038\n",
      "Accuracy: 165.9434\n",
      "Accuracy: 165.8835\n",
      "Accuracy: 165.6367\n",
      "Accuracy: 165.5784\n",
      "Accuracy: 165.6134\n",
      "Accuracy: 165.4630\n",
      "Accuracy: 165.3137\n",
      "Accuracy: 165.3493\n",
      "Accuracy: 165.2015\n",
      "Accuracy: 165.5109\n",
      "Accuracy: 165.4091\n",
      "Accuracy: 165.7156\n",
      "Accuracy: 165.8394\n",
      "Accuracy: 165.6924\n",
      "Accuracy: 165.5466\n",
      "Accuracy: 165.4911\n",
      "Accuracy: 165.4359\n",
      "Accuracy: 165.5585\n",
      "Accuracy: 165.6802\n",
      "Accuracy: 165.6250\n",
      "Accuracy: 165.7456\n",
      "Accuracy: 165.7780\n",
      "Accuracy: 165.6359\n",
      "Accuracy: 165.6684\n",
      "Accuracy: 165.6142\n",
      "Accuracy: 165.5603\n",
      "Accuracy: 165.4210\n",
      "Accuracy: 165.5394\n",
      "Accuracy: 165.6570\n",
      "Accuracy: 165.6888\n",
      "Accuracy: 166.3983\n",
      "Accuracy: 166.2584\n",
      "Accuracy: 166.5404\n",
      "Accuracy: 166.4849\n",
      "Accuracy: 166.3880\n",
      "Accuracy: 166.3333\n",
      "Accuracy: 166.4452\n",
      "Accuracy: 166.3493\n",
      "Accuracy: 166.4604\n",
      "Accuracy: 166.5707\n",
      "Accuracy: 166.8443\n",
      "Accuracy: 166.6667\n",
      "Accuracy: 166.6531\n",
      "Accuracy: 166.4367\n",
      "Accuracy: 167.1117\n",
      "Accuracy: 167.1371\n",
      "Accuracy: 167.2428\n",
      "Accuracy: 167.3478\n",
      "Accuracy: 167.5319\n",
      "Accuracy: 167.4761\n",
      "Accuracy: 167.3413\n",
      "Accuracy: 167.2864\n",
      "Accuracy: 167.1530\n",
      "Accuracy: 166.9418\n",
      "Accuracy: 166.9671\n",
      "Accuracy: 167.2266\n",
      "Accuracy: 167.5623\n",
      "Accuracy: 167.6630\n",
      "Accuracy: 167.4536\n",
      "Accuracy: 167.3997\n",
      "Accuracy: 167.2692\n",
      "Accuracy: 167.2163\n",
      "Accuracy: 167.1636\n",
      "Accuracy: 167.0351\n",
      "Accuracy: 166.9833\n",
      "Accuracy: 167.0833\n",
      "Accuracy: 166.9562\n",
      "Accuracy: 166.7922\n",
      "Accuracy: 166.8919\n",
      "Accuracy: 167.0659\n",
      "Accuracy: 167.0896\n",
      "Accuracy: 167.1131\n",
      "Accuracy: 167.0623\n",
      "Accuracy: 166.9379\n",
      "Accuracy: 167.0354\n",
      "Accuracy: 167.1324\n",
      "Accuracy: 167.0088\n",
      "Accuracy: 166.8860\n",
      "Accuracy: 166.8367\n",
      "Accuracy: 166.7878\n",
      "Accuracy: 166.7029\n",
      "Accuracy: 167.0159\n",
      "Accuracy: 167.2550\n",
      "Accuracy: 167.1336\n",
      "Accuracy: 167.2278\n",
      "Accuracy: 167.1786\n",
      "Accuracy: 167.0584\n",
      "Accuracy: 166.9389\n",
      "Accuracy: 167.0326\n",
      "Accuracy: 167.1257\n",
      "Accuracy: 167.0775\n",
      "Accuracy: 167.0997\n",
      "Accuracy: 167.0518\n",
      "Accuracy: 167.2835\n",
      "Accuracy: 167.2354\n",
      "Accuracy: 167.2569\n",
      "Accuracy: 167.2784\n",
      "Accuracy: 167.1616\n",
      "Accuracy: 167.2176\n",
      "Accuracy: 167.1703\n",
      "Accuracy: 167.0548\n",
      "Accuracy: 167.2131\n",
      "Accuracy: 167.0981\n",
      "Accuracy: 167.0516\n",
      "Accuracy: 167.0054\n",
      "Accuracy: 167.0608\n",
      "Accuracy: 167.0148\n",
      "Accuracy: 166.9019\n",
      "Accuracy: 166.9236\n",
      "Accuracy: 167.0120\n",
      "Accuracy: 166.9000\n",
      "Accuracy: 166.9215\n",
      "Accuracy: 166.8767\n",
      "Accuracy: 166.7659\n",
      "Accuracy: 166.8536\n",
      "Accuracy: 166.8092\n",
      "Accuracy: 166.6995\n",
      "Accuracy: 166.5903\n",
      "Accuracy: 166.6123\n",
      "Accuracy: 166.6992\n",
      "Accuracy: 166.5909\n",
      "Accuracy: 166.8070\n",
      "Accuracy: 166.6990\n",
      "Accuracy: 166.8492\n",
      "Accuracy: 167.1272\n",
      "Accuracy: 167.3397\n",
      "Accuracy: 167.2954\n",
      "Accuracy: 167.3151\n",
      "Accuracy: 167.1438\n",
      "Accuracy: 167.1003\n",
      "Accuracy: 167.1203\n",
      "Accuracy: 167.2033\n",
      "Accuracy: 167.2229\n",
      "Accuracy: 167.2425\n",
      "Accuracy: 167.1992\n",
      "Accuracy: 167.0938\n",
      "Accuracy: 167.1758\n",
      "Accuracy: 167.1331\n",
      "Accuracy: 167.0906\n",
      "Accuracy: 167.0792\n",
      "Accuracy: 167.2840\n",
      "Accuracy: 167.3030\n",
      "Accuracy: 167.2604\n",
      "Accuracy: 167.4632\n",
      "Accuracy: 167.6039\n",
      "Accuracy: 167.5000\n",
      "Accuracy: 167.4574\n",
      "Accuracy: 167.3544\n",
      "Accuracy: 167.3123\n",
      "Accuracy: 167.4517\n",
      "Accuracy: 167.4096\n",
      "Accuracy: 167.3978\n",
      "Accuracy: 167.4760\n",
      "Accuracy: 167.4342\n",
      "Accuracy: 167.3926\n",
      "Accuracy: 167.4107\n",
      "Accuracy: 167.3100\n",
      "Accuracy: 167.2097\n",
      "Accuracy: 167.2281\n",
      "Accuracy: 167.1285\n",
      "Accuracy: 167.0882\n",
      "Accuracy: 166.9894\n",
      "Accuracy: 166.9496\n",
      "Accuracy: 167.0269\n",
      "Accuracy: 167.1037\n",
      "Accuracy: 167.1802\n",
      "Accuracy: 167.1404\n",
      "Accuracy: 167.2743\n",
      "Accuracy: 167.1767\n",
      "Accuracy: 167.1371\n",
      "Accuracy: 167.2126\n",
      "Accuracy: 167.2878\n",
      "Accuracy: 167.1911\n",
      "Accuracy: 167.2089\n",
      "Accuracy: 167.2836\n",
      "Accuracy: 167.2443\n",
      "Accuracy: 167.1202\n",
      "Accuracy: 167.1380\n",
      "Accuracy: 167.2122\n",
      "Accuracy: 167.2297\n",
      "Accuracy: 167.3034\n",
      "Accuracy: 167.2646\n",
      "Accuracy: 167.3098\n",
      "Accuracy: 167.2712\n",
      "Accuracy: 167.2327\n",
      "Accuracy: 167.1944\n",
      "Accuracy: 167.2672\n",
      "Accuracy: 167.3396\n",
      "Accuracy: 167.3013\n",
      "Accuracy: 167.3183\n",
      "Accuracy: 167.3352\n",
      "Accuracy: 167.2971\n",
      "Accuracy: 167.4781\n",
      "Accuracy: 167.3854\n",
      "Accuracy: 167.3475\n",
      "Accuracy: 167.3098\n",
      "Accuracy: 167.2722\n",
      "Accuracy: 167.1807\n",
      "Accuracy: 167.0896\n",
      "Accuracy: 167.3761\n",
      "Accuracy: 167.3925\n",
      "Accuracy: 167.3552\n",
      "Accuracy: 167.2912\n",
      "Accuracy: 167.5214\n",
      "Accuracy: 167.5640\n",
      "Accuracy: 167.5798\n",
      "Accuracy: 167.4894\n",
      "Accuracy: 167.5583\n",
      "Accuracy: 167.5740\n",
      "Accuracy: 167.5897\n",
      "Accuracy: 167.5000\n",
      "Accuracy: 167.5683\n",
      "Accuracy: 167.5314\n",
      "Accuracy: 167.4948\n",
      "Accuracy: 167.4061\n",
      "Accuracy: 167.3698\n",
      "Accuracy: 167.2817\n",
      "Accuracy: 167.2459\n",
      "Accuracy: 167.4689\n",
      "Accuracy: 167.5362\n",
      "Accuracy: 167.5515\n",
      "Accuracy: 167.7212\n",
      "Accuracy: 167.7875\n",
      "Accuracy: 167.7510\n",
      "Accuracy: 167.6636\n",
      "Accuracy: 167.6276\n",
      "Accuracy: 167.6935\n",
      "Accuracy: 167.7083\n",
      "Accuracy: 167.6724\n",
      "Accuracy: 167.6872\n",
      "Accuracy: 167.7020\n",
      "Accuracy: 167.6663\n",
      "Accuracy: 167.5805\n",
      "Accuracy: 167.5452\n",
      "Accuracy: 167.5100\n",
      "Accuracy: 167.5250\n",
      "Accuracy: 167.4900\n",
      "Accuracy: 167.5548\n",
      "Accuracy: 167.4702\n",
      "Accuracy: 167.4355\n",
      "Accuracy: 167.6485\n",
      "Accuracy: 167.6136\n",
      "Accuracy: 167.5542\n",
      "Accuracy: 167.4705\n",
      "Accuracy: 167.3870\n",
      "Accuracy: 167.3529\n",
      "Accuracy: 167.3190\n",
      "Accuracy: 167.3828\n",
      "Accuracy: 167.4464\n",
      "Accuracy: 167.5097\n",
      "Accuracy: 167.6699\n",
      "Accuracy: 167.8295\n",
      "Accuracy: 167.7950\n",
      "Accuracy: 167.9537\n",
      "Accuracy: 167.9672\n",
      "Accuracy: 167.9327\n",
      "Accuracy: 167.8983\n",
      "Accuracy: 168.0556\n",
      "Accuracy: 168.1644\n",
      "Accuracy: 168.2729\n",
      "Accuracy: 168.2857\n",
      "Accuracy: 168.2510\n",
      "Accuracy: 168.2638\n",
      "Accuracy: 168.3239\n",
      "Accuracy: 168.2892\n",
      "Accuracy: 168.2547\n",
      "Accuracy: 168.2203\n",
      "Accuracy: 168.1861\n",
      "Accuracy: 168.1285\n",
      "Accuracy: 168.3287\n",
      "Accuracy: 168.3879\n",
      "Accuracy: 168.6800\n",
      "Accuracy: 168.5987\n",
      "Accuracy: 168.6571\n",
      "Accuracy: 168.6688\n",
      "Accuracy: 168.7269\n",
      "Accuracy: 168.6922\n",
      "Accuracy: 168.6577\n",
      "Accuracy: 168.7155\n",
      "Accuracy: 168.7270\n",
      "Accuracy: 168.7385\n",
      "Accuracy: 168.7042\n",
      "Accuracy: 168.7614\n",
      "Accuracy: 168.8184\n",
      "Accuracy: 168.8752\n",
      "Accuracy: 168.8864\n",
      "Accuracy: 168.8748\n",
      "Accuracy: 168.8406\n",
      "Accuracy: 168.7613\n",
      "Accuracy: 168.7726\n",
      "Accuracy: 168.6486\n",
      "Accuracy: 168.5926\n",
      "Accuracy: 168.5144\n",
      "Accuracy: 168.5260\n",
      "Accuracy: 168.5376\n",
      "Accuracy: 168.4598\n",
      "Accuracy: 168.4269\n",
      "Accuracy: 168.3496\n",
      "Accuracy: 168.2726\n",
      "Accuracy: 168.3289\n",
      "Accuracy: 168.2965\n",
      "Accuracy: 168.2200\n",
      "Accuracy: 168.2319\n",
      "Accuracy: 168.1998\n",
      "Accuracy: 168.1239\n",
      "Accuracy: 168.1360\n",
      "Accuracy: 168.1042\n",
      "Accuracy: 167.9633\n",
      "Accuracy: 167.8883\n",
      "Accuracy: 167.9007\n",
      "Accuracy: 167.8261\n",
      "Accuracy: 167.8819\n",
      "Accuracy: 167.8076\n",
      "Accuracy: 167.8633\n",
      "Accuracy: 167.8325\n",
      "Accuracy: 167.8233\n",
      "Accuracy: 167.7926\n",
      "Accuracy: 167.8050\n",
      "Accuracy: 167.8602\n",
      "Accuracy: 167.8296\n",
      "Accuracy: 167.8846\n",
      "Accuracy: 167.9821\n",
      "Accuracy: 167.9089\n",
      "Accuracy: 167.9634\n",
      "Accuracy: 167.9754\n",
      "Accuracy: 167.9449\n",
      "Accuracy: 167.9146\n",
      "Accuracy: 167.9265\n",
      "Accuracy: 167.9174\n",
      "Accuracy: 168.0135\n",
      "Accuracy: 168.0252\n",
      "Accuracy: 168.0579\n",
      "Accuracy: 167.9858\n",
      "Accuracy: 167.8930\n",
      "Accuracy: 167.8214\n",
      "Accuracy: 167.8333\n",
      "Accuracy: 167.7621\n",
      "Accuracy: 167.7326\n",
      "Accuracy: 167.7446\n",
      "Accuracy: 167.7566\n",
      "Accuracy: 167.7066\n",
      "Accuracy: 167.7186\n",
      "Accuracy: 167.7306\n",
      "Accuracy: 167.7015\n",
      "Accuracy: 167.7545\n",
      "Accuracy: 167.7254\n",
      "Accuracy: 167.6964\n",
      "Accuracy: 167.6675\n",
      "Accuracy: 167.5979\n",
      "Accuracy: 167.5692\n",
      "Accuracy: 167.5407\n",
      "Accuracy: 167.4716\n",
      "Accuracy: 167.4838\n",
      "Accuracy: 167.5364\n",
      "Accuracy: 167.5485\n",
      "Accuracy: 167.5605\n",
      "Accuracy: 167.7134\n",
      "Accuracy: 167.6447\n",
      "Accuracy: 167.5762\n",
      "Accuracy: 167.5080\n",
      "Accuracy: 167.5200\n",
      "Accuracy: 167.4521\n",
      "Accuracy: 167.4641\n",
      "Accuracy: 167.4363\n",
      "Accuracy: 167.3688\n",
      "Accuracy: 167.3413\n",
      "Accuracy: 167.2345\n",
      "Accuracy: 167.1282\n",
      "Accuracy: 167.0616\n",
      "Accuracy: 166.9953\n",
      "Accuracy: 166.9291\n",
      "Accuracy: 166.8632\n",
      "Accuracy: 167.0330\n",
      "Accuracy: 167.0063\n",
      "Accuracy: 166.9797\n",
      "Accuracy: 167.0312\n",
      "Accuracy: 167.1217\n",
      "Accuracy: 167.0950\n",
      "Accuracy: 167.1073\n",
      "Accuracy: 167.0419\n",
      "Accuracy: 166.9186\n",
      "Accuracy: 166.8150\n",
      "Accuracy: 166.7890\n",
      "Accuracy: 166.8403\n",
      "Accuracy: 166.7758\n",
      "Accuracy: 166.7885\n",
      "Accuracy: 166.8395\n",
      "Accuracy: 166.8903\n",
      "Accuracy: 166.9410\n",
      "Accuracy: 166.9534\n",
      "Accuracy: 166.8893\n",
      "Accuracy: 166.9398\n",
      "Accuracy: 167.1043\n",
      "Accuracy: 167.0783\n",
      "Accuracy: 167.1093\n",
      "Accuracy: 167.0076\n",
      "Accuracy: 166.9062\n",
      "Accuracy: 166.8051\n",
      "Accuracy: 166.7798\n",
      "Accuracy: 166.7169\n",
      "Accuracy: 166.8045\n",
      "Accuracy: 166.8168\n",
      "Accuracy: 166.8291\n",
      "Accuracy: 166.8039\n",
      "Accuracy: 166.7788\n",
      "Accuracy: 166.8284\n",
      "Accuracy: 166.8033\n",
      "Accuracy: 166.8155\n",
      "Accuracy: 166.8648\n",
      "Accuracy: 166.8398\n",
      "Accuracy: 166.7778\n",
      "Accuracy: 166.6790\n",
      "Accuracy: 166.5805\n",
      "Accuracy: 166.4823\n",
      "Accuracy: 166.4580\n",
      "Accuracy: 166.4706\n",
      "Accuracy: 166.4097\n",
      "Accuracy: 166.3856\n",
      "Accuracy: 166.4348\n",
      "Accuracy: 166.4839\n",
      "Accuracy: 166.4599\n",
      "Accuracy: 166.4359\n",
      "Accuracy: 166.4483\n",
      "Accuracy: 166.4244\n",
      "Accuracy: 166.3643\n",
      "Accuracy: 166.3043\n",
      "Accuracy: 166.2808\n",
      "Accuracy: 166.2934\n",
      "Accuracy: 166.2698\n",
      "Accuracy: 166.2824\n",
      "Accuracy: 166.2590\n",
      "Accuracy: 166.2356\n",
      "Accuracy: 166.3558\n",
      "Accuracy: 166.2966\n",
      "Accuracy: 166.2732\n",
      "Accuracy: 166.2143\n",
      "Accuracy: 166.1198\n",
      "Accuracy: 166.0256\n",
      "Accuracy: 165.9673\n",
      "Accuracy: 165.9446\n",
      "Accuracy: 165.9220\n",
      "Accuracy: 165.9348\n",
      "Accuracy: 165.9123\n",
      "Accuracy: 165.8898\n",
      "Accuracy: 165.7969\n",
      "Accuracy: 165.7746\n",
      "Accuracy: 165.7876\n",
      "Accuracy: 165.8006\n",
      "Accuracy: 165.7784\n",
      "Accuracy: 165.8613\n",
      "Accuracy: 165.8392\n",
      "Accuracy: 165.7821\n",
      "Accuracy: 165.6904\n",
      "Accuracy: 165.6337\n",
      "Accuracy: 165.5772\n",
      "Accuracy: 165.5903\n",
      "Accuracy: 165.7074\n",
      "Accuracy: 165.6856\n",
      "Accuracy: 165.6293\n",
      "Accuracy: 165.6423\n",
      "Accuracy: 165.5862\n",
      "Accuracy: 165.5647\n",
      "Accuracy: 165.5433\n",
      "Accuracy: 165.5907\n",
      "Accuracy: 165.6036\n",
      "Accuracy: 165.5137\n",
      "Accuracy: 165.4241\n",
      "Accuracy: 165.3347\n",
      "Accuracy: 165.2797\n",
      "Accuracy: 165.3270\n",
      "Accuracy: 165.3061\n",
      "Accuracy: 165.2514\n",
      "Accuracy: 165.1967\n",
      "Accuracy: 165.1762\n",
      "Accuracy: 165.1218\n",
      "Accuracy: 165.0676\n",
      "Accuracy: 165.1147\n",
      "Accuracy: 165.1617\n",
      "Accuracy: 165.0572\n",
      "Accuracy: 164.9698\n",
      "Accuracy: 164.8826\n",
      "Accuracy: 164.9296\n",
      "Accuracy: 164.9096\n",
      "Accuracy: 164.8563\n",
      "Accuracy: 164.8364\n",
      "Accuracy: 164.7833\n",
      "Accuracy: 164.7636\n",
      "Accuracy: 164.7773\n",
      "Accuracy: 164.7244\n",
      "Accuracy: 164.6718\n",
      "Accuracy: 164.6854\n",
      "Accuracy: 164.6660\n",
      "Accuracy: 164.7127\n",
      "Accuracy: 164.7592\n",
      "Accuracy: 164.6739\n",
      "Accuracy: 164.5888\n",
      "Accuracy: 164.5039\n",
      "Accuracy: 164.4521\n",
      "Accuracy: 164.4659\n",
      "Accuracy: 164.5124\n",
      "Accuracy: 164.4608\n",
      "Accuracy: 164.4093\n",
      "Accuracy: 164.5209\n",
      "Accuracy: 164.5020\n",
      "Accuracy: 164.5156\n",
      "Accuracy: 164.4643\n",
      "Accuracy: 164.3807\n",
      "Accuracy: 164.4592\n",
      "Accuracy: 164.4082\n",
      "Accuracy: 164.3249\n",
      "Accuracy: 164.2419\n",
      "Accuracy: 164.1591\n",
      "Accuracy: 164.1409\n",
      "Accuracy: 164.0585\n",
      "Accuracy: 164.0404\n",
      "Accuracy: 164.0545\n",
      "Accuracy: 164.0365\n",
      "Accuracy: 163.9546\n",
      "Accuracy: 163.9049\n",
      "Accuracy: 163.9509\n",
      "Accuracy: 163.9331\n",
      "Accuracy: 163.9472\n",
      "Accuracy: 163.9295\n",
      "Accuracy: 163.9435\n",
      "Accuracy: 163.9575\n",
      "Accuracy: 163.8766\n",
      "Accuracy: 163.7958\n",
      "Accuracy: 163.7153\n",
      "Accuracy: 163.6980\n",
      "Accuracy: 163.6807\n",
      "Accuracy: 163.6950\n",
      "Accuracy: 163.9604\n",
      "Accuracy: 163.9743\n",
      "Accuracy: 164.0194\n",
      "Accuracy: 164.0957\n",
      "Accuracy: 164.0469\n",
      "Accuracy: 164.0293\n",
      "Accuracy: 164.0118\n",
      "Accuracy: 164.0567\n",
      "Accuracy: 164.1636\n",
      "Accuracy: 164.0839\n",
      "Accuracy: 164.0354\n",
      "Accuracy: 164.0180\n",
      "Accuracy: 163.9697\n",
      "Accuracy: 163.9524\n",
      "Accuracy: 163.9352\n",
      "Accuracy: 163.9180\n",
      "Accuracy: 163.9317\n",
      "Accuracy: 163.8838\n",
      "Accuracy: 163.9281\n",
      "Accuracy: 163.8804\n",
      "Accuracy: 163.8634\n",
      "Accuracy: 163.8158\n",
      "Accuracy: 163.7683\n",
      "Accuracy: 163.7515\n",
      "Accuracy: 163.7652\n",
      "Accuracy: 163.7789\n",
      "Accuracy: 163.7926\n",
      "Accuracy: 163.7758\n",
      "Accuracy: 163.7591\n",
      "Accuracy: 163.6818\n",
      "Accuracy: 163.6350\n",
      "Accuracy: 163.6185\n",
      "Accuracy: 163.6322\n",
      "Accuracy: 163.6460\n",
      "Accuracy: 163.5693\n",
      "Accuracy: 163.4928\n",
      "Accuracy: 163.4916\n",
      "Accuracy: 163.5054\n",
      "Accuracy: 163.5492\n",
      "Accuracy: 163.5329\n",
      "Accuracy: 163.4868\n",
      "Accuracy: 163.5006\n",
      "Accuracy: 163.5442\n",
      "Accuracy: 163.4684\n",
      "Accuracy: 163.3929\n",
      "Accuracy: 163.3769\n",
      "Accuracy: 163.3610\n",
      "Accuracy: 163.4045\n",
      "Accuracy: 163.3590\n",
      "Accuracy: 163.3284\n",
      "Accuracy: 163.3126\n",
      "Accuracy: 163.2674\n",
      "Accuracy: 163.2518\n",
      "Accuracy: 163.2067\n",
      "Accuracy: 163.1912\n",
      "Accuracy: 163.1169\n",
      "Accuracy: 163.0428\n",
      "Accuracy: 162.9689\n",
      "Accuracy: 162.9537\n",
      "Accuracy: 162.9971\n",
      "Accuracy: 163.0403\n",
      "Accuracy: 163.0834\n",
      "Accuracy: 163.1556\n",
      "Accuracy: 163.1403\n",
      "Accuracy: 163.1541\n",
      "Accuracy: 163.1678\n",
      "Accuracy: 163.1526\n",
      "Accuracy: 163.1663\n",
      "Accuracy: 163.1510\n",
      "Accuracy: 163.1936\n",
      "Accuracy: 163.3227\n",
      "Accuracy: 163.2497\n",
      "Accuracy: 163.1768\n",
      "Accuracy: 163.1041\n",
      "Accuracy: 163.0460\n",
      "Accuracy: 163.0310\n",
      "Accuracy: 163.0734\n",
      "Accuracy: 163.0584\n",
      "Accuracy: 163.0149\n",
      "Accuracy: 163.0571\n",
      "Accuracy: 163.0993\n",
      "Accuracy: 163.1129\n",
      "Accuracy: 163.0979\n",
      "Accuracy: 163.0830\n",
      "Accuracy: 163.0682\n",
      "Accuracy: 163.0533\n",
      "Accuracy: 163.0669\n",
      "Accuracy: 162.9955\n",
      "Accuracy: 162.9242\n",
      "Accuracy: 162.8531\n",
      "Accuracy: 162.8386\n",
      "Accuracy: 162.7959\n",
      "Accuracy: 162.8378\n",
      "Accuracy: 162.8234\n",
      "Accuracy: 162.8090\n",
      "Accuracy: 162.8227\n",
      "Accuracy: 162.8363\n",
      "Accuracy: 162.7660\n",
      "Accuracy: 162.8076\n",
      "Accuracy: 162.8492\n",
      "Accuracy: 162.8348\n",
      "Accuracy: 162.8205\n",
      "Accuracy: 162.7784\n",
      "Accuracy: 162.7086\n",
      "Accuracy: 162.6389\n",
      "Accuracy: 162.6249\n",
      "Accuracy: 162.7217\n",
      "Accuracy: 162.6800\n",
      "Accuracy: 162.6936\n",
      "Accuracy: 162.6381\n",
      "Accuracy: 162.6794\n",
      "Accuracy: 162.6929\n",
      "Accuracy: 162.6514\n",
      "Accuracy: 162.6375\n",
      "Accuracy: 162.5687\n",
      "Accuracy: 162.5000\n",
      "Accuracy: 162.4315\n",
      "Accuracy: 162.4452\n",
      "Accuracy: 162.4316\n",
      "Accuracy: 162.3634\n",
      "Accuracy: 162.3499\n",
      "Accuracy: 162.3364\n",
      "Accuracy: 162.3230\n",
      "Accuracy: 162.3368\n",
      "Accuracy: 162.2962\n",
      "Accuracy: 162.2828\n",
      "Accuracy: 162.2966\n",
      "Accuracy: 162.3917\n",
      "Accuracy: 162.3512\n",
      "Accuracy: 162.3514\n",
      "Accuracy: 162.3380\n",
      "Accuracy: 162.3247\n",
      "Accuracy: 162.3384\n",
      "Accuracy: 162.2982\n",
      "Accuracy: 162.2849\n",
      "Accuracy: 162.2986\n",
      "Accuracy: 162.3659\n",
      "Accuracy: 162.3258\n",
      "Accuracy: 162.2859\n",
      "Accuracy: 162.3262\n",
      "Accuracy: 162.3397\n",
      "Accuracy: 162.3533\n",
      "Accuracy: 162.3401\n",
      "Accuracy: 162.3003\n",
      "Accuracy: 162.2340\n",
      "Accuracy: 162.1679\n",
      "Accuracy: 162.1019\n",
      "Accuracy: 162.0891\n",
      "Accuracy: 162.1822\n",
      "Accuracy: 162.1693\n",
      "Accuracy: 162.1829\n",
      "Accuracy: 162.1832\n",
      "Accuracy: 162.2495\n",
      "Accuracy: 162.1839\n",
      "Accuracy: 162.1711\n",
      "Accuracy: 162.1583\n",
      "Accuracy: 162.0930\n",
      "Accuracy: 162.0278\n",
      "Accuracy: 161.9628\n",
      "Accuracy: 161.9503\n",
      "Accuracy: 161.9378\n",
      "Accuracy: 161.9775\n",
      "Accuracy: 161.9911\n",
      "Accuracy: 162.0308\n",
      "Accuracy: 162.1484\n",
      "Accuracy: 162.2138\n",
      "Accuracy: 162.2791\n",
      "Accuracy: 162.3183\n",
      "Accuracy: 162.2796\n",
      "Accuracy: 162.2668\n",
      "Accuracy: 162.2541\n",
      "Accuracy: 162.1898\n",
      "Accuracy: 162.1255\n",
      "Accuracy: 162.0614\n",
      "Accuracy: 162.0490\n",
      "Accuracy: 162.0366\n",
      "Accuracy: 162.0242\n",
      "Accuracy: 162.0118\n",
      "Accuracy: 161.9995\n",
      "Accuracy: 161.9872\n",
      "Accuracy: 161.9749\n",
      "Accuracy: 161.9626\n",
      "Accuracy: 161.9248\n",
      "Accuracy: 161.9127\n",
      "Accuracy: 161.9770\n",
      "Accuracy: 161.9011\n",
      "Accuracy: 161.8381\n",
      "Accuracy: 161.7752\n",
      "Accuracy: 161.7632\n",
      "Accuracy: 161.7259\n",
      "Accuracy: 161.7140\n",
      "Accuracy: 161.6768\n",
      "Accuracy: 161.7156\n",
      "Accuracy: 161.6785\n",
      "Accuracy: 161.6414\n",
      "Accuracy: 161.6801\n",
      "Accuracy: 161.6683\n",
      "Accuracy: 161.6062\n",
      "Accuracy: 161.5443\n",
      "Accuracy: 161.4824\n",
      "Accuracy: 161.4960\n",
      "Accuracy: 161.4845\n",
      "Accuracy: 161.4729\n",
      "Accuracy: 161.4364\n",
      "Accuracy: 161.4500\n",
      "Accuracy: 161.4885\n",
      "Accuracy: 161.5269\n",
      "Accuracy: 161.5653\n",
      "Accuracy: 161.5289\n",
      "Accuracy: 161.5672\n",
      "Accuracy: 161.5805\n",
      "Accuracy: 161.5442\n",
      "Accuracy: 161.4955\n",
      "Accuracy: 161.4346\n",
      "Accuracy: 161.3738\n",
      "Accuracy: 161.3378\n",
      "Accuracy: 161.3513\n",
      "Accuracy: 161.3401\n",
      "Accuracy: 161.3042\n",
      "Accuracy: 161.3177\n",
      "Accuracy: 161.2574\n",
      "Accuracy: 161.1971\n",
      "Accuracy: 161.1370\n",
      "Accuracy: 161.1506\n",
      "Accuracy: 161.1152\n",
      "Accuracy: 161.1288\n",
      "Accuracy: 161.1179\n",
      "Accuracy: 161.1070\n",
      "Accuracy: 161.0718\n",
      "Accuracy: 161.1098\n",
      "Accuracy: 161.1233\n",
      "Accuracy: 161.0881\n",
      "Accuracy: 161.1260\n",
      "Accuracy: 161.1152\n",
      "Accuracy: 161.0801\n",
      "Accuracy: 161.0451\n",
      "Accuracy: 160.9859\n",
      "Accuracy: 160.9269\n",
      "Accuracy: 160.8922\n",
      "Accuracy: 160.8816\n",
      "Accuracy: 160.9194\n",
      "Accuracy: 160.9089\n",
      "Accuracy: 160.9224\n",
      "Accuracy: 160.8879\n",
      "Accuracy: 160.9495\n",
      "Accuracy: 160.8910\n",
      "Accuracy: 160.8565\n",
      "Accuracy: 160.8941\n",
      "Accuracy: 160.8357\n",
      "Accuracy: 160.7775\n",
      "Accuracy: 160.7672\n",
      "Accuracy: 160.7808\n",
      "Accuracy: 160.7467\n",
      "Accuracy: 160.7364\n",
      "Accuracy: 160.7262\n",
      "Accuracy: 160.7636\n",
      "Accuracy: 160.7652\n",
      "Accuracy: 160.7550\n",
      "Accuracy: 160.7448\n",
      "Accuracy: 160.7109\n",
      "Accuracy: 160.7481\n",
      "Accuracy: 160.6906\n",
      "Accuracy: 160.6333\n",
      "Accuracy: 160.5760\n",
      "Accuracy: 160.5660\n",
      "Accuracy: 160.5325\n",
      "Accuracy: 160.5932\n",
      "Accuracy: 160.5597\n",
      "Accuracy: 160.5263\n",
      "Accuracy: 160.5869\n",
      "Accuracy: 160.5652\n",
      "Accuracy: 160.5319\n",
      "Accuracy: 160.6156\n",
      "Accuracy: 160.6057\n",
      "Accuracy: 160.5491\n",
      "Accuracy: 160.4925\n",
      "Accuracy: 160.4594\n",
      "Accuracy: 160.4963\n",
      "Accuracy: 160.4399\n",
      "Accuracy: 160.5000\n",
      "Accuracy: 160.4902\n",
      "Accuracy: 160.5037\n",
      "Accuracy: 160.4940\n",
      "Accuracy: 160.4379\n",
      "Accuracy: 160.4051\n",
      "Accuracy: 160.4186\n",
      "Accuracy: 160.4090\n",
      "Accuracy: 160.3763\n",
      "Accuracy: 160.3436\n",
      "Accuracy: 160.2880\n",
      "Accuracy: 160.2325\n",
      "Accuracy: 160.1771\n",
      "Accuracy: 160.1677\n",
      "Accuracy: 160.1584\n",
      "Accuracy: 160.1950\n",
      "Accuracy: 160.2314\n",
      "Accuracy: 160.2450\n",
      "Accuracy: 160.2356\n",
      "Accuracy: 160.2034\n",
      "Accuracy: 160.2397\n",
      "Accuracy: 160.2532\n",
      "Accuracy: 160.2894\n",
      "Accuracy: 160.2801\n",
      "Accuracy: 160.2480\n",
      "Accuracy: 160.2386\n",
      "Accuracy: 160.1839\n",
      "Accuracy: 160.1293\n",
      "Accuracy: 160.0748\n",
      "Accuracy: 160.0883\n",
      "Accuracy: 160.1018\n",
      "Accuracy: 160.0927\n",
      "Accuracy: 160.1061\n",
      "Accuracy: 160.0970\n",
      "Accuracy: 160.0879\n",
      "Accuracy: 160.0788\n",
      "Accuracy: 160.0473\n",
      "Accuracy: 160.0157\n",
      "Accuracy: 160.0517\n",
      "Accuracy: 160.0426\n",
      "Accuracy: 160.0112\n",
      "Accuracy: 159.9574\n",
      "Accuracy: 159.9933\n",
      "Accuracy: 159.9620\n",
      "Accuracy: 159.9307\n",
      "Accuracy: 159.9442\n",
      "Accuracy: 159.9353\n",
      "Accuracy: 159.9710\n",
      "Accuracy: 160.0067\n",
      "Accuracy: 159.9755\n",
      "Accuracy: 159.9889\n",
      "Accuracy: 160.0022\n",
      "Accuracy: 159.9823\n",
      "Accuracy: 159.9291\n",
      "Accuracy: 159.9424\n",
      "Accuracy: 159.9336\n",
      "Accuracy: 159.9469\n",
      "Accuracy: 159.9382\n",
      "Accuracy: 159.9515\n",
      "Accuracy: 159.8986\n",
      "Accuracy: 159.8458\n",
      "Accuracy: 159.7931\n",
      "Accuracy: 159.8285\n",
      "Accuracy: 159.9077\n",
      "Accuracy: 159.8771\n",
      "Accuracy: 159.8904\n",
      "Accuracy: 159.9255\n",
      "Accuracy: 159.9606\n",
      "Accuracy: 159.9519\n",
      "Accuracy: 159.9869\n",
      "Accuracy: 159.9563\n",
      "Accuracy: 159.9476\n",
      "Accuracy: 159.9172\n",
      "Accuracy: 159.8868\n",
      "Accuracy: 159.8346\n",
      "Accuracy: 159.7826\n",
      "Accuracy: 159.7307\n",
      "Accuracy: 159.7222\n",
      "Accuracy: 159.7788\n",
      "Accuracy: 159.7920\n",
      "Accuracy: 159.7835\n",
      "Accuracy: 159.7535\n",
      "Accuracy: 159.8315\n",
      "Accuracy: 159.8014\n",
      "Accuracy: 159.7498\n",
      "Accuracy: 159.6983\n",
      "Accuracy: 159.6684\n",
      "Accuracy: 159.7031\n",
      "Accuracy: 159.6948\n",
      "Accuracy: 159.7079\n",
      "Accuracy: 159.6781\n",
      "Accuracy: 159.7127\n",
      "Accuracy: 159.6829\n",
      "Accuracy: 159.6640\n",
      "Accuracy: 159.7198\n",
      "Accuracy: 159.6902\n",
      "Accuracy: 159.7246\n",
      "Accuracy: 159.6736\n",
      "Accuracy: 159.6228\n",
      "Accuracy: 159.5720\n",
      "Accuracy: 159.5426\n",
      "Accuracy: 159.5132\n",
      "Accuracy: 159.4839\n",
      "Accuracy: 159.4970\n",
      "Accuracy: 159.5526\n",
      "Accuracy: 159.5869\n",
      "Accuracy: 159.5787\n",
      "Accuracy: 159.6129\n",
      "Accuracy: 159.6048\n",
      "Accuracy: 159.5756\n",
      "Accuracy: 159.5675\n",
      "Accuracy: 159.5594\n",
      "Accuracy: 159.5093\n",
      "Accuracy: 159.4592\n",
      "Accuracy: 159.4092\n",
      "Accuracy: 159.3592\n",
      "Accuracy: 159.3304\n",
      "Accuracy: 159.3435\n",
      "Accuracy: 159.3462\n",
      "Accuracy: 159.3174\n",
      "Accuracy: 159.3096\n",
      "Accuracy: 159.3436\n",
      "Accuracy: 159.3567\n",
      "Accuracy: 159.3907\n",
      "Accuracy: 159.3932\n",
      "Accuracy: 159.4271\n",
      "Accuracy: 159.3984\n",
      "Accuracy: 159.3490\n",
      "Accuracy: 159.2997\n",
      "Accuracy: 159.2712\n",
      "Accuracy: 159.2324\n",
      "Accuracy: 159.2454\n",
      "Accuracy: 159.2585\n",
      "Accuracy: 159.2508\n",
      "Accuracy: 159.1811\n",
      "Accuracy: 159.1529\n",
      "Accuracy: 159.1040\n",
      "Accuracy: 159.1172\n",
      "Accuracy: 159.0993\n",
      "Accuracy: 159.0507\n",
      "Accuracy: 159.0021\n",
      "Accuracy: 158.9535\n",
      "Accuracy: 158.9462\n",
      "Accuracy: 158.9183\n",
      "Accuracy: 158.9110\n",
      "Accuracy: 158.9037\n",
      "Accuracy: 158.8862\n",
      "Accuracy: 158.8687\n",
      "Accuracy: 158.8818\n",
      "Accuracy: 158.8746\n",
      "Accuracy: 158.8980\n",
      "Accuracy: 158.8703\n",
      "Accuracy: 158.8223\n",
      "Accuracy: 158.7948\n",
      "Accuracy: 158.8080\n",
      "Accuracy: 158.8008\n",
      "Accuracy: 158.8140\n",
      "Accuracy: 158.7967\n",
      "Accuracy: 158.7895\n",
      "Accuracy: 158.8229\n",
      "Accuracy: 158.7955\n",
      "Accuracy: 158.7682\n",
      "Accuracy: 158.7611\n",
      "Accuracy: 158.7944\n",
      "Accuracy: 158.7470\n",
      "Accuracy: 158.7198\n",
      "Accuracy: 158.7127\n",
      "Accuracy: 158.6856\n",
      "Accuracy: 158.6786\n",
      "Accuracy: 158.6716\n",
      "Accuracy: 158.6647\n",
      "Accuracy: 158.6577\n",
      "Accuracy: 158.6307\n",
      "Accuracy: 158.6038\n",
      "Accuracy: 158.5568\n",
      "Accuracy: 158.5100\n",
      "8 8\n",
      "15851.0 10000.0\n",
      "158.51\n",
      "tensor([9, 0, 1, 2, 3, 4, 5, 6])\n",
      "tensor([[9],\n",
      "        [0],\n",
      "        [1],\n",
      "        [2],\n",
      "        [3],\n",
      "        [4],\n",
      "        [5],\n",
      "        [6]])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# test the model\n",
    "total = 0.0\n",
    "correct = 0.0\n",
    "\n",
    "for data in testloader:\n",
    "    # get the data\n",
    "    inputs = data[0]\n",
    "    labels = data[1]\n",
    "\n",
    "    # make predictions\n",
    "    output = predictionModel(inputs)[0]\n",
    "\n",
    " \n",
    "\n",
    "    # accuracy =getaccuracy(output,labels)\n",
    "    _, predicted = torch.max(output, 1)\n",
    "    total += labels.size(0)\n",
    "    correct += (predicted == labels).sum().item()\n",
    "    accuracy = 100 *correct /total\n",
    "    print ('Accuracy: {:.4f}'.format(accuracy))\n",
    "\n",
    "print(len(labels), len(predicted))\n",
    "print(correct, total)\n",
    "print(accuracy)\n",
    "print(predicted)\n",
    "print(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'torch' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 19\u001b[0m\n\u001b[0;32m     16\u001b[0m img_resized \u001b[39m=\u001b[39m cv2\u001b[39m.\u001b[39mresize(test_image, (\u001b[39m28\u001b[39m, \u001b[39m28\u001b[39m), interpolation\u001b[39m=\u001b[39mcv2\u001b[39m.\u001b[39mINTER_LINEAR)\n\u001b[0;32m     18\u001b[0m \u001b[39m# convert test image to tensor for prediction\u001b[39;00m\n\u001b[1;32m---> 19\u001b[0m tensor_img \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mfrom_numpy(\u001b[39m~\u001b[39mimg_resized)\n\u001b[0;32m     20\u001b[0m reshaped_img \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mreshape(tensor_img, [\u001b[39m1\u001b[39m,\u001b[39m1\u001b[39m,\u001b[39m28\u001b[39m,\u001b[39m28\u001b[39m])\n\u001b[0;32m     22\u001b[0m \u001b[39m# make predictions\u001b[39;00m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'torch' is not defined"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import os\n",
    "from PIL import Image\n",
    "\n",
    "# read list of test images\n",
    "folder = './images/'\n",
    "img_list =  os.listdir(folder)\n",
    "\n",
    "# predict for each file\n",
    "for image in img_list:\n",
    "    file_path = folder + image\n",
    "\n",
    "    test_image = cv2.imread(file_path, cv2.IMREAD_GRAYSCALE)\n",
    "\n",
    "    # format Image\n",
    "    img_resized = cv2.resize(test_image, (28, 28), interpolation=cv2.INTER_LINEAR)\n",
    "\n",
    "    # convert test image to tensor for prediction\n",
    "    tensor_img = torch.from_numpy(~img_resized)\n",
    "    reshaped_img = np.reshape(tensor_img, [1,1,28,28])\n",
    " \n",
    "    # make predictions\n",
    "    output = predictionModel(reshaped_img)[0]\n",
    "    _, predicted = torch.max(output, 1)\n",
    "    print(f\"\\nImage: {file_path} \\nPrediction: {predicted}\")\n",
    "\n",
    "    # squeeze 1 dimension\n",
    "    reshaped_img = reshaped_img.squeeze()\n",
    "    numImage = Image.fromarray((reshaped_img.numpy()))\n",
    "\n",
    "    display(numImage)\n",
    "\n",
    "# Note the testing did not work on images that were not inverted\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.5 ('env': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "eb5d223395ffa13ce0bb8900d854bb07b9948fb92b9110105edc4f288e135ea0"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
